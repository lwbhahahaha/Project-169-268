{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `g:\\桌面\\2022 Fall\\cs268\\final_proj`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n",
    "# Pkg.add(\"DataLoaders\")\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"CUDA\")\n",
    "# Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataLoaders\n",
    "using Plots\n",
    "using CUDA\n",
    "using Distributions \n",
    "using LinearAlgebra\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom ML structure to apply custom optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "The Iris dataset is from UC Irvine Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot(y)\n",
    "    rslt = zeros(3)\n",
    "    rslt[trunc(Int, y) + 1] = 1.0\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all lines of data in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "train_x_data = []\n",
    "train_y_data = []\n",
    "# 8:2\n",
    "# test_x_data = []\n",
    "# test_y_data = []\n",
    "for line in eachline(\"iris.txt\")\n",
    "    splitted = collect(split(line, \" \"))\n",
    "    push!(train_x_data, [parse(Float64, splitted[4]), parse(Float64, splitted[7]), parse(Float64, splitted[10]), parse(Float64, splitted[13])])\n",
    "    push!(train_y_data, one_hot(parse(Float64, splitted[16])))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect shape of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148,)(148,)"
     ]
    }
   ],
   "source": [
    "print(size(train_x_data))\n",
    "print(size(train_y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoaders.GetObsParallel{DataLoaders.BatchViewCollated{Tuple{Vector{Any}, Vector{Any}}}}(batchviewcollated() with 5 batches of size 33, false)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 33\n",
    "dl_train = DataLoader((train_x_data, train_y_data), batch_size)\n",
    "# dl_test = DataLoader((test_x, test_y), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create whole machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an activation function: ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relu! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Relu!(Xs)\n",
    "    for i = 1 : length(Xs)\n",
    "        Xs[i] = max(0.0,Xs[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fc (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function fc(Xs, num_in_channel, num_out_channel, W)\n",
    "    rslt = zeros(num_out_channel)\n",
    "    Xs = reshape(Xs, 1, num_in_channel)\n",
    "    b = rand()\n",
    "    for i = 1:num_out_channel\n",
    "        temp = reshape(W[:, i], num_in_channel, 1)\n",
    "        rslt[i] = (Xs * temp)[1]+b\n",
    "    end\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function MLP(Xs, out_channel; num_hidden_layers = 0, hidden_layer_size = 64, Ws = nothing)\n",
    "    Ws_idx = 1\n",
    "    hidden_outs = []\n",
    "    # layer in\n",
    "    if Ws == nothing\n",
    "        Wss = []\n",
    "        W = rand(Uniform(-0.1, 0.1), size(Xs)[1], hidden_layer_size)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(Xs, size(Xs)[1], hidden_layer_size, W)\n",
    "    Relu!(out)\n",
    "    push!(hidden_outs, deepcopy(out))\n",
    "    # layer hidden\n",
    "    for i = 1 : num_hidden_layers\n",
    "        if Ws == nothing\n",
    "            W = rand(Uniform(-0.1, 0.1), hidden_layer_size, hidden_layer_size)\n",
    "            push!(Wss, W)\n",
    "        else\n",
    "            W = Ws[Ws_idx]\n",
    "            Ws_idx += 1\n",
    "        end\n",
    "        out = fc(out, hidden_layer_size, hidden_layer_size, W)\n",
    "        Relu!(out)\n",
    "        push!(hidden_outs, deepcopy(out))\n",
    "    end\n",
    "    # layer out\n",
    "    if Ws == nothing\n",
    "        W = rand(Uniform(-0.1, 0.1), hidden_layer_size, out_channel)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(out, hidden_layer_size, out_channel, W)\n",
    "    if Ws == nothing\n",
    "        return out, hidden_outs, Wss\n",
    "    end\n",
    "    return out, hidden_outs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loss function: soft-max loss\n",
    "Soft-max loss is a combination of a soft-max activation layer and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred in only 1 set of prediction in a batch. Ys in only 1 set of truth label in a batch.\n",
    "function softmax_loss(Y_pred, Ys)\n",
    "    # softmax\n",
    "    temp_sum = 0.0\n",
    "    for x in Y_pred\n",
    "        temp_sum += exp(x)\n",
    "    end\n",
    "    l = length(Y_pred)\n",
    "    scores = zeros(l)\n",
    "    for i = 1:l\n",
    "        scores[i] = exp(Y_pred[i])/temp_sum\n",
    "    end\n",
    "    # loss\n",
    "    losses = zeros(l)\n",
    "    for i = 1:l\n",
    "        losses[i] = -Ys[i] * log(scores[i])\n",
    "    end\n",
    "    total_loss = sum(losses)\n",
    "    \n",
    "    return scores, losses, total_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many threads we have for cpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a back-propagation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇ (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ws[idx of layer][idx of node, idx of weights]\n",
    "function ∇(Xs, Ys, Ws, scores, outs, hidden_outs)\n",
    "    num_in_channel = size(Xs)[1]\n",
    "    num_out_channel = size(Ys)[1]\n",
    "    hidden_layer_size = size(hidden_outs[1])[1]\n",
    "    num_hidden_layers = size(hidden_outs)[1]\n",
    "    # step -2\n",
    "    ∂loss_∂netout = zeros(num_out_channel)\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] = -Ys[i]/scores[i]\n",
    "    end\n",
    "    # println(\"step -2: $∂loss_∂netout\")\n",
    "    # step -1\n",
    "    outs_exp_sum = 0.0\n",
    "    for out in outs\n",
    "        outs_exp_sum += exp(out)\n",
    "    end\n",
    "    # println(\"step -1: $outs_exp_sum\")\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] *= (exp(outs[i])*(outs_exp_sum - exp(outs[i])))/(outs_exp_sum^2)\n",
    "    end\n",
    "    # println(\"step -1: $∂loss_∂netout\")\n",
    "    # Assume we use same hidden_size of all hidden layers for the sake for brevity\n",
    "    ∇s = []\n",
    "    # first layer\n",
    "    push!(∇s, zeros(num_in_channel, hidden_layer_size))\n",
    "    # hidden layer\n",
    "    for i = 1 : num_hidden_layers-1\n",
    "        push!(∇s, zeros(hidden_layer_size, hidden_layer_size))\n",
    "    end\n",
    "    # last layer\n",
    "    push!(∇s, zeros(hidden_layer_size, num_out_channel))\n",
    "\n",
    "    # Start back_propagations\n",
    "    ∂loss_∂prev_nodes = zeros(hidden_layer_size)\n",
    "    # layer weights from outs to last of hidden layer\n",
    "    for i = 1 : hidden_layer_size\n",
    "        ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[num_hidden_layers][i],\n",
    "        num_hidden_layers+1, i, ∇s,\n",
    "        Ws[num_hidden_layers+1][i, :], ∂loss_∂netout)\n",
    "    end\n",
    "    # layer weights from i-th hidden layer to i-1th hidden layer\n",
    "    for k = num_hidden_layers : -1 : 2\n",
    "        ∂loss_∂curr_nodes = deepcopy(∂loss_∂prev_nodes)\n",
    "        for i = 1 : hidden_layer_size\n",
    "            ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[k-1][i],\n",
    "            k, i, ∇s,\n",
    "            Ws[k][i, :], ∂loss_∂curr_nodes)\n",
    "        end\n",
    "    end\n",
    "    # layer weights from 1st hidden layer to input layer\n",
    "    for i = 1 : num_in_channel\n",
    "        back_propagation(Xs[i],\n",
    "        1, i, ∇s,\n",
    "        Ws[1][i, :], ∂loss_∂prev_nodes)\n",
    "    end\n",
    "    return ∇s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagation (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function back_propagation(node_out, layer_idx, i, ∇s, weights, ∂loss_∂nodes)\n",
    "    ∂loss_∂node_i = (node_out==0) ? 0 : dot(weights, ∂loss_∂nodes) # considering d of ReLU\n",
    "    ∇s[layer_idx][i, :] = ∂loss_∂nodes*node_out\n",
    "    return ∂loss_∂node_i\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_step(optimizer, Xs_batch, Ys_batch, Ws_wrapped; hidden_layer_size = 64)\n",
    "    b_s = size(Xs_batch)[2]\n",
    "    # b_s = 33\n",
    "    cache_new_Ws_unwrapped = zeros(b_s, num_Ws)\n",
    "    total_losses = zeros(b_s)\n",
    "    Ws_unwrapped = _unwrap(Ws_wrapped)\n",
    "    optimizer.k = optimizer.k + 1\n",
    "    Threads.@threads for i in 1:b_s-1\n",
    "        Xs = Xs_batch[:, i]\n",
    "        Y_truth = one_hot(Ys_batch[i])\n",
    "        #forward\n",
    "        Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "        scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "        total_losses[i] = total_loss\n",
    "        #backward\n",
    "        if typeof(optimizer) <: NesterovMomentum_SGD \n",
    "            cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "        else\n",
    "            gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "            cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "        end\n",
    "        # if i==1\n",
    "        #     println()\n",
    "        #     println(Y_pred)\n",
    "        #     println(scores)\n",
    "        #     println(Y_truth)\n",
    "        #     println()\n",
    "        # end\n",
    "    end\n",
    "    i = b_s\n",
    "    Xs = Xs_batch[:, i]\n",
    "    Y_truth = one_hot(Ys_batch[i])\n",
    "    #forward\n",
    "    Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "    total_losses[i] = total_loss\n",
    "    #backward\n",
    "    gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "    if typeof(optimizer) <: NesterovMomentum_SGD \n",
    "        cache_new_Ws_unwrapped[i,:] = step!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "    else\n",
    "        gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "        cache_new_Ws_unwrapped[i,:] = step!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "    end\n",
    "\n",
    "    #update parameters\n",
    "    # println(cache_new_Ws_unwrapped[:,1])\n",
    "    new_Ws_unwrapped = mean(cache_new_Ws_unwrapped, dims=1)\n",
    "    # println(new_Ws_unwrapped[1])\n",
    "    # println(\" loss = $(mean(total_losses))\")\n",
    "    # println(\" mean new para = $(mean(new_Ws_unwrapped))\")\n",
    "    return _wrap(new_Ws_unwrapped, params_shape), mean(total_losses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function epoch_step(optimizer, new_Ws_wrapped; hidden_layer_size = 64)\n",
    "    ct_batch = 1\n",
    "    curr_loss = 0\n",
    "    for (xs, ys) in dl_train\n",
    "        print(\"batch# $ct_batch --> \")\n",
    "        ct_batch += 1\n",
    "        new_Ws_wrapped, curr_loss = batch_step(optimizer, xs, ys, new_Ws_wrapped; hidden_layer_size = hidden_layer_size)\n",
    "        flush(stdout)\n",
    "    end\n",
    "    println(\"\\n\\ttrain loss =  $curr_loss\")\n",
    "    return new_Ws_wrapped, curr_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to make inputs compatible with optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_wrap (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _unwrap(wrapped)\n",
    "    unwrapped = []\n",
    "    for layer in wrapped\n",
    "        unwrapped = vcat(unwrapped, reduce(vcat,layer))\n",
    "    end\n",
    "    return unwrapped\n",
    "end\n",
    "function _wrap(unwrapped, shape)\n",
    "    wrapped = []\n",
    "    curr_idx = 1\n",
    "    for s in shape\n",
    "        ct = s[1]*s[2]\n",
    "        push!(wrapped, Float64.(reshape(unwrapped[curr_idx:curr_idx-1+ct], s)))\n",
    "        curr_idx += ct\n",
    "    end\n",
    "    return wrapped\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steepest gradient descent\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct SteepestGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::SteepestGD)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end\n",
    "function step_without_update!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct Adam <: DescentMethod\n",
    "    α # learning rate\n",
    "    γv # decay\n",
    "    γs # decay\n",
    "    ϵ # small value\n",
    "    k # step counter\n",
    "    v # 1st moment estimate\n",
    "    s # 2nd moment estimate\n",
    "end\n",
    "function init!(M::Adam, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "    M.s = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    s, v, g = M.s, M.v, ∇f\n",
    "    M.v = γv*v + (1-γv)*g\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    # M.k = k += 1\n",
    "    v_hat = M.v ./ (1 - γv^k)\n",
    "    s_hat = M.s ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end\n",
    "function step_without_update!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    vv = γv*M.v + (1-γv)*∇f\n",
    "    ss = γs*M.s + (1-γs)*∇f.*∇f\n",
    "    # M.k = k += 1\n",
    "    v_hat = vv ./ (1 - γv^k)\n",
    "    s_hat = ss ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate gradient descent\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 3 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct ConjugateGD <: DescentMethod\n",
    "    α\n",
    "    g\n",
    "    f\n",
    "    d\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::ConjugateGD)\n",
    "    M.k = 0\n",
    "    M.d = -M.g/sqrt(sum(M.g.^2))\n",
    "end\n",
    "# function step!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step!(M::ConjugateGD, g, x)\n",
    "    d_prev, g_prev = M.d, M.g\n",
    "    β = dot(transpose(g), g-g_prev)/dot(transpose(g_prev), g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    d = d/sqrt(sum(d.^2))\n",
    "    # M.α = strong_backtracking_line_search(train_x, train_y, M.f, x, d, g; α = M.α, σ=0.9)\n",
    "    x_next = x + M.α*d\n",
    "    M.d, M.g = d, g\n",
    "    return x_next\n",
    "end\n",
    "# function step_without_update!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step_without_update!(M::ConjugateGD, g, x)\n",
    "    d_prev, g_prev = M.d, M.g\n",
    "    β = dot(transpose(g), g-g_prev)/dot(transpose(g_prev), g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    d = d/sqrt(sum(d.^2))\n",
    "    # M.α = strong_backtracking_line_search(train_x, train_y, M.f, x, d, g; α = M.α, σ=0.9)\n",
    "    x_next = x + M.α*d\n",
    "    # M.d, M.g = d, g\n",
    "    return x_next\n",
    "end\n",
    "# # implementing strong backtracking line search\n",
    "# function strong_backtracking_line_search(train_x, train_y, f, x, d, g; α=1, β=1e-4, σ=0.1, p=2)\n",
    "#     if g == zeros(length(g))\n",
    "#         # println(\"******************* g = 0 *******************\")\n",
    "#         return 0.0\n",
    "#     end\n",
    "#     # println(\"\\tStart bracketing phase\")\n",
    "#     # bracketing phase\n",
    "#     prev_α = 0\n",
    "#     f_x, dumb1, dumb2, dumb3 = f(train_x, train_y, x)\n",
    "#     x_αd = x+α*d\n",
    "#     ∇f_x = transpose(g)*d\n",
    "#     f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#     ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#     ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#     condition_a = f_x_αd >= f_x\n",
    "#     condition_b = f_x_αd > f_x + β*α*∇f_x\n",
    "#     condition_c = ∇f_x_αd >= 0\n",
    "#     while !(condition_a || condition_b || condition_c)\n",
    "#         prev_α, α = α, α*p\n",
    "#         # println(\"\\t\\tα = $α\")\n",
    "#         x_αd = x+α*d\n",
    "#         f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#         ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#         ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#         condition_a = f_x_αd >= f_x\n",
    "#         condition_b = f_x_αd > f_x + β*α*∇f_x\n",
    "#         condition_c = ∇f_x_αd >= 0\n",
    "#     end\n",
    "#     # println(\"\\t bracket = [$prev_α, $α]\")\n",
    "#     # println(\"\\tFinish bracketing phase\")\n",
    "#     # println(\"\\tStart bracketing phase\")\n",
    "#     # zoom phase\n",
    "#     # println(\"\\t\\tStart sufficient decrease condition\")\n",
    "#     # sufficient decrease condition\n",
    "#     sufficient_decrease_condition = !condition_b\n",
    "#     while !sufficient_decrease_condition\n",
    "#         α = (prev_α + α)/2 \n",
    "#         if α == 0.0\n",
    "#             # println(\"******************* α = 0 *******************\")\n",
    "#             return 0.0\n",
    "#         end\n",
    "#         # println(\"\\t\\t\\tα = $α\")\n",
    "#         x_αd = x+α*d\n",
    "#         f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#         sufficient_decrease_condition = f_x_αd <= f_x + β*α*∇f_x\n",
    "#     end\n",
    "#     # println(\"\\t\\tFinish sufficient decrease condition\")\n",
    "#     # strong curvature condition \n",
    "#     # println(\"\\t\\tStart strong curvature condition\")\n",
    "#     ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#     ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#     strong_curvature_condition = abs(∇f_x_αd) <= -σ*∇f_x\n",
    "#     while !strong_curvature_condition\n",
    "#         α = (prev_α + α)/2\n",
    "#         if α == 0.0\n",
    "#             # println(\"******************* α = 0 *******************\")\n",
    "#             return 0.0\n",
    "#         end\n",
    "#         # println(\"\\t\\t\\tα = $α\")\n",
    "#         x_αd = x+α*d\n",
    "#         f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#         ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#         ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#         strong_curvature_condition = abs(∇f_x_αd) <= -σ*∇f_x\n",
    "#     end\n",
    "#     # println(\"\\t\\tFinish strong curvature condition\")\n",
    "#     # println(\"\\tFinish bracketing phase\")\n",
    "#     return α\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Momentum (SGD)\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 4 methods)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From K&W 76\n",
    "mutable struct NesterovMomentum_SGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    β # momentum decay\n",
    "    f\n",
    "    v # momentum\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::NesterovMomentum_SGD, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "end\n",
    "function step!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    M.v = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + M.v\n",
    "end\n",
    "function step_without_update!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    vv = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + vv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control variable for a scientifically correct comparison: **Give every optimizers a same NN to train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created!\n",
      "Inital loss = 1.0654244064213234\n"
     ]
    }
   ],
   "source": [
    "loss_ε = 0.01 # stop when loss <= loss_ε\n",
    "# Initializing \n",
    "Y_pred, hidden_outs, Ws_copy = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "# println(Y_pred)\n",
    "scores, losses, total_loss = softmax_loss(Y_pred, train_y_data[1])\n",
    "∇_copy = ∇(train_x_data[1], train_y_data[1], Ws_copy, scores, Y_pred, hidden_outs);\n",
    "println(\"New model created!\\nInital loss = $total_loss\")\n",
    "params_shape = []\n",
    "num_Ws = 0\n",
    "for layer in Ws_copy\n",
    "    # println(size(layer))\n",
    "    s1, s2 = size(layer)\n",
    "    num_Ws += s1 * s2\n",
    "    push!(params_shape, size(layer))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Steepest gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Steepest_GD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_SteepestGD = SteepestGD(0.01, nothing)\n",
    "init!(optimizer_SteepestGD)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0574794132351686\n",
      "Epoch# 2\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0090353614456307\n",
      "Epoch# 3\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9771846559311114\n",
      "Epoch# 4\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9437258493200867\n",
      "Epoch# 5\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9343944950249846\n",
      "Epoch# 6\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8752723694546143\n",
      "Epoch# 7\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8561123833847518\n",
      "Epoch# 8\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.813260383669708\n",
      "Epoch# 9\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8404007268218463\n",
      "Epoch# 10\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8203210201516009\n",
      "Epoch# 11\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8116846325575325\n",
      "Epoch# 12\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7566648162100802\n",
      "Epoch# 13\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7286125599491762\n",
      "Epoch# 14\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7631190651244443\n",
      "Epoch# 15\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7513194869041256\n",
      "Epoch# 16\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.701482606927998\n",
      "Epoch# 17\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7398658858801206\n",
      "Epoch# 18\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6038939629710199\n",
      "Epoch# 19\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5961462495779316\n",
      "Epoch# 20\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6441170510099457\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Steepest_GD\n",
    "    print(\"Epoch# $i\\n\\t\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_SteepestGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adam = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adam = Adam(0.01, 0.9, 0.999, 1e-7, nothing, nothing, nothing)\n",
    "init!(optimizer_Adam, num_Ws);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9657101169461695\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6840494596566998\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.22822763346511754\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.007713104168938901\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Adam\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adam, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = ConjugateGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch_ConjugateGD = 20;\n",
    "# function f(train_x, train_y, w_unwrapped)\n",
    "#     Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "#     scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "#     return total_loss, hidden_outs, Y_pred, scores\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "# optimizer_ConjugateGD = ConjugateGD(0.01, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "optimizer_ConjugateGD = ConjugateGD(0.01, _unwrap(deepcopy(∇_copy)), nothing, nothing, nothing)\n",
    "init!(optimizer_ConjugateGD);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0472862669053193\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0301267086428338\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9837931614922919\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9637532240414706\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9160669277648521\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8966267650069976\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.849643614354938\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8160657133511946\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8113718573244258\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.693148799892027\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6703874134291553\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6685418784711156\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6021213838136396\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5293295937410307\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5921415170139307\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5690829268650954\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5075937527674705\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5153078557631839\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.4660693959685156\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.4195980742082804\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_ConjugateGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_ConjugateGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = NesterovMomentum_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch_NesterovMomentum_SGD = 20\n",
    "function f(train_x, train_y, w_unwrapped)\n",
    "    Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "    return total_loss, hidden_outs, Y_pred, scores\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_NesterovMomentum_SGD = NesterovMomentum_SGD(0.01, 0.99, f, nothing, nothing)\n",
    "init!(optimizer_NesterovMomentum_SGD, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9328953591232785\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6094325110678358\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.2540328688773901\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.2172282368491196\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.10283362300724119\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.0014416007397873934\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_NesterovMomentum_SGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_NesterovMomentum_SGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (auto threads) 1.7.2",
   "language": "julia",
   "name": "julia-(auto-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
