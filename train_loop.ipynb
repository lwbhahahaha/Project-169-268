{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `g:\\桌面\\2022 Fall\\cs268\\final_proj\\Project-Optimizer`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n",
    "# Pkg.add(\"DataLoaders\")\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"CUDA\")\n",
    "# Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataLoaders\n",
    "using Plots\n",
    "using CUDA\n",
    "using Distributions \n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom ML structure to apply custom optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "The Iris dataset is from UC Irvine Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot(y)\n",
    "    rslt = zeros(3)\n",
    "    rslt[trunc(Int, y) + 1] = 1.0\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all lines of data in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "train_x_data = []\n",
    "train_y_data = []\n",
    "# 8:2\n",
    "# test_x_data = []\n",
    "# test_y_data = []\n",
    "for line in eachline(\"iris.txt\")\n",
    "    splitted = collect(split(line, \" \"))\n",
    "    push!(train_x_data, [parse(Float64, splitted[4]), parse(Float64, splitted[7]), parse(Float64, splitted[10]), parse(Float64, splitted[13])])\n",
    "    push!(train_y_data, one_hot(parse(Float64, splitted[16])))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect shape of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148,)(148,)"
     ]
    }
   ],
   "source": [
    "print(size(train_x_data))\n",
    "print(size(train_y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoaders.GetObsParallel{DataLoaders.BatchViewCollated{Tuple{Vector{Any}, Vector{Any}}}}(batchviewcollated() with 5 batches of size 33, false)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 33\n",
    "dl_train = DataLoader((train_x_data, train_y_data), batch_size)\n",
    "# dl_test = DataLoader((test_x, test_y), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create whole machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an activation function: ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relu! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Relu!(Xs)\n",
    "    for i = 1 : length(Xs)\n",
    "        Xs[i] = max(0.0,Xs[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fc (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function fc(Xs, num_in_channel, num_out_channel, W)\n",
    "    rslt = zeros(num_out_channel)\n",
    "    Xs = reshape(Xs, 1, num_in_channel)\n",
    "    b = rand()\n",
    "    for i = 1:num_out_channel\n",
    "        temp = reshape(W[:, i], num_in_channel, 1)\n",
    "        rslt[i] = (Xs * temp)[1]+b\n",
    "    end\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function MLP(Xs, out_channel; num_hidden_layers = 0, hidden_layer_size = 64, Ws = nothing, para_init=0.5)\n",
    "    Ws_idx = 1\n",
    "    hidden_outs = []\n",
    "    # layer in\n",
    "    if Ws == nothing\n",
    "        Wss = []\n",
    "        W = rand(Uniform(-para_init, para_init), size(Xs)[1], hidden_layer_size)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(Xs, size(Xs)[1], hidden_layer_size, W)\n",
    "    Relu!(out)\n",
    "    push!(hidden_outs, deepcopy(out))\n",
    "    # layer hidden\n",
    "    for i = 1 : num_hidden_layers\n",
    "        if Ws == nothing\n",
    "            W = rand(Uniform(-para_init, para_init), hidden_layer_size, hidden_layer_size)\n",
    "            push!(Wss, W)\n",
    "        else\n",
    "            W = Ws[Ws_idx]\n",
    "            Ws_idx += 1\n",
    "        end\n",
    "        out = fc(out, hidden_layer_size, hidden_layer_size, W)\n",
    "        Relu!(out)\n",
    "        push!(hidden_outs, deepcopy(out))\n",
    "    end\n",
    "    # layer out\n",
    "    if Ws == nothing\n",
    "        W = rand(Uniform(-para_init, para_init), hidden_layer_size, out_channel)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(out, hidden_layer_size, out_channel, W)\n",
    "    if Ws == nothing\n",
    "        return out, hidden_outs, Wss\n",
    "    end\n",
    "    return out, hidden_outs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loss function: soft-max loss\n",
    "Soft-max loss is a combination of a soft-max activation layer and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred in only 1 set of prediction in a batch. Ys in only 1 set of truth label in a batch.\n",
    "function softmax_loss(Y_pred, Ys)\n",
    "    # softmax\n",
    "    temp_sum = 0.0\n",
    "    for x in Y_pred\n",
    "        temp_sum += exp(x)\n",
    "    end\n",
    "    l = length(Y_pred)\n",
    "    scores = zeros(l)\n",
    "    for i = 1:l\n",
    "        scores[i] = exp(Y_pred[i])/temp_sum\n",
    "    end\n",
    "    # loss\n",
    "    losses = zeros(l)\n",
    "    for i = 1:l\n",
    "        losses[i] = -Ys[i] * log(scores[i])\n",
    "    end\n",
    "    total_loss = sum(losses)\n",
    "    \n",
    "    return scores, losses, total_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many threads we have for cpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a back-propagation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇ (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ws[idx of layer][idx of node, idx of weights]\n",
    "function ∇(Xs, Ys, Ws, scores, outs, hidden_outs)\n",
    "    num_in_channel = size(Xs)[1]\n",
    "    num_out_channel = size(Ys)[1]\n",
    "    hidden_layer_size = size(hidden_outs[1])[1]\n",
    "    num_hidden_layers = size(hidden_outs)[1]\n",
    "    # step -2\n",
    "    ∂loss_∂netout = zeros(num_out_channel)\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] = -Ys[i]/scores[i]\n",
    "    end\n",
    "    # println(\"step -2: $∂loss_∂netout\")\n",
    "    # step -1\n",
    "    outs_exp_sum = 0.0\n",
    "    for out in outs\n",
    "        outs_exp_sum += exp(out)\n",
    "    end\n",
    "    # println(\"step -1: $outs_exp_sum\")\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] *= (exp(outs[i])*(outs_exp_sum - exp(outs[i])))/(outs_exp_sum^2)\n",
    "    end\n",
    "    # println(\"step -1: $∂loss_∂netout\")\n",
    "    # Assume we use same hidden_size of all hidden layers for the sake for brevity\n",
    "    ∇s = []\n",
    "    # first layer\n",
    "    push!(∇s, zeros(num_in_channel, hidden_layer_size))\n",
    "    # hidden layer\n",
    "    for i = 1 : num_hidden_layers-1\n",
    "        push!(∇s, zeros(hidden_layer_size, hidden_layer_size))\n",
    "    end\n",
    "    # last layer\n",
    "    push!(∇s, zeros(hidden_layer_size, num_out_channel))\n",
    "\n",
    "    # Start back_propagations\n",
    "    ∂loss_∂prev_nodes = zeros(hidden_layer_size)\n",
    "    # layer weights from outs to last of hidden layer\n",
    "    for i = 1 : hidden_layer_size\n",
    "        ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[num_hidden_layers][i],\n",
    "        num_hidden_layers+1, i, ∇s,\n",
    "        Ws[num_hidden_layers+1][i, :], ∂loss_∂netout)\n",
    "    end\n",
    "    # layer weights from i-th hidden layer to i-1th hidden layer\n",
    "    for k = num_hidden_layers : -1 : 2\n",
    "        ∂loss_∂curr_nodes = deepcopy(∂loss_∂prev_nodes)\n",
    "        for i = 1 : hidden_layer_size\n",
    "            ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[k-1][i],\n",
    "            k, i, ∇s,\n",
    "            Ws[k][i, :], ∂loss_∂curr_nodes)\n",
    "        end\n",
    "    end\n",
    "    # layer weights from 1st hidden layer to input layer\n",
    "    for i = 1 : num_in_channel\n",
    "        back_propagation(Xs[i],\n",
    "        1, i, ∇s,\n",
    "        Ws[1][i, :], ∂loss_∂prev_nodes)\n",
    "    end\n",
    "    return ∇s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagation (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function back_propagation(node_out, layer_idx, i, ∇s, weights, ∂loss_∂nodes)\n",
    "    ∂loss_∂node_i = (node_out==0) ? 0 : dot(weights, ∂loss_∂nodes) # considering d of ReLU\n",
    "    ∇s[layer_idx][i, :] = ∂loss_∂nodes*node_out\n",
    "    return ∂loss_∂node_i\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_step(optimizer, Xs_batch, Ys_batch, Ws_wrapped; hidden_layer_size = 64)\n",
    "    b_s = size(Xs_batch)[2]\n",
    "    t = typeof(optimizer)\n",
    "    # b_s = 33\n",
    "    cache_new_Ws_unwrapped = zeros(b_s, num_Ws)\n",
    "    total_losses = zeros(b_s)\n",
    "    Ws_unwrapped = _unwrap(Ws_wrapped)\n",
    "    optimizer.k = optimizer.k + 1\n",
    "    if t <: LimitedMemoryBFGS\n",
    "        m = length(optimizer.δs)\n",
    "        δs, γs, qs = deepcopy(optimizer.δs), deepcopy(optimizer.γs), deepcopy(optimizer.qs)\n",
    "    end\n",
    "    Threads.@threads for i in 1:b_s-1\n",
    "        Xs = Xs_batch[:, i]\n",
    "        Y_truth = one_hot(Ys_batch[i])\n",
    "        #forward\n",
    "        Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "        scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "        total_losses[i] = total_loss\n",
    "        #backward\n",
    "        if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "            cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "        else\n",
    "            gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "            if t <: ConjugateGD\n",
    "                cache_new_Ws_unwrapped[i,:] = \n",
    "                step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss)\n",
    "            elseif t <: LimitedMemoryBFGS\n",
    "                cache_new_Ws_unwrapped[i,:] = \n",
    "                step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss, m,\n",
    "                deepcopy(δs), deepcopy(γs), deepcopy(qs))\n",
    "            elseif t <: DFP\n",
    "                cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped,\n",
    "                total_loss, Xs, Y_truth)\n",
    "            else\n",
    "                cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "            end\n",
    "        end\n",
    "        # if i==1\n",
    "        #     println()\n",
    "        #     println(Y_pred)\n",
    "        #     println(scores)\n",
    "        #     println(Y_truth)\n",
    "        #     println()\n",
    "        # end\n",
    "    end\n",
    "    i = b_s\n",
    "    Xs = Xs_batch[:, i]\n",
    "    Y_truth = one_hot(Ys_batch[i])\n",
    "    #forward\n",
    "    Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "    total_losses[i] = total_loss\n",
    "    #backward\n",
    "    gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs)\n",
    "    if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "        cache_new_Ws_unwrapped[i,:] = step!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "    else\n",
    "        gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "        if t <: ConjugateGD\n",
    "            cache_new_Ws_unwrapped[i,:] = \n",
    "            step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss)\n",
    "        elseif t <: LimitedMemoryBFGS\n",
    "            cache_new_Ws_unwrapped[i,:] = \n",
    "            step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss, length(optimizer.δs),\n",
    "            optimizer.δs, optimizer.γs, optimizer.qs)\n",
    "        elseif t <: DFP\n",
    "            cache_new_Ws_unwrapped[i,:] = step!(optimizer, _unwrap(gradients), Ws_unwrapped,\n",
    "            total_loss, Xs, Y_truth)\n",
    "        else\n",
    "            cache_new_Ws_unwrapped[i,:] = step!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #update parameters\n",
    "    # println(cache_new_Ws_unwrapped[:,1])\n",
    "    # println(mean(cache_new_Ws_unwrapped[1]))\n",
    "    # println(mean(cache_new_Ws_unwrapped[2]))\n",
    "    # println(mean(cache_new_Ws_unwrapped[3]))\n",
    "    # println(mean(cache_new_Ws_unwrapped[4]))\n",
    "    # println(mean(cache_new_Ws_unwrapped[5]))\n",
    "    new_Ws_unwrapped = mean(cache_new_Ws_unwrapped, dims=1)\n",
    "    # println(new_Ws_unwrapped[1])\n",
    "    # println(\" loss = $(mean(total_losses))\")\n",
    "    # println(\" mean new para = $(mean(new_Ws_unwrapped))\")\n",
    "    # println(mean(new_Ws_unwrapped))\n",
    "    # println()\n",
    "    return _wrap(new_Ws_unwrapped, params_shape), mean(total_losses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function epoch_step(optimizer, new_Ws_wrapped; hidden_layer_size = 64)\n",
    "    ct_batch = 1\n",
    "    curr_loss = 0\n",
    "    for (xs, ys) in dl_train\n",
    "        print(\"batch# $ct_batch --> \")\n",
    "        ct_batch += 1\n",
    "        new_Ws_wrapped, curr_loss = batch_step(optimizer, xs, ys, new_Ws_wrapped; hidden_layer_size = hidden_layer_size)\n",
    "        flush(stdout)\n",
    "    end\n",
    "    println(\"\\n\\ttrain loss =  $curr_loss\")\n",
    "    return new_Ws_wrapped, curr_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to make inputs compatible with optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_wrap (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _unwrap(wrapped)\n",
    "    unwrapped = []\n",
    "    for layer in wrapped\n",
    "        unwrapped = vcat(unwrapped, reduce(vcat,layer))\n",
    "    end\n",
    "    return unwrapped\n",
    "end\n",
    "function _wrap(unwrapped, shape)\n",
    "    wrapped = []\n",
    "    curr_idx = 1\n",
    "    for s in shape\n",
    "        ct = s[1]*s[2]\n",
    "        push!(wrapped, Float64.(reshape(unwrapped[curr_idx:curr_idx-1+ct], s)))\n",
    "        curr_idx += ct\n",
    "    end\n",
    "    return wrapped\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple and fast line search algorithm by David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line_search (generic function with 1 method)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function line_search(f, g, x, curr_loss, train_x, train_y; α=0.05, p=0.5, β=1e-4, loop_limit = 20)\n",
    "    # d = -g\n",
    "    d = -g/norm(g)\n",
    "    new_x = x + α*d\n",
    "    loss_new, dumb1, dumb2, dumb3 = f(train_x, train_y, new_x) \n",
    "    ct = 0\n",
    "    while loss_new > curr_loss + β*α*(g⋅d) && ct <= loop_limit\n",
    "        α *= p\n",
    "        new_x = x + α*d\n",
    "        loss_new, hidden_outs, Y_pred, scores = f(train_x, train_y, new_x)\n",
    "        ct += 1\n",
    "    end\n",
    "    # println(\"\\t here α = $α\")\n",
    "    return new_x, scores, Y_pred, hidden_outs, loss_new\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function line_serach(F, d, x, train_x, train_y; r=0.5,c=1e-4,nmax=20)\n",
    "    \n",
    "#     # params\n",
    "#     # F: function to be optimized\n",
    "#     # x: variable\n",
    "#     # d: direction\n",
    "#     # r: factor by which to reduce step size at each iteration\n",
    "#     # c: parameter [0,1]\n",
    "#     # nmax: max iteration\n",
    "\n",
    "#     # return\n",
    "#     # α step size\n",
    "#     # fk1: function value at new x\n",
    "#     # gkk: gradient at new x\n",
    "\n",
    "#     #https://en.wikipedia.org/wiki/Backtracking_line_search\n",
    "#     α=1\n",
    "\n",
    "#     fk, hidden_outs, Y_pred, scores = F(train_x, train_y, x)\n",
    "#     # println(\"(train_x,train_y,x,scores,Y_pred,hidden_outs)=>\n",
    "#     # [$(size(train_x))],[$(size(train_y))],[$(size(x))],[$(size(scores))],[$(size(Y_pred))],[$(size(hidden_outs))]\")\n",
    "#     gk = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#     # fk,gk=F(x)\n",
    "\n",
    "#     xx=x\n",
    "#     x=x+α*d\n",
    "\n",
    "#     fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "#     gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#     # fk1,gk1=F(x)\n",
    "#     n=1\n",
    "    \n",
    "#     while fk1 > fk+c*α*(gk'*d) && n < nmax\n",
    "#         n = n+1\n",
    "#         α = α*0.5\n",
    "#         x = xx+α*d\n",
    "\n",
    "#         fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "#         gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#         # fk1,gk1=F(x)\n",
    "#     end\n",
    "#     if α == 1\n",
    "#         α = -1\n",
    "#         while fk1>fk+c*α*(gk'*d) && n < nmax\n",
    "#             n=n+1\n",
    "#             α=α*0.5\n",
    "#             x=xx+α*d\n",
    "    \n",
    "#             fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "#             gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#             # fk1,gk1=F(x)\n",
    "#         end\n",
    "#     end\n",
    "#     return α, fk1, gk1\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steepest gradient descent\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct SteepestGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::SteepestGD)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end\n",
    "function step_without_update!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct Adam <: DescentMethod\n",
    "    α # learning rate\n",
    "    γv # decay\n",
    "    γs # decay\n",
    "    ϵ # small value\n",
    "    k # step counter\n",
    "    v # 1st moment estimate\n",
    "    s # 2nd moment estimate\n",
    "end\n",
    "function init!(M::Adam, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "    M.s = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    s, v, g = M.s, M.v, ∇f\n",
    "    M.v = γv*v + (1-γv)*g\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    # M.k = k += 1\n",
    "    v_hat = M.v ./ (1 - γv^k)\n",
    "    s_hat = M.s ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end\n",
    "function step_without_update!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    vv = γv*M.v + (1-γv)*∇f\n",
    "    ss = γs*M.s + (1-γs)*∇f.*∇f\n",
    "    # M.k = k += 1\n",
    "    v_hat = vv ./ (1 - γv^k)\n",
    "    s_hat = ss ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate gradient descent\n",
    "Credit: Yashuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 7 methods)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct ConjugateGD <: DescentMethod\n",
    "    α\n",
    "    g\n",
    "    f\n",
    "    d\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::ConjugateGD)\n",
    "    M.k = 0\n",
    "    M.d = -M.g/sqrt(sum(M.g.^2))\n",
    "end\n",
    "# function step!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step!(M::ConjugateGD, g, x, train_x, train_y, curr_loss)\n",
    "    d_prev, g_prev = M.d, M.g\n",
    "    β = dot(transpose(g), g-g_prev)/dot(transpose(g_prev), g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    d = d/sqrt(sum(d.^2))\n",
    "    x_next, dumb1, dumb2, dumb3, loss_new = line_search(M.f, g, x, curr_loss, train_x, train_y)\n",
    "    # M.α, dumb1, dumb2 = line_serach(M.f, g, x, train_x, train_y)\n",
    "    # x_next = x + M.α*d\n",
    "    M.d, M.g = d, g\n",
    "    return x_next\n",
    "end\n",
    "# function step_without_update!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step_without_update!(M::ConjugateGD, g, x, train_x, train_y, curr_loss)\n",
    "    d_prev, g_prev = M.d, M.g\n",
    "    β = dot(transpose(g), g-g_prev)/dot(transpose(g_prev), g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    d = d/sqrt(sum(d.^2))\n",
    "    x_next, dumb1, dumb2, dumb3, loss_new = line_search(M.f, g, x, curr_loss, train_x, train_y)\n",
    "    # α, dumb1, dumb2 = line_serach(M.f, g, x, train_x, train_y)\n",
    "    # x_next = x + α*d\n",
    "    # M.d, M.g = d, g\n",
    "    return x_next\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Momentum (SGD)\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 4 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From K&W 76\n",
    "mutable struct NesterovMomentum_SGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    β # momentum decay\n",
    "    f\n",
    "    v # momentum\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::NesterovMomentum_SGD, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "end\n",
    "function step!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    M.v = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + M.v\n",
    "end\n",
    "function step_without_update!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    vv = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + vv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adadelta\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 5 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: K&W page 80\n",
    "mutable struct Adadelta <: DescentMethod\n",
    "    γs # gradient decay\n",
    "    γx # update decay\n",
    "    ϵ # small value\n",
    "    s # sum of squared gradients\n",
    "    u # sum of squared updates\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::Adadelta, x_length)\n",
    "    M.k = 0\n",
    "    M.s = zeros(x_length)\n",
    "    M.u = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adadelta, g, x)\n",
    "    γs, γx, ϵ, s, u = M.γs, M.γx, M.ϵ, M.s, M.u\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    Δx = - (sqrt.(u) .+ ϵ) ./ (sqrt.(M.s) .+ ϵ) .* g\n",
    "    M.u = γx*u + (1-γx)*Δx.*Δx\n",
    "    return x + Δx\n",
    "end\n",
    "function step_without_update!(M::Adadelta, g, x)\n",
    "    γs, γx, ϵ, s, u = M.γs, M.γx, M.ϵ, M.s, M.u\n",
    "    ss = γs*s + (1-γs)*g.*g\n",
    "    Δx = - (sqrt.(u) .+ ϵ) ./ (sqrt.(ss) .+ ϵ) .* g\n",
    "    # u[:] = γx*u + (1-γx)*Δx.*Δx\n",
    "    return x + Δx\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nelder-Mead\n",
    "Credit: Yashuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 6 methods)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct Nelder_Mead <: DescentMethod\n",
    "    f\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::Nelder_Mead)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::Nelder_Mead, x, train_x, train_y)\n",
    "    return nmsmax(M.f, x, train_x, train_y)\n",
    "end\n",
    "function step_without_update!(M::Nelder_Mead, x, train_x, train_y)\n",
    "    return nmsmax(M.f, x, train_x, train_y)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nmsmax (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nmsmax(fun, x, train_x, train_y; trace = true, initial_simplex = 0, target_f = Inf, max_its = Inf, max_evals = Inf, tol = 1e-3 )\n",
    "    x0 = x[:];  # Work with column vector internally.\n",
    "    n = length(x0);\n",
    "\n",
    "   #  V = [zeros(n,1) eye(n)];\n",
    "    V = [zeros(n,1) Matrix(1.0I, n, n)];\n",
    "    f = zeros(n+1,1);\n",
    "    V[:,1] = x0; \n",
    "    \n",
    "    f[1], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "   #  f[1] = fun(x);\n",
    "\n",
    "    \n",
    "    fmax_old = f[1];\n",
    "    fmax     = -Inf; # Some initial value\n",
    "\n",
    "   #  if trace\n",
    "   #      @printf \"f(x0) = %9.4e\\n\" f[1]\n",
    "   #  end\n",
    "\n",
    "    k = 0; m = 0;\n",
    "\n",
    "    # Set up initial simplex.\n",
    "    scale = max(norm(x0,Inf),1);\n",
    "    if initial_simplex == 0\n",
    "       # Regular simplex - all edges have same length.\n",
    "       # Generated from construction given in reference [18, pp. 80-81] of [1].\n",
    "       alpha = scale / (n*sqrt(2)) * [ sqrt(n+1)-1+n  sqrt(n+1)-1 ];\n",
    "       V[:,2:n+1] = (x0 + alpha[2]*ones(n,1)) * ones(1,n);\n",
    "       for j=2:n+1\n",
    "           V[j-1,j] = x0[j-1] + alpha[1];\n",
    "           x[:] = V[:,j]; \n",
    "\n",
    "           f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "         #   f[j] = fun(x);\n",
    "       end\n",
    "    else\n",
    "       # Right-angled simplex based on co-ordinate axes.\n",
    "       alpha = scale*ones(n+1,1);\n",
    "       for j=2:n+1\n",
    "           V[:,j] = x0 + alpha[j]*V[:,j];\n",
    "           x[:] = V[:,j]; \n",
    "           f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "         #   f[j] = fun(x);\n",
    "       end\n",
    "    end\n",
    "    nf = n+1;\n",
    "    how = \"initial  \";\n",
    "\n",
    "    j = sortperm(f[:]);\n",
    "    temp = f[j];\n",
    "    j = j[n+1:-1:1];\n",
    "    f = f[j]; V = V[:,j];\n",
    "\n",
    "    alpha = 1;  beta = 1/2;  gamma = 2;\n",
    "\n",
    "    msg = \"\"\n",
    "\n",
    "    while true    ###### Outer (and only) loop.\n",
    "    k = k+1;\n",
    "\n",
    "        fmax = f[1];\n",
    "      #   if fmax > fmax_old\n",
    "      #       if trace\n",
    "      #          @printf \"Iter. %2.0f,\" k\n",
    "      #          print(string(\"  how = \", how, \" \"));\n",
    "      #          @printf \"nf = %3.0f,  f = %9.4e  (%2.1f%%)\\n\" nf fmax 100*(fmax-fmax_old)/(abs(fmax_old)+eps(fmax_old));\n",
    "      #       end\n",
    "      #   end\n",
    "        fmax_old = fmax;\n",
    "\n",
    "        ### Three stopping tests from MDSMAX.M\n",
    "\n",
    "        # Stopping Test 1 - f reached target value?\n",
    "        if fmax >= target_f\n",
    "           msg = \"Exceeded target...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 2 - too many f-evals?\n",
    "        if nf >= max_evals\n",
    "           msg = \"Max no. of function evaluations exceeded...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 3 - too many iterations?\n",
    "        if k > max_its\n",
    "           msg = \"Max no. of iterations exceeded...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 4 - converged?   This is test (4.3) in [1].\n",
    "        v1 = V[:,1];\n",
    "        size_simplex = norm(V[:,2:n+1]-v1[:,ones(Int,n)],1) / max(1, norm(v1,1));\n",
    "        if size_simplex <= tol\n",
    "         #   msg = @sprintf(\"Simplex size %9.4e <= %9.4e...quitting\\n\", size_simplex, tol)\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        #  One step of the Nelder-Mead simplex algorithm\n",
    "        #  NJH: Altered function calls and changed CNT to NF.\n",
    "        #       Changed each `fr < f[1]' type test to `>' for maximization\n",
    "        #       and re-ordered function values after sort.\n",
    "\n",
    "        temp = sum(V[:,1:n]'; dims = 1)\n",
    "        vbar = (temp/n)';  # Mean value\n",
    "        vr = (1 + alpha)*vbar - alpha*V[:,n+1]; x[:] = vr; \n",
    "        \n",
    "        fr, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "      #   fr = fun(x);\n",
    "\n",
    "        nf = nf + 1;\n",
    "        vk = vr;  fk = fr; how = \"reflect, \";\n",
    "        if fr > f[n]\n",
    "                if fr > f[1]\n",
    "                   ve = gamma*vr + (1-gamma)*vbar; x[:] = ve; \n",
    "\n",
    "                   fe, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                  #  fe = fun(x);\n",
    "\n",
    "                   nf = nf + 1;\n",
    "                   if fe > f[1]\n",
    "                      vk = ve; fk = fe;\n",
    "                      how = \"expand,  \";\n",
    "                   end\n",
    "                end\n",
    "        else\n",
    "                vt = V[:,n+1]; ft = f[n+1];\n",
    "                if fr > ft\n",
    "                   vt = vr;  ft = fr;\n",
    "                end\n",
    "                vc = beta*vt + (1-beta)*vbar; x[:] = vc; \n",
    "                \n",
    "                fc, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "               #  fc = fun(x);\n",
    "\n",
    "                nf = nf + 1;\n",
    "                if fc > f[n]\n",
    "                   vk = vc; fk = fc;\n",
    "                   how = \"contract,\";\n",
    "                else\n",
    "                   for j = 2:n\n",
    "                       V[:,j] = (V[:,1] + V[:,j])/2;\n",
    "                       x[:] = V[:,j]; \n",
    "                       \n",
    "                       f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                     #   f[j] = fun(x);\n",
    "                   end\n",
    "                   nf = nf + n-1;\n",
    "                   vk = (V[:,1] + V[:,n+1])/2; x[:] = vk; \n",
    "\n",
    "                   fk, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                  #  fk = fun(x);\n",
    "\n",
    "                   nf = nf + 1;\n",
    "                   how = \"shrink,  \";\n",
    "                end\n",
    "        end\n",
    "        V[:,n+1] = vk;\n",
    "        f[n+1] = fk;\n",
    "        j = sortperm(f[:]);\n",
    "        temp = f[j];\n",
    "        j = j[n+1:-1:1];\n",
    "        f = f[j]; V = V[:,j];\n",
    "\n",
    "    end   ###### End of outer (and only) loop.\n",
    "\n",
    "    # Finished.\n",
    "   #  if trace\n",
    "   #      print(msg)\n",
    "   #  end\n",
    "    x[:] = V[:,1];\n",
    "\n",
    "   #  return x, fmax, nf, k-1\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davidon-Fletcher-Powel gradient descent(DFP)\n",
    "Credit: Jinghua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 11 methods)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K&W 93\n",
    "mutable struct DFP <: DescentMethod\n",
    "    Q\n",
    "    f\n",
    "    k # dumb var\n",
    "end\n",
    "function init!(M::DFP, x_length)\n",
    "    M.k = 0\n",
    "    M.Q = Matrix(1.0I, x_length, x_length)\n",
    "end\n",
    "function step!(M::DFP, g, x, curr_loss, train_x, train_y)\n",
    "    Q = M.Q\n",
    "    x′, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, M.Q*g, x, curr_loss, train_x, train_y; α=0.25)\n",
    "    # x′ = line_search(f, x, -Q*g)\n",
    "    g′ = _unwrap(∇(train_x, train_y, _wrap(x′, params_shape), scores, Y_pred, hidden_outs))\n",
    "    # g′ = ∇f(x′)\n",
    "    δ = x′ - x\n",
    "    γ = g′ - g\n",
    "    M.Q = Q - Q*γ*γ'*Q/(γ'*Q*γ) + δ*δ'/(δ'*γ)\n",
    "    return x′\n",
    "end\n",
    "function step_without_update!(M::DFP, g, x, curr_loss, train_x, train_y)\n",
    "    x′, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, M.Q*g, x, curr_loss, train_x, train_y; α=0.25)\n",
    "    return x′\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limited memory Broyden-Fletcher-Goldfarb-Shanno gradient descent(LBFGS)\n",
    "Credit: David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 9 methods)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct LimitedMemoryBFGS <: DescentMethod\n",
    "    m\n",
    "    f\n",
    "    δs\n",
    "    γs\n",
    "    qs\n",
    "    k # dumb var\n",
    "end\n",
    "function init!(M::LimitedMemoryBFGS)\n",
    "    M.k = 0\n",
    "    M.δs = []\n",
    "    M.γs = []\n",
    "    M.qs = []\n",
    "end\n",
    "function step!(M::LimitedMemoryBFGS, g, x, train_x, train_y, curr_loss, l, δs, γs, qs)\n",
    "    sleep(0.02)\n",
    "    while l > M.m+1\n",
    "        popfirst!(M.δs); popfirst!(M.γs); popfirst!(M.qs)\n",
    "        l -= 1\n",
    "    end\n",
    "    x_new = step_without_update!(M, g, x, train_x, train_y, curr_loss, l, δs, γs, qs)\n",
    "    return x_new\n",
    "end\n",
    "function step_without_update!(M::LimitedMemoryBFGS, g, x, train_x, train_y, curr_loss, m, δs, γs, qs)\n",
    "    m = min(M.m, m)\n",
    "    # println(m)\n",
    "    g′ = g\n",
    "    # m = length(δs)\n",
    "    if m > 0\n",
    "        q = g\n",
    "        for i in m : -1 : 1\n",
    "            qs[i] = q\n",
    "            q -= (δs[i]⋅q)/(γs[i]⋅δs[i])*γs[i]\n",
    "        end\n",
    "        z = (γs[m] .* δs[m] .* q) / (γs[m]⋅γs[m])\n",
    "        for i in 1 : m\n",
    "            z += δs[i]*(δs[i]⋅qs[i] - γs[i]⋅z)/(γs[i]⋅δs[i])\n",
    "        end\n",
    "        x_neww, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, z, x, curr_loss, train_x, train_y; α=1)\n",
    "        if loss_new < curr_loss\n",
    "            x_new = x_neww\n",
    "        else\n",
    "            x_new, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, g, x, curr_loss, train_x, train_y; α=0.05)\n",
    "        end\n",
    "        g′ = _unwrap(∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs))\n",
    "        # x_new = line_search(f, x, -z)\n",
    "    else\n",
    "        x_new, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, g, x, curr_loss, train_x, train_y; α=0.05)\n",
    "        g′ = _unwrap(∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs))\n",
    "        # x_new = line_search(f, x, -g)\n",
    "    end\n",
    "    # dumb1, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    # g′ = ∇f(x_new)\n",
    "\n",
    "    push!(M.qs, zeros(length(x)))\n",
    "    push!(M.δs, x_new - x)\n",
    "    push!(M.γs, g′ - g)\n",
    "    return x_new\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutable struct LBFGS <: DescentMethod\n",
    "# \tn # number of variables\n",
    "# \tf\n",
    "\n",
    "#     m # Memory length, was ∈ [2, 54] in paper\n",
    "# \tprev_g # gradient at previous timestep \n",
    "# \tprev_x # x at previous timestep \n",
    "# \tSm # previous m x's\n",
    "# \tYm # previous m gradients\n",
    "# \tk # Internal iteration index\n",
    "# end\n",
    "\n",
    "# function init!(M::LBFGS)\n",
    "#     M.m = 20 \n",
    "#     M.prev_g = zeros(M.n)\n",
    "#     M.prev_x = zeros(M.n)\n",
    "#     M.Sm = zeros(M.n,M.m)\n",
    "#     M.Ym = zeros(M.n,M.m)\n",
    "#     M.k = 0 \n",
    "# end\n",
    "# function step!(o::LBFGS, ∇, x, train_x, train_y)\n",
    "#     # o.k += 1\n",
    "# \tm = o.m\n",
    "\t\n",
    "# \tgnorm = norm(∇)\n",
    "\t\n",
    "# \t# if gnorm < τgrad # tolerance for the norm of the slope \n",
    "# \t# \treturn; \n",
    "# \t# end\n",
    "\t\n",
    "# \ts0 = x-o.prev_x\n",
    "# \ty0 = ∇-o.prev_g\n",
    "\t\n",
    "# \t# println(\"y0=$y0\")\n",
    "# \tH0 = s0'*y0/(y0'*y0) # hessian diagonal satisfying secant condition\n",
    "    \n",
    "#     k = o.k\n",
    "# \t# update Sm and Ym\n",
    "# \tif k <= m\n",
    "# \t\to.Sm[:,k].=s0\n",
    "# \t\to.Ym[:,k].=y0\n",
    "# \t\tp=-approxInvHess(∇,o.Sm[:,1:k],o.Ym[:,1:k],H0) \n",
    "# \t# only keep m entries in Sm and Ym so purge the old ones\n",
    "\t\t\n",
    "# \telse\n",
    "# \t\to.Sm[:,1:(m-1)].=o.Sm[:,2:m]\n",
    "# \t\to.Ym[:,1:(m-1)].=o.Sm[:,2:m]\n",
    "# \t\to.Sm[:,m].=s0\n",
    "# \t\to.Ym[:,m].=y0\n",
    "# \t\tp.=-approxInvHess(∇,o.Sm,o.Ym,H0)\n",
    "# \tend\n",
    "\t\n",
    "# \t# new direction=p, find new step size\n",
    "#     # α = 0.01\n",
    "# \tα, fs, gs=line_serach(o.f, p, x, train_x, train_y)\n",
    "\t\n",
    "# \t# update for next iteration\n",
    "# \to.prev_x = x\n",
    "# \to.prev_g = ∇\n",
    "# \tx .= x + α.*p\n",
    "#     return x\n",
    "# \t# f1=fs\n",
    "# \t# g1=gs\n",
    "# \t# k=k+1\n",
    "\t\n",
    "# \t# if verbose == 1 \n",
    "# \t# \tprintln(\"Iteration: $k -- x = $x1\")\n",
    "# \t# end\n",
    "# end\n",
    "# function step_without_update!(o::LBFGS, ∇, x, train_x, train_y)\n",
    "#     Smm = deepcopy(o.Sm)\n",
    "#     Ymm = deepcopy(o.Ym)\n",
    "\n",
    "#     # o.k += 1\n",
    "# \tm = o.m\n",
    "\t\n",
    "# \tgnorm = norm(∇)\n",
    "\t\n",
    "# \t# if gnorm < τgrad # tolerance for the norm of the slope \n",
    "# \t# \treturn; \n",
    "# \t# end\n",
    "\t\n",
    "# \ts0 = x-o.prev_x\n",
    "# \ty0 = ∇-o.prev_g\n",
    "\t\n",
    "# \t# println(\"y0=$y0\")\n",
    "# \tH0 = s0'*y0/(y0'*y0) # hessian diagonal satisfying secant condition\n",
    "\n",
    "#     k = o.k\n",
    "# \t# update Sm and Ym\n",
    "# \tif k <= m\n",
    "# \t\tSmm[:,k].=s0\n",
    "# \t\tYmm[:,k].=y0\n",
    "# \t\tp=-approxInvHess(∇,Smm[:,1:k],Ymm[:,1:k],H0) \n",
    "# \t# only keep m entries in Sm and Ym so purge the old ones\n",
    "\t\t\n",
    "# \telse\n",
    "# \t\tSmm[:,1:(m-1)].=Smm[:,2:m]\n",
    "# \t\tYmm[:,1:(m-1)].=Smm[:,2:m]\n",
    "# \t\tSmm[:,m].=s0\n",
    "# \t\tYmm[:,m].=y0\n",
    "# \t\tp.=-approxInvHess(∇,Smm,Ymm,H0)\n",
    "# \tend\n",
    "\t\n",
    "# \t# new direction=p, find new step size\n",
    "#     # α = 0.01\n",
    "# \tα, fs, gs=line_serach(o.f, p, x, train_x, train_y)\n",
    "\t\n",
    "# \t# update for next iteration\n",
    "# \t# o.prev_x = x\n",
    "# \t# o.prev_g = ∇\n",
    "# \tx .= x + α.*p\n",
    "#     return x\n",
    "# \t# f1=fs\n",
    "# \t# g1=gs\n",
    "# \t# k=k+1\n",
    "\t\n",
    "# \t# if verbose == 1 \n",
    "# \t# \tprintln(\"Iteration: $k -- x = $x1\")\n",
    "# \t# end\n",
    "# end\n",
    "\n",
    "# function approxInvHess(g,S,Y,H0)\n",
    "#     #INPUT\n",
    "\n",
    "#     #g: gradient nx1 vector\n",
    "#     #S: nxk matrixs storing S[i]=x[i+1]-x[i]\n",
    "#     #Y: nxk matrixs storing Y[i]=g[i+1]-g[i]\n",
    "#     #H0: initial hessian diagnol scalar\n",
    "\n",
    "#     #OUTPUT\n",
    "#     # p:  the approximate inverse hessian multiplied by the gradient g\n",
    "#     #     which is the new direction\n",
    "#     #notation follows:\n",
    "#     #https://en.wikipedia.org/wiki/Limited-memory_BFGS\n",
    "\n",
    "#     n,k=size(S)\n",
    "#     rho=zeros(k)\n",
    "#     for i=1:k\n",
    "#         rho[i] = 1 /(Y[:,i]'*S[:,i])\n",
    "#         if rho[i]<0\n",
    "#             rho[i]=-rho[i]\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "\n",
    "#     q=zeros(n,k+1)\n",
    "#     r=zeros(n,1)\n",
    "#     α=zeros(k,1)\n",
    "#     β=zeros(k,1)\n",
    "\n",
    "#     q[:,k+1]=g\n",
    "\n",
    "#     for i=k:-1:1\n",
    "#         α[i] =rho[i]*S[:,i]'*q[:,i+1]\n",
    "#         q[:,i].=q[:,i+1]-α[i]*Y[:,i]\n",
    "#     end\n",
    "\n",
    "#     z=zeros(size(q[:,1])[1])\n",
    "#     # println(size(H0))\n",
    "#     # println()\n",
    "#     # println(size(H0*q[:,1]))\n",
    "#     z.= H0*q[:,1]\n",
    "\n",
    "\n",
    "#     for i=1:k\n",
    "#         β[i] = rho[i]*Y[:,i]'*z\n",
    "#         z.=z+S[:,i]*(α[i]-β[i])\n",
    "#     end\n",
    "\n",
    "#     p=copy(z)\n",
    "\n",
    "#     return p\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control variable for a scientifically correct comparison: **Give every optimizers a same NN to train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created!\n",
      "Inital loss = 4.221411682348338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ε = 0.001 # stop when loss <= loss_ε\n",
    "# Initializing \n",
    "Y_pred, hidden_outs, Ws_copy = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "# println(Y_pred)\n",
    "scores, losses, total_loss = softmax_loss(Y_pred, train_y_data[1])\n",
    "∇_copy = ∇(train_x_data[1], train_y_data[1], Ws_copy, scores, Y_pred, hidden_outs);\n",
    "println(\"New model created!\\nInital loss = $total_loss\")\n",
    "params_shape = []\n",
    "num_Ws = 0\n",
    "for layer in Ws_copy\n",
    "    # println(size(layer))\n",
    "    s1, s2 = size(layer)\n",
    "    num_Ws += s1 * s2\n",
    "    push!(params_shape, size(layer))\n",
    "end\n",
    "function f(train_x, train_y, w_unwrapped)\n",
    "    Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "    return total_loss, hidden_outs, Y_pred, scores\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Steepest gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Steepest_GD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_SteepestGD = SteepestGD(0.01, nothing)\n",
    "init!(optimizer_SteepestGD)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "\tbatch# 1 --> -0.4039592652087397\n",
      "-0.4039592652087397\n",
      "-0.4039592652087397\n",
      "-0.4039592652087397\n",
      "-0.4039592652087397\n",
      "0.018172660185809354\n",
      "\n",
      "batch# 2 --> -0.4039592652087394\n",
      "-0.4039592652087394\n",
      "-0.4039592652087394\n",
      "-0.4039592652087394\n",
      "-0.4039592652087394\n",
      "0.0182190045827225\n",
      "\n",
      "batch# 3 --> -0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "0.01875221858742239\n",
      "\n",
      "batch# 4 --> -0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "0.019845463607220247\n",
      "\n",
      "batch# 5 --> -0.40395926520873915\n",
      "-0.40395926520873915\n",
      "-0.40395926520873915\n",
      "-0.40395926520873915\n",
      "-0.40395926520873915\n",
      "0.021141036797893795\n",
      "\n",
      "\n",
      "\ttrain loss =  1.29530290574206\n",
      "Epoch# 2\n",
      "\tbatch# 1 --> -0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "0.022185185085921426\n",
      "\n",
      "batch# 2 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02355762569093872\n",
      "\n",
      "batch# 3 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02553436253340399\n",
      "\n",
      "batch# 4 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.028477988928405546\n",
      "\n",
      "batch# 5 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.028541954365640027\n",
      "\n",
      "\n",
      "\ttrain loss =  0.012589430352074481\n",
      "Epoch# 3\n",
      "\tbatch# 1 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03149468078446577\n",
      "\n",
      "batch# 2 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.0359785989973713\n",
      "\n",
      "batch# 3 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.0359806599655242\n",
      "\n",
      "batch# 4 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.0362176129499191\n",
      "\n",
      "batch# 5 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03621870969689282\n",
      "\n",
      "\n",
      "\ttrain loss =  0.00013853873494362983\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Steepest_GD\n",
    "    print(\"Epoch# $i\\n\\t\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_SteepestGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adam = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adam = Adam(0.01, 0.9, 0.999, 1e-7, nothing, nothing, nothing)\n",
    "init!(optimizer_Adam, num_Ws);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> -0.4039592652087397\n",
      "-0.4039592652087397\n",
      "-0.4039592652087397\n",
      "-0.4039592652087397\n",
      "-0.4039592652087397\n",
      "0.02020029212076652\n",
      "\n",
      "batch# 2 --> -0.4039592652087394\n",
      "-0.4039592652087394\n",
      "-0.4039592652087394\n",
      "-0.4039592652087394\n",
      "-0.4039592652087394\n",
      "0.020234983213546717\n",
      "\n",
      "batch# 3 --> -0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "-0.40395926520873926\n",
      "0.020151731808728656\n",
      "\n",
      "batch# 4 --> -0.40395926520873915\n",
      "-0.40395926520873915\n",
      "-0.40395926520873915\n",
      "-0.40395926520873915\n",
      "-0.40395926520873915\n",
      "0.02018139980220326\n",
      "\n",
      "batch# 5 --> -0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "0.02040244445881287\n",
      "\n",
      "\n",
      "\ttrain loss =  5.492742770148451\n",
      "Epoch# 2\n",
      "batch# 1 --> -0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "-0.40395926520873904\n",
      "0.02065562965620115\n",
      "\n",
      "batch# 2 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.021061695716125713\n",
      "\n",
      "batch# 3 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02179312938418818\n",
      "\n",
      "batch# 4 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02263003777411841\n",
      "\n",
      "batch# 5 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02347029349786147\n",
      "\n",
      "\n",
      "\ttrain loss =  2.280655376075914\n",
      "Epoch# 3\n",
      "batch# 1 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02432762700294628\n",
      "\n",
      "batch# 2 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02537191808117556\n",
      "\n",
      "batch# 3 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02647101959693609\n",
      "\n",
      "batch# 4 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.02764107342275519\n",
      "\n",
      "batch# 5 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.028765062588852135\n",
      "\n",
      "\n",
      "\ttrain loss =  0.12186062467825487\n",
      "Epoch# 4\n",
      "batch# 1 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.030012880881115613\n",
      "\n",
      "batch# 2 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.0310356740241017\n",
      "\n",
      "batch# 3 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03219158567150305\n",
      "\n",
      "batch# 4 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.0330559295221333\n",
      "\n",
      "batch# 5 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03384940658065751\n",
      "\n",
      "\n",
      "\ttrain loss =  0.18878314374557315\n",
      "Epoch# 5\n",
      "batch# 1 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03482754699330212\n",
      "\n",
      "batch# 2 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03600190332959394\n",
      "\n",
      "batch# 3 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.037108720231999605\n",
      "\n",
      "batch# 4 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.03788032590213666\n",
      "\n",
      "batch# 5 --> -0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "-0.403959265208739\n",
      "0.038958093934975485\n",
      "\n",
      "\n",
      "\ttrain loss =  0.002863977709828663\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Adam\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adam, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = ConjugateGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch_ConjugateGD = 20;\n",
    "# function f(train_x, train_y, w_unwrapped)\n",
    "#     Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "#     scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "#     return total_loss, hidden_outs, Y_pred, scores\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "# optimizer_ConjugateGD = ConjugateGD(0.01, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "optimizer_ConjugateGD = ConjugateGD(0.01, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "init!(optimizer_ConjugateGD);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  8.076402459846074\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  7.976756049423995\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  7.2666144845121865\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  6.663586835971843\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  6.380534975169033\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  5.710111476736678\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  4.965581449203234\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  3.430782934603087\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  2.3243355239357384\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7951947904234816\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.12725990508932797\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.15815871202759654\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.2851213085170745\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6145104424134525\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  7.329463425124297e-12\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_ConjugateGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_ConjugateGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = NesterovMomentum_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_NesterovMomentum_SGD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_NesterovMomentum_SGD = NesterovMomentum_SGD(0.01, 0.99, f, nothing, nothing)\n",
    "init!(optimizer_NesterovMomentum_SGD, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.25194512925344076\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  NaN\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_NesterovMomentum_SGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_NesterovMomentum_SGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adadelta = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adadelta = Adadelta(0.95, 0.95, 1e-3, nothing, nothing, nothing)\n",
    "init!(optimizer_Adadelta, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  5.6139681319177654\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  2.7567381048215425\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.18265272037526006\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.14066581129820765\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  7.594660287781993e-5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Adadelta\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adadelta, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Nelder-Mead\n",
    "\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***NOT WORKING**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Nelder_Mead = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Nelder_Mead = Nelder_Mead(f, nothing)\n",
    "init!(optimizer_Nelder_Mead)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# for i = 1:num_epoch_Nelder_Mead\n",
    "#     println(\"Epoch# $i\")\n",
    "#     Ws, curr_loss = epoch_step(optimizer_Nelder_Mead, Ws; hidden_layer_size = 16)\n",
    "#     if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "#         break\n",
    "#     end\n",
    "# end\n",
    "# println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = DFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_DFP = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_LimitedMemoryDFP = DFP(nothing, f, nothing)\n",
    "init!(optimizer_LimitedMemoryDFP, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  13.578740171338644\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  9.15963558839599\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  7.970562149527849\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  18.54572407179537\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  33.96093833822887\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  8.503789283155388\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  3.6360308532852814\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  9.64495520574083\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  2.381222499043595\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.3478694480968594\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.008048444390121857\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.02048962394984767\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.025022907759692275\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.0010045439860830565\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.00012859453184061405\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_DFP\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_LimitedMemoryDFP, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_LimitedMemoryBFGS = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_LimitedMemoryBFGS = LimitedMemoryBFGS(20, f, nothing, nothing, nothing, nothing)\n",
    "init!(optimizer_LimitedMemoryBFGS)\n",
    "Ws = deepcopy(Ws_copy);\n",
    "# # Initializing \n",
    "# optimizer_LBFGS = LBFGS(num_Ws, f, nothing, nothing, nothing, nothing, nothing, nothing)\n",
    "# init!(optimizer_LBFGS)\n",
    "# Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  6.106546168371511\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  9.532932206053392\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  9.025868150671995\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  41.173119188819236\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  41.455387276249326\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  15.603596643638724\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  3.055875394359737\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.19223749808419804\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5093617460839075\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  6.879981029751543e-8\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_LimitedMemoryBFGS\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_LimitedMemoryBFGS, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (auto threads) 1.7.2",
   "language": "julia",
   "name": "julia-(auto-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
