{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `g:\\桌面\\2022 Fall\\cs268\\final_proj\\Project-Optimizer`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n",
    "# Pkg.add(\"DataLoaders\")\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"CUDA\")\n",
    "# Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataLoaders\n",
    "using Plots\n",
    "using CUDA\n",
    "using Distributions \n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom ML structure to apply custom optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "The Iris dataset is from UC Irvine Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot(y)\n",
    "    rslt = zeros(3)\n",
    "    rslt[trunc(Int, y) + 1] = 1.0\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all lines of data in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "train_x_data = []\n",
    "train_y_data = []\n",
    "# 8:2\n",
    "# test_x_data = []\n",
    "# test_y_data = []\n",
    "for line in eachline(\"iris.txt\")\n",
    "    splitted = collect(split(line, \" \"))\n",
    "    push!(train_x_data, [parse(Float64, splitted[4]), parse(Float64, splitted[7]), parse(Float64, splitted[10]), parse(Float64, splitted[13])])\n",
    "    push!(train_y_data, one_hot(parse(Float64, splitted[16])))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect shape of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148,)(148,)"
     ]
    }
   ],
   "source": [
    "print(size(train_x_data))\n",
    "print(size(train_y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoaders.GetObsParallel{DataLoaders.BatchViewCollated{Tuple{Vector{Any}, Vector{Any}}}}(batchviewcollated() with 5 batches of size 33, false)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 33\n",
    "dl_train = DataLoader((train_x_data, train_y_data), batch_size)\n",
    "# dl_test = DataLoader((test_x, test_y), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create whole machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an activation function: ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relu! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Relu!(Xs)\n",
    "    for i = 1 : length(Xs)\n",
    "        Xs[i] = max(0.0,Xs[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fc (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function fc(Xs, num_in_channel, num_out_channel, W)\n",
    "    rslt = zeros(num_out_channel)\n",
    "    Xs = reshape(Xs, 1, num_in_channel)\n",
    "    b = rand()\n",
    "    for i = 1:num_out_channel\n",
    "        temp = reshape(W[:, i], num_in_channel, 1)\n",
    "        rslt[i] = (Xs * temp)[1]+b\n",
    "    end\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function MLP(Xs, out_channel; num_hidden_layers = 0, hidden_layer_size = 64, Ws = nothing)\n",
    "    Ws_idx = 1\n",
    "    hidden_outs = []\n",
    "    # layer in\n",
    "    if Ws == nothing\n",
    "        Wss = []\n",
    "        W = rand(Uniform(-0.1, 0.1), size(Xs)[1], hidden_layer_size)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(Xs, size(Xs)[1], hidden_layer_size, W)\n",
    "    Relu!(out)\n",
    "    push!(hidden_outs, deepcopy(out))\n",
    "    # layer hidden\n",
    "    for i = 1 : num_hidden_layers\n",
    "        if Ws == nothing\n",
    "            W = rand(Uniform(-0.1, 0.1), hidden_layer_size, hidden_layer_size)\n",
    "            push!(Wss, W)\n",
    "        else\n",
    "            W = Ws[Ws_idx]\n",
    "            Ws_idx += 1\n",
    "        end\n",
    "        out = fc(out, hidden_layer_size, hidden_layer_size, W)\n",
    "        Relu!(out)\n",
    "        push!(hidden_outs, deepcopy(out))\n",
    "    end\n",
    "    # layer out\n",
    "    if Ws == nothing\n",
    "        W = rand(Uniform(-0.1, 0.1), hidden_layer_size, out_channel)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(out, hidden_layer_size, out_channel, W)\n",
    "    if Ws == nothing\n",
    "        return out, hidden_outs, Wss\n",
    "    end\n",
    "    return out, hidden_outs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loss function: soft-max loss\n",
    "Soft-max loss is a combination of a soft-max activation layer and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred in only 1 set of prediction in a batch. Ys in only 1 set of truth label in a batch.\n",
    "function softmax_loss(Y_pred, Ys)\n",
    "    # softmax\n",
    "    temp_sum = 0.0\n",
    "    for x in Y_pred\n",
    "        temp_sum += exp(x)\n",
    "    end\n",
    "    l = length(Y_pred)\n",
    "    scores = zeros(l)\n",
    "    for i = 1:l\n",
    "        scores[i] = exp(Y_pred[i])/temp_sum\n",
    "    end\n",
    "    # loss\n",
    "    losses = zeros(l)\n",
    "    for i = 1:l\n",
    "        losses[i] = -Ys[i] * log(scores[i])\n",
    "    end\n",
    "    total_loss = sum(losses)\n",
    "    \n",
    "    return scores, losses, total_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many threads we have for cpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a back-propagation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇ (generic function with 1 method)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ws[idx of layer][idx of node, idx of weights]\n",
    "function ∇(Xs, Ys, Ws, scores, outs, hidden_outs)\n",
    "    num_in_channel = size(Xs)[1]\n",
    "    num_out_channel = size(Ys)[1]\n",
    "    hidden_layer_size = size(hidden_outs[1])[1]\n",
    "    num_hidden_layers = size(hidden_outs)[1]\n",
    "    # step -2\n",
    "    ∂loss_∂netout = zeros(num_out_channel)\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] = -Ys[i]/scores[i]\n",
    "    end\n",
    "    # println(\"step -2: $∂loss_∂netout\")\n",
    "    # step -1\n",
    "    outs_exp_sum = 0.0\n",
    "    for out in outs\n",
    "        outs_exp_sum += exp(out)\n",
    "    end\n",
    "    # println(\"step -1: $outs_exp_sum\")\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] *= (exp(outs[i])*(outs_exp_sum - exp(outs[i])))/(outs_exp_sum^2)\n",
    "    end\n",
    "    # println(\"step -1: $∂loss_∂netout\")\n",
    "    # Assume we use same hidden_size of all hidden layers for the sake for brevity\n",
    "    ∇s = []\n",
    "    # first layer\n",
    "    push!(∇s, zeros(num_in_channel, hidden_layer_size))\n",
    "    # hidden layer\n",
    "    for i = 1 : num_hidden_layers-1\n",
    "        push!(∇s, zeros(hidden_layer_size, hidden_layer_size))\n",
    "    end\n",
    "    # last layer\n",
    "    push!(∇s, zeros(hidden_layer_size, num_out_channel))\n",
    "\n",
    "    # Start back_propagations\n",
    "    ∂loss_∂prev_nodes = zeros(hidden_layer_size)\n",
    "    # layer weights from outs to last of hidden layer\n",
    "    for i = 1 : hidden_layer_size\n",
    "        ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[num_hidden_layers][i],\n",
    "        num_hidden_layers+1, i, ∇s,\n",
    "        Ws[num_hidden_layers+1][i, :], ∂loss_∂netout)\n",
    "    end\n",
    "    # layer weights from i-th hidden layer to i-1th hidden layer\n",
    "    for k = num_hidden_layers : -1 : 2\n",
    "        ∂loss_∂curr_nodes = deepcopy(∂loss_∂prev_nodes)\n",
    "        for i = 1 : hidden_layer_size\n",
    "            ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[k-1][i],\n",
    "            k, i, ∇s,\n",
    "            Ws[k][i, :], ∂loss_∂curr_nodes)\n",
    "        end\n",
    "    end\n",
    "    # layer weights from 1st hidden layer to input layer\n",
    "    for i = 1 : num_in_channel\n",
    "        back_propagation(Xs[i],\n",
    "        1, i, ∇s,\n",
    "        Ws[1][i, :], ∂loss_∂prev_nodes)\n",
    "    end\n",
    "    return ∇s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagation (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function back_propagation(node_out, layer_idx, i, ∇s, weights, ∂loss_∂nodes)\n",
    "    ∂loss_∂node_i = (node_out==0) ? 0 : dot(weights, ∂loss_∂nodes) # considering d of ReLU\n",
    "    ∇s[layer_idx][i, :] = ∂loss_∂nodes*node_out\n",
    "    return ∂loss_∂node_i\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_step(optimizer, Xs_batch, Ys_batch, Ws_wrapped; hidden_layer_size = 64)\n",
    "    b_s = size(Xs_batch)[2]\n",
    "    t = typeof(optimizer)\n",
    "    # b_s = 33\n",
    "    cache_new_Ws_unwrapped = zeros(b_s, num_Ws)\n",
    "    total_losses = zeros(b_s)\n",
    "    Ws_unwrapped = _unwrap(Ws_wrapped)\n",
    "    optimizer.k = optimizer.k + 1\n",
    "    Threads.@threads for i in 1:b_s-1\n",
    "        Xs = Xs_batch[:, i]\n",
    "        Y_truth = one_hot(Ys_batch[i])\n",
    "        #forward\n",
    "        Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "        scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "        total_losses[i] = total_loss\n",
    "        #backward\n",
    "        if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "            cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "        else\n",
    "            gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "            if t <: LBFGS || t <: ConjugateGD\n",
    "                cache_new_Ws_unwrapped[i,:] = \n",
    "                step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth)\n",
    "            else\n",
    "                cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "            end\n",
    "        end\n",
    "        # if i==1\n",
    "        #     println()\n",
    "        #     println(Y_pred)\n",
    "        #     println(scores)\n",
    "        #     println(Y_truth)\n",
    "        #     println()\n",
    "        # end\n",
    "    end\n",
    "    i = b_s\n",
    "    Xs = Xs_batch[:, i]\n",
    "    Y_truth = one_hot(Ys_batch[i])\n",
    "    #forward\n",
    "    Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "    total_losses[i] = total_loss\n",
    "    #backward\n",
    "    gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "    if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "        cache_new_Ws_unwrapped[i,:] = step!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "    else\n",
    "        gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "        if t <: LBFGS || t <: ConjugateGD\n",
    "            cache_new_Ws_unwrapped[i,:] = \n",
    "            step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth)\n",
    "        else\n",
    "            cache_new_Ws_unwrapped[i,:] = step!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #update parameters\n",
    "    # println(cache_new_Ws_unwrapped[:,1])\n",
    "    new_Ws_unwrapped = mean(cache_new_Ws_unwrapped, dims=1)\n",
    "    # println(new_Ws_unwrapped[1])\n",
    "    # println(\" loss = $(mean(total_losses))\")\n",
    "    # println(\" mean new para = $(mean(new_Ws_unwrapped))\")\n",
    "    return _wrap(new_Ws_unwrapped, params_shape), mean(total_losses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function epoch_step(optimizer, new_Ws_wrapped; hidden_layer_size = 64)\n",
    "    ct_batch = 1\n",
    "    curr_loss = 0\n",
    "    for (xs, ys) in dl_train\n",
    "        print(\"batch# $ct_batch --> \")\n",
    "        ct_batch += 1\n",
    "        new_Ws_wrapped, curr_loss = batch_step(optimizer, xs, ys, new_Ws_wrapped; hidden_layer_size = hidden_layer_size)\n",
    "        flush(stdout)\n",
    "    end\n",
    "    println(\"\\n\\ttrain loss =  $curr_loss\")\n",
    "    return new_Ws_wrapped, curr_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to make inputs compatible with optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_wrap (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _unwrap(wrapped)\n",
    "    unwrapped = []\n",
    "    for layer in wrapped\n",
    "        unwrapped = vcat(unwrapped, reduce(vcat,layer))\n",
    "    end\n",
    "    return unwrapped\n",
    "end\n",
    "function _wrap(unwrapped, shape)\n",
    "    wrapped = []\n",
    "    curr_idx = 1\n",
    "    for s in shape\n",
    "        ct = s[1]*s[2]\n",
    "        push!(wrapped, Float64.(reshape(unwrapped[curr_idx:curr_idx-1+ct], s)))\n",
    "        curr_idx += ct\n",
    "    end\n",
    "    return wrapped\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple and fast line search algorithm by David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line_serach (generic function with 1 method)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function line_serach(F, d, x, train_x, train_y; r=0.5,c=1e-4,nmax=10)\n",
    "    \n",
    "    # params\n",
    "    # F: function to be optimized\n",
    "    # x: variable\n",
    "    # d: direction\n",
    "    # r: factor by which to reduce step size at each iteration\n",
    "    # c: parameter [0,1]\n",
    "    # nmax: max iteration\n",
    "\n",
    "    # return\n",
    "    # α step size\n",
    "    # fk1: function value at new x\n",
    "    # gkk: gradient at new x\n",
    "\n",
    "    #https://en.wikipedia.org/wiki/Backtracking_line_search\n",
    "    α=1\n",
    "\n",
    "    fk, hidden_outs, Y_pred, scores = F(train_x, train_y, x)\n",
    "    # println(\"(train_x,train_y,x,scores,Y_pred,hidden_outs)=>\n",
    "    # [$(size(train_x))],[$(size(train_y))],[$(size(x))],[$(size(scores))],[$(size(Y_pred))],[$(size(hidden_outs))]\")\n",
    "    gk = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "    # fk,gk=F(x)\n",
    "\n",
    "    xx=x\n",
    "    x=x+α*d\n",
    "\n",
    "    fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "    gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "    # fk1,gk1=F(x)\n",
    "    n=1\n",
    "    \n",
    "    while fk1>fk+c*α*(gk'*d) && n < nmax\n",
    "        n=n+1\n",
    "        α=α*0.5\n",
    "        x=xx+α*d\n",
    "\n",
    "        fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "        gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "        # fk1,gk1=F(x)\n",
    "    end\n",
    "    if α == 1\n",
    "        α = -1\n",
    "        while fk1>fk+c*α*(gk'*d) && n < nmax\n",
    "            n=n+1\n",
    "            α=α*0.5\n",
    "            x=xx+α*d\n",
    "    \n",
    "            fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "            gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "            # fk1,gk1=F(x)\n",
    "        end\n",
    "    end\n",
    "    return α, fk1, gk1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steepest gradient descent\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct SteepestGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::SteepestGD)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end\n",
    "function step_without_update!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct Adam <: DescentMethod\n",
    "    α # learning rate\n",
    "    γv # decay\n",
    "    γs # decay\n",
    "    ϵ # small value\n",
    "    k # step counter\n",
    "    v # 1st moment estimate\n",
    "    s # 2nd moment estimate\n",
    "end\n",
    "function init!(M::Adam, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "    M.s = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    s, v, g = M.s, M.v, ∇f\n",
    "    M.v = γv*v + (1-γv)*g\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    # M.k = k += 1\n",
    "    v_hat = M.v ./ (1 - γv^k)\n",
    "    s_hat = M.s ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end\n",
    "function step_without_update!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    vv = γv*M.v + (1-γv)*∇f\n",
    "    ss = γs*M.s + (1-γs)*∇f.*∇f\n",
    "    # M.k = k += 1\n",
    "    v_hat = vv ./ (1 - γv^k)\n",
    "    s_hat = ss ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate gradient descent\n",
    "Credit: Yashuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 8 methods)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct ConjugateGD <: DescentMethod\n",
    "    α\n",
    "    g\n",
    "    f\n",
    "    d\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::ConjugateGD)\n",
    "    M.k = 0\n",
    "    M.d = -M.g/sqrt(sum(M.g.^2))\n",
    "end\n",
    "# function step!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "    d_prev, g_prev = M.d, M.g\n",
    "    β = dot(transpose(g), g-g_prev)/dot(transpose(g_prev), g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    d = d/sqrt(sum(d.^2))\n",
    "    M.α, dumb1, dumb2 = line_serach(M.f, g, x, train_x, train_y)\n",
    "    x_next = x + M.α*d\n",
    "    M.d, M.g = d, g\n",
    "    return x_next\n",
    "end\n",
    "# function step_without_update!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step_without_update!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "    d_prev, g_prev = M.d, M.g\n",
    "    β = dot(transpose(g), g-g_prev)/dot(transpose(g_prev), g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    d = d/sqrt(sum(d.^2))\n",
    "    α, dumb1, dumb2 = line_serach(M.f, g, x, train_x, train_y)\n",
    "    x_next = x + α*d\n",
    "    # M.d, M.g = d, g\n",
    "    return x_next\n",
    "end\n",
    "# # implementing strong backtracking line search\n",
    "# function strong_backtracking_line_search(train_x, train_y, f, x, d, g; α=1, β=1e-4, σ=0.1, p=2)\n",
    "#     if g == zeros(length(g))\n",
    "#         # println(\"******************* g = 0 *******************\")\n",
    "#         return 0.0\n",
    "#     end\n",
    "#     # println(\"\\tStart bracketing phase\")\n",
    "#     # bracketing phase\n",
    "#     prev_α = 0\n",
    "#     f_x, dumb1, dumb2, dumb3 = f(train_x, train_y, x)\n",
    "#     x_αd = x+α*d\n",
    "#     ∇f_x = transpose(g)*d\n",
    "#     f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#     ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#     ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#     condition_a = f_x_αd >= f_x\n",
    "#     condition_b = f_x_αd > f_x + β*α*∇f_x\n",
    "#     condition_c = ∇f_x_αd >= 0\n",
    "#     while !(condition_a || condition_b || condition_c)\n",
    "#         prev_α, α = α, α*p\n",
    "#         # println(\"\\t\\tα = $α\")\n",
    "#         x_αd = x+α*d\n",
    "#         f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#         ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#         ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#         condition_a = f_x_αd >= f_x\n",
    "#         condition_b = f_x_αd > f_x + β*α*∇f_x\n",
    "#         condition_c = ∇f_x_αd >= 0\n",
    "#     end\n",
    "#     # println(\"\\t bracket = [$prev_α, $α]\")\n",
    "#     # println(\"\\tFinish bracketing phase\")\n",
    "#     # println(\"\\tStart bracketing phase\")\n",
    "#     # zoom phase\n",
    "#     # println(\"\\t\\tStart sufficient decrease condition\")\n",
    "#     # sufficient decrease condition\n",
    "#     sufficient_decrease_condition = !condition_b\n",
    "#     while !sufficient_decrease_condition\n",
    "#         α = (prev_α + α)/2 \n",
    "#         if α == 0.0\n",
    "#             # println(\"******************* α = 0 *******************\")\n",
    "#             return 0.0\n",
    "#         end\n",
    "#         # println(\"\\t\\t\\tα = $α\")\n",
    "#         x_αd = x+α*d\n",
    "#         f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#         sufficient_decrease_condition = f_x_αd <= f_x + β*α*∇f_x\n",
    "#     end\n",
    "#     # println(\"\\t\\tFinish sufficient decrease condition\")\n",
    "#     # strong curvature condition \n",
    "#     # println(\"\\t\\tStart strong curvature condition\")\n",
    "#     ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#     ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#     strong_curvature_condition = abs(∇f_x_αd) <= -σ*∇f_x\n",
    "#     while !strong_curvature_condition\n",
    "#         α = (prev_α + α)/2\n",
    "#         if α == 0.0\n",
    "#             # println(\"******************* α = 0 *******************\")\n",
    "#             return 0.0\n",
    "#         end\n",
    "#         # println(\"\\t\\t\\tα = $α\")\n",
    "#         x_αd = x+α*d\n",
    "#         f_x_αd, hidden_outs, Y_pred, scores = f(train_x, train_y, x_αd) \n",
    "#         ∇_f_x_αd = ∇(train_x, train_y, _wrap(x_αd, params_shape), scores, Y_pred, hidden_outs)\n",
    "#         ∇f_x_αd = transpose(_unwrap(∇_f_x_αd))*d\n",
    "#         strong_curvature_condition = abs(∇f_x_αd) <= -σ*∇f_x\n",
    "#     end\n",
    "#     # println(\"\\t\\tFinish strong curvature condition\")\n",
    "#     # println(\"\\tFinish bracketing phase\")\n",
    "#     return α\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Momentum (SGD)\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 4 methods)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From K&W 76\n",
    "mutable struct NesterovMomentum_SGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    β # momentum decay\n",
    "    f\n",
    "    v # momentum\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::NesterovMomentum_SGD, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "end\n",
    "function step!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    M.v = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + M.v\n",
    "end\n",
    "function step_without_update!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    vv = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + vv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adadelta\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 5 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: K&W page 80\n",
    "mutable struct Adadelta <: DescentMethod\n",
    "    γs # gradient decay\n",
    "    γx # update decay\n",
    "    ϵ # small value\n",
    "    s # sum of squared gradients\n",
    "    u # sum of squared updates\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::Adadelta, x_length)\n",
    "    M.k = 0\n",
    "    M.s = zeros(x_length)\n",
    "    M.u = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adadelta, g, x)\n",
    "    γs, γx, ϵ, s, u = M.γs, M.γx, M.ϵ, M.s, M.u\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    Δx = - (sqrt.(u) .+ ϵ) ./ (sqrt.(M.s) .+ ϵ) .* g\n",
    "    M.u = γx*u + (1-γx)*Δx.*Δx\n",
    "    return x + Δx\n",
    "end\n",
    "function step_without_update!(M::Adadelta, g, x)\n",
    "    γs, γx, ϵ, s, u = M.γs, M.γx, M.ϵ, M.s, M.u\n",
    "    ss = γs*s + (1-γs)*g.*g\n",
    "    Δx = - (sqrt.(u) .+ ϵ) ./ (sqrt.(ss) .+ ϵ) .* g\n",
    "    # u[:] = γx*u + (1-γx)*Δx.*Δx\n",
    "    return x + Δx\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nelder-Mead\n",
    "Credit: Yashuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 6 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct Nelder_Mead <: DescentMethod\n",
    "    f\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::Nelder_Mead)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::Nelder_Mead, x, train_x, train_y)\n",
    "    return nmsmax(M.f, x, train_x, train_y)\n",
    "end\n",
    "function step_without_update!(M::Nelder_Mead, x, train_x, train_y)\n",
    "    return nmsmax(M.f, x, train_x, train_y)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nmsmax (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nmsmax(fun, x, train_x, train_y; trace = true, initial_simplex = 0, target_f = Inf, max_its = Inf, max_evals = Inf, tol = 1e-3 )\n",
    "    x0 = x[:];  # Work with column vector internally.\n",
    "    n = length(x0);\n",
    "\n",
    "   #  V = [zeros(n,1) eye(n)];\n",
    "    V = [zeros(n,1) Matrix(1.0I, n, n)];\n",
    "    f = zeros(n+1,1);\n",
    "    V[:,1] = x0; \n",
    "    \n",
    "    f[1], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "   #  f[1] = fun(x);\n",
    "\n",
    "    \n",
    "    fmax_old = f[1];\n",
    "    fmax     = -Inf; # Some initial value\n",
    "\n",
    "   #  if trace\n",
    "   #      @printf \"f(x0) = %9.4e\\n\" f[1]\n",
    "   #  end\n",
    "\n",
    "    k = 0; m = 0;\n",
    "\n",
    "    # Set up initial simplex.\n",
    "    scale = max(norm(x0,Inf),1);\n",
    "    if initial_simplex == 0\n",
    "       # Regular simplex - all edges have same length.\n",
    "       # Generated from construction given in reference [18, pp. 80-81] of [1].\n",
    "       alpha = scale / (n*sqrt(2)) * [ sqrt(n+1)-1+n  sqrt(n+1)-1 ];\n",
    "       V[:,2:n+1] = (x0 + alpha[2]*ones(n,1)) * ones(1,n);\n",
    "       for j=2:n+1\n",
    "           V[j-1,j] = x0[j-1] + alpha[1];\n",
    "           x[:] = V[:,j]; \n",
    "\n",
    "           f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "         #   f[j] = fun(x);\n",
    "       end\n",
    "    else\n",
    "       # Right-angled simplex based on co-ordinate axes.\n",
    "       alpha = scale*ones(n+1,1);\n",
    "       for j=2:n+1\n",
    "           V[:,j] = x0 + alpha[j]*V[:,j];\n",
    "           x[:] = V[:,j]; \n",
    "           f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "         #   f[j] = fun(x);\n",
    "       end\n",
    "    end\n",
    "    nf = n+1;\n",
    "    how = \"initial  \";\n",
    "\n",
    "    j = sortperm(f[:]);\n",
    "    temp = f[j];\n",
    "    j = j[n+1:-1:1];\n",
    "    f = f[j]; V = V[:,j];\n",
    "\n",
    "    alpha = 1;  beta = 1/2;  gamma = 2;\n",
    "\n",
    "    msg = \"\"\n",
    "\n",
    "    while true    ###### Outer (and only) loop.\n",
    "    k = k+1;\n",
    "\n",
    "        fmax = f[1];\n",
    "      #   if fmax > fmax_old\n",
    "      #       if trace\n",
    "      #          @printf \"Iter. %2.0f,\" k\n",
    "      #          print(string(\"  how = \", how, \" \"));\n",
    "      #          @printf \"nf = %3.0f,  f = %9.4e  (%2.1f%%)\\n\" nf fmax 100*(fmax-fmax_old)/(abs(fmax_old)+eps(fmax_old));\n",
    "      #       end\n",
    "      #   end\n",
    "        fmax_old = fmax;\n",
    "\n",
    "        ### Three stopping tests from MDSMAX.M\n",
    "\n",
    "        # Stopping Test 1 - f reached target value?\n",
    "        if fmax >= target_f\n",
    "           msg = \"Exceeded target...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 2 - too many f-evals?\n",
    "        if nf >= max_evals\n",
    "           msg = \"Max no. of function evaluations exceeded...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 3 - too many iterations?\n",
    "        if k > max_its\n",
    "           msg = \"Max no. of iterations exceeded...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 4 - converged?   This is test (4.3) in [1].\n",
    "        v1 = V[:,1];\n",
    "        size_simplex = norm(V[:,2:n+1]-v1[:,ones(Int,n)],1) / max(1, norm(v1,1));\n",
    "        if size_simplex <= tol\n",
    "         #   msg = @sprintf(\"Simplex size %9.4e <= %9.4e...quitting\\n\", size_simplex, tol)\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        #  One step of the Nelder-Mead simplex algorithm\n",
    "        #  NJH: Altered function calls and changed CNT to NF.\n",
    "        #       Changed each `fr < f[1]' type test to `>' for maximization\n",
    "        #       and re-ordered function values after sort.\n",
    "\n",
    "        temp = sum(V[:,1:n]'; dims = 1)\n",
    "        vbar = (temp/n)';  # Mean value\n",
    "        vr = (1 + alpha)*vbar - alpha*V[:,n+1]; x[:] = vr; \n",
    "        \n",
    "        fr, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "      #   fr = fun(x);\n",
    "\n",
    "        nf = nf + 1;\n",
    "        vk = vr;  fk = fr; how = \"reflect, \";\n",
    "        if fr > f[n]\n",
    "                if fr > f[1]\n",
    "                   ve = gamma*vr + (1-gamma)*vbar; x[:] = ve; \n",
    "\n",
    "                   fe, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                  #  fe = fun(x);\n",
    "\n",
    "                   nf = nf + 1;\n",
    "                   if fe > f[1]\n",
    "                      vk = ve; fk = fe;\n",
    "                      how = \"expand,  \";\n",
    "                   end\n",
    "                end\n",
    "        else\n",
    "                vt = V[:,n+1]; ft = f[n+1];\n",
    "                if fr > ft\n",
    "                   vt = vr;  ft = fr;\n",
    "                end\n",
    "                vc = beta*vt + (1-beta)*vbar; x[:] = vc; \n",
    "                \n",
    "                fc, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "               #  fc = fun(x);\n",
    "\n",
    "                nf = nf + 1;\n",
    "                if fc > f[n]\n",
    "                   vk = vc; fk = fc;\n",
    "                   how = \"contract,\";\n",
    "                else\n",
    "                   for j = 2:n\n",
    "                       V[:,j] = (V[:,1] + V[:,j])/2;\n",
    "                       x[:] = V[:,j]; \n",
    "                       \n",
    "                       f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                     #   f[j] = fun(x);\n",
    "                   end\n",
    "                   nf = nf + n-1;\n",
    "                   vk = (V[:,1] + V[:,n+1])/2; x[:] = vk; \n",
    "\n",
    "                   fk, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                  #  fk = fun(x);\n",
    "\n",
    "                   nf = nf + 1;\n",
    "                   how = \"shrink,  \";\n",
    "                end\n",
    "        end\n",
    "        V[:,n+1] = vk;\n",
    "        f[n+1] = fk;\n",
    "        j = sortperm(f[:]);\n",
    "        temp = f[j];\n",
    "        j = j[n+1:-1:1];\n",
    "        f = f[j]; V = V[:,j];\n",
    "\n",
    "    end   ###### End of outer (and only) loop.\n",
    "\n",
    "    # Finished.\n",
    "   #  if trace\n",
    "   #      print(msg)\n",
    "   #  end\n",
    "    x[:] = V[:,1];\n",
    "\n",
    "   #  return x, fmax, nf, k-1\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LBFGS\n",
    "Credit: David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "approxInvHess (generic function with 1 method)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct LBFGS <: DescentMethod\n",
    "\tn # number of variables\n",
    "\tf\n",
    "\n",
    "    m # Memory length, was ∈ [2, 54] in paper\n",
    "\tprev_g # gradient at previous timestep \n",
    "\tprev_x # x at previous timestep \n",
    "\tSm # previous m x's\n",
    "\tYm # previous m gradients\n",
    "\tk # Internal iteration index\n",
    "end\n",
    "\n",
    "function init!(M::LBFGS)\n",
    "    M.m = 20 \n",
    "    M.prev_g = zeros(M.n)\n",
    "    M.prev_x = zeros(M.n)\n",
    "    M.Sm = zeros(M.n,M.m)\n",
    "    M.Ym = zeros(M.n,M.m)\n",
    "    M.k = 0 \n",
    "end\n",
    "function step!(o::LBFGS, ∇, x, train_x, train_y)\n",
    "    # o.k += 1\n",
    "\tm = o.m\n",
    "\t\n",
    "\tgnorm = norm(∇)\n",
    "\t\n",
    "\t# if gnorm < τgrad # tolerance for the norm of the slope \n",
    "\t# \treturn; \n",
    "\t# end\n",
    "\t\n",
    "\ts0 = x-o.prev_x\n",
    "\ty0 = ∇-o.prev_g\n",
    "\t\n",
    "\t# println(\"y0=$y0\")\n",
    "\tH0 = s0'*y0/(y0'*y0) # hessian diagonal satisfying secant condition\n",
    "    \n",
    "    k = o.k\n",
    "\t# update Sm and Ym\n",
    "\tif k <= m\n",
    "\t\to.Sm[:,k].=s0\n",
    "\t\to.Ym[:,k].=y0\n",
    "\t\tp=-approxInvHess(∇,o.Sm[:,1:k],o.Ym[:,1:k],H0) \n",
    "\t# only keep m entries in Sm and Ym so purge the old ones\n",
    "\t\t\n",
    "\telse\n",
    "\t\to.Sm[:,1:(m-1)].=o.Sm[:,2:m]\n",
    "\t\to.Ym[:,1:(m-1)].=o.Sm[:,2:m]\n",
    "\t\to.Sm[:,m].=s0\n",
    "\t\to.Ym[:,m].=y0\n",
    "\t\tp.=-approxInvHess(∇,o.Sm,o.Ym,H0)\n",
    "\tend\n",
    "\t\n",
    "\t# new direction=p, find new step size\n",
    "    # α = 0.01\n",
    "\tα, fs, gs=line_serach(o.f, p, x, train_x, train_y)\n",
    "\t\n",
    "\t# update for next iteration\n",
    "\to.prev_x = x\n",
    "\to.prev_g = ∇\n",
    "\tx .= x + α.*p\n",
    "    return x\n",
    "\t# f1=fs\n",
    "\t# g1=gs\n",
    "\t# k=k+1\n",
    "\t\n",
    "\t# if verbose == 1 \n",
    "\t# \tprintln(\"Iteration: $k -- x = $x1\")\n",
    "\t# end\n",
    "end\n",
    "function step_without_update!(o::LBFGS, ∇, x, train_x, train_y)\n",
    "    Smm = deepcopy(o.Sm)\n",
    "    Ymm = deepcopy(o.Ym)\n",
    "\n",
    "    # o.k += 1\n",
    "\tm = o.m\n",
    "\t\n",
    "\tgnorm = norm(∇)\n",
    "\t\n",
    "\t# if gnorm < τgrad # tolerance for the norm of the slope \n",
    "\t# \treturn; \n",
    "\t# end\n",
    "\t\n",
    "\ts0 = x-o.prev_x\n",
    "\ty0 = ∇-o.prev_g\n",
    "\t\n",
    "\t# println(\"y0=$y0\")\n",
    "\tH0 = s0'*y0/(y0'*y0) # hessian diagonal satisfying secant condition\n",
    "\n",
    "    k = o.k\n",
    "\t# update Sm and Ym\n",
    "\tif k <= m\n",
    "\t\tSmm[:,k].=s0\n",
    "\t\tYmm[:,k].=y0\n",
    "\t\tp=-approxInvHess(∇,Smm[:,1:k],Ymm[:,1:k],H0) \n",
    "\t# only keep m entries in Sm and Ym so purge the old ones\n",
    "\t\t\n",
    "\telse\n",
    "\t\tSmm[:,1:(m-1)].=Smm[:,2:m]\n",
    "\t\tYmm[:,1:(m-1)].=Smm[:,2:m]\n",
    "\t\tSmm[:,m].=s0\n",
    "\t\tYmm[:,m].=y0\n",
    "\t\tp.=-approxInvHess(∇,Smm,Ymm,H0)\n",
    "\tend\n",
    "\t\n",
    "\t# new direction=p, find new step size\n",
    "    # α = 0.01\n",
    "\tα, fs, gs=line_serach(o.f, p, x, train_x, train_y)\n",
    "\t\n",
    "\t# update for next iteration\n",
    "\t# o.prev_x = x\n",
    "\t# o.prev_g = ∇\n",
    "\tx .= x + α.*p\n",
    "    return x\n",
    "\t# f1=fs\n",
    "\t# g1=gs\n",
    "\t# k=k+1\n",
    "\t\n",
    "\t# if verbose == 1 \n",
    "\t# \tprintln(\"Iteration: $k -- x = $x1\")\n",
    "\t# end\n",
    "end\n",
    "\n",
    "function approxInvHess(g,S,Y,H0)\n",
    "    #INPUT\n",
    "\n",
    "    #g: gradient nx1 vector\n",
    "    #S: nxk matrixs storing S[i]=x[i+1]-x[i]\n",
    "    #Y: nxk matrixs storing Y[i]=g[i+1]-g[i]\n",
    "    #H0: initial hessian diagnol scalar\n",
    "\n",
    "    #OUTPUT\n",
    "    # p:  the approximate inverse hessian multiplied by the gradient g\n",
    "    #     which is the new direction\n",
    "    #notation follows:\n",
    "    #https://en.wikipedia.org/wiki/Limited-memory_BFGS\n",
    "\n",
    "    n,k=size(S)\n",
    "    rho=zeros(k)\n",
    "    for i=1:k\n",
    "        rho[i] = 1 /(Y[:,i]'*S[:,i])\n",
    "        if rho[i]<0\n",
    "            rho[i]=-rho[i]\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    q=zeros(n,k+1)\n",
    "    r=zeros(n,1)\n",
    "    α=zeros(k,1)\n",
    "    β=zeros(k,1)\n",
    "\n",
    "    q[:,k+1]=g\n",
    "\n",
    "    for i=k:-1:1\n",
    "        α[i] =rho[i]*S[:,i]'*q[:,i+1]\n",
    "        q[:,i].=q[:,i+1]-α[i]*Y[:,i]\n",
    "    end\n",
    "\n",
    "    z=zeros(size(q[:,1])[1])\n",
    "    # println(size(H0))\n",
    "    # println()\n",
    "    # println(size(H0*q[:,1]))\n",
    "    z.= H0*q[:,1]\n",
    "\n",
    "\n",
    "    for i=1:k\n",
    "        β[i] = rho[i]*Y[:,i]'*z\n",
    "        z.=z+S[:,i]*(α[i]-β[i])\n",
    "    end\n",
    "\n",
    "    p=copy(z)\n",
    "\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control variable for a scientifically correct comparison: **Give every optimizers a same NN to train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created!\n",
      "Inital loss = 1.0677200789081591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ε = 0.01 # stop when loss <= loss_ε\n",
    "# Initializing \n",
    "Y_pred, hidden_outs, Ws_copy = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "# println(Y_pred)\n",
    "scores, losses, total_loss = softmax_loss(Y_pred, train_y_data[1])\n",
    "∇_copy = ∇(train_x_data[1], train_y_data[1], Ws_copy, scores, Y_pred, hidden_outs);\n",
    "println(\"New model created!\\nInital loss = $total_loss\")\n",
    "params_shape = []\n",
    "num_Ws = 0\n",
    "for layer in Ws_copy\n",
    "    # println(size(layer))\n",
    "    s1, s2 = size(layer)\n",
    "    num_Ws += s1 * s2\n",
    "    push!(params_shape, size(layer))\n",
    "end\n",
    "function f(train_x, train_y, w_unwrapped)\n",
    "    Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "    return total_loss, hidden_outs, Y_pred, scores\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Steepest gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Steepest_GD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_SteepestGD = SteepestGD(0.01, nothing)\n",
    "init!(optimizer_SteepestGD)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0524875483449176\n",
      "Epoch# 2\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0085472899505192\n",
      "Epoch# 3\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.03351415011928\n",
      "Epoch# 4\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.964298680455558\n",
      "Epoch# 5\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9292304985643729\n",
      "Epoch# 6\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8800571011375639\n",
      "Epoch# 7\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8572806283191442\n",
      "Epoch# 8\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8548702865114243\n",
      "Epoch# 9\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8803701640133363\n",
      "Epoch# 10\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7549363231725236\n",
      "Epoch# 11\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7457468594041161\n",
      "Epoch# 12\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7291856190210241\n",
      "Epoch# 13\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7163288377206827\n",
      "Epoch# 14\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7223318957535758\n",
      "Epoch# 15\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7097181425028362\n",
      "Epoch# 16\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6265487421965829\n",
      "Epoch# 17\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6049690764745917\n",
      "Epoch# 18\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5590821201595595\n",
      "Epoch# 19\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.6125028799178036\n",
      "Epoch# 20\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5117418033302187\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Steepest_GD\n",
    "    print(\"Epoch# $i\\n\\t\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_SteepestGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adam = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adam = Adam(0.01, 0.9, 0.999, 1e-7, nothing, nothing, nothing)\n",
    "init!(optimizer_Adam, num_Ws);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7489066868963872\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.290768542840515\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.015261841793336985\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  7.658779896820299e-5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Adam\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adam, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = ConjugateGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch_ConjugateGD = 20;\n",
    "# function f(train_x, train_y, w_unwrapped)\n",
    "#     Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "#     scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "#     return total_loss, hidden_outs, Y_pred, scores\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "# optimizer_ConjugateGD = ConjugateGD(0.01, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "optimizer_ConjugateGD = ConjugateGD(0.01, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "init!(optimizer_ConjugateGD);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0238170966475377\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0178325729449416\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0027438252035015\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9745267630733193\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8093880158735735\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.7089836926357035\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.5772751448216842\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.32247722613946095\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.06871508478345457\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.05852380183956063\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.07451688870355576\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.13286031802997716\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.08360425920353674\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.07677787136564673\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.037798235954260066\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.1891329917146274\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.1581707245645614\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.07630259087903507\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.23064976644105528\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.05434176076497208\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_ConjugateGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_ConjugateGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = NesterovMomentum_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_NesterovMomentum_SGD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_NesterovMomentum_SGD = NesterovMomentum_SGD(0.01, 0.99, f, nothing, nothing)\n",
    "init!(optimizer_NesterovMomentum_SGD, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.9909591715471994\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.2350562847293356\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.37973243651744487\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.1836650524263877\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.0031698707054810273\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_NesterovMomentum_SGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_NesterovMomentum_SGD, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adadelta = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adadelta = Adadelta(0.95, 0.95, 1e-3, nothing, nothing, nothing)\n",
    "init!(optimizer_Adadelta, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  1.0289471623730455\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.8914206181512585\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.17906318944518004\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  0.005917156067241449\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_Adadelta\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adadelta, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Nelder-Mead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Nelder_Mead = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Nelder_Mead = Nelder_Mead(f, nothing)\n",
    "init!(optimizer_Nelder_Mead)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# for i = 1:num_epoch_Nelder_Mead\n",
    "#     println(\"Epoch# $i\")\n",
    "#     Ws, curr_loss = epoch_step(optimizer_Nelder_Mead, Ws; hidden_layer_size = 16)\n",
    "#     if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "#         break\n",
    "#     end\n",
    "# end\n",
    "# println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_LBFGS = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_LBFGS = LBFGS(num_Ws, f, nothing, nothing, nothing, nothing, nothing, nothing)\n",
    "init!(optimizer_LBFGS)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> \n",
      "\ttrain loss =  NaN\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_LBFGS\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_LBFGS, Ws; hidden_layer_size = 16)\n",
    "    if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (auto threads) 1.7.2",
   "language": "julia",
   "name": "julia-(auto-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
