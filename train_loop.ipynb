{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `g:\\桌面\\2022 Fall\\cs268\\final_proj\\Project-Optimizer`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n",
    "# Pkg.add(\"DataLoaders\")\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"CUDA\")\n",
    "# Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataLoaders\n",
    "using Plots\n",
    "using CUDA\n",
    "using Distributions \n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom ML structure to apply custom optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "The Iris dataset is from UC Irvine Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot(y)\n",
    "    rslt = zeros(3)\n",
    "    rslt[trunc(Int, y) + 1] = 1.0\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all lines of data in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "features = [[],[],[],[]]\n",
    "means = zeros(4)\n",
    "stds = zeros(4)\n",
    "for line in eachline(\"iris.txt\")\n",
    "    splitted = collect(split(line, \" \"))\n",
    "    push!(features[1], parse(Float64, splitted[4]))\n",
    "    push!(features[2], parse(Float64, splitted[7]))\n",
    "    push!(features[3], parse(Float64, splitted[10]))\n",
    "    push!(features[4], parse(Float64, splitted[13]))\n",
    "end\n",
    "for i = 1:4\n",
    "    means[i] = mean(features[i])\n",
    "    stds[i] = stdm(features[i], means[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "train_x_data = []\n",
    "train_y_data = []\n",
    "# 118, 30\n",
    "test_x_data = []\n",
    "test_y_data = []\n",
    "ct = zeros(3)\n",
    "for line in eachline(\"iris.txt\")\n",
    "    splitted = collect(split(line, \" \"))\n",
    "    y = parse(Float64, splitted[16])\n",
    "    if ct[trunc(Int, y)+1] < 8 && rand() <= 0.3\n",
    "        ct[trunc(Int, y)+1] += 1\n",
    "        push!(test_x_data, \n",
    "            [(parse(Float64, splitted[4]) - means[1])/stds[1], \n",
    "            (parse(Float64, splitted[7]) - means[2])/stds[2], \n",
    "            (parse(Float64, splitted[10]) - means[3])/stds[3], \n",
    "            (parse(Float64, splitted[13]) - means[4])/stds[4]])\n",
    "        push!(test_y_data, one_hot(y))\n",
    "    else\n",
    "        push!(train_x_data, \n",
    "            [(parse(Float64, splitted[4]) - means[1])/stds[1], \n",
    "            (parse(Float64, splitted[7]) - means[2])/stds[2], \n",
    "            (parse(Float64, splitted[10]) - means[3])/stds[3], \n",
    "            (parse(Float64, splitted[13]) - means[4])/stds[4]])\n",
    "        push!(train_y_data, one_hot(y))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect shape of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124,)(124,)(24,)(24,)"
     ]
    }
   ],
   "source": [
    "print(size(train_x_data))\n",
    "print(size(train_y_data))\n",
    "print(size(test_x_data))\n",
    "print(size(test_y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoaders.GetObsParallel{DataLoaders.BatchViewCollated{Tuple{Vector{Any}, Vector{Any}}}}(batchviewcollated() with 24 batches of size 1, false)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dl_train = DataLoader((train_x_data, train_y_data), batch_size)\n",
    "dl_test = DataLoader((test_x_data, test_y_data), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create whole machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an activation function: ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relu! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Relu!(Xs)\n",
    "    for i = 1 : length(Xs)\n",
    "        Xs[i] = max(0.0,Xs[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fc (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function fc(Xs, num_in_channel, num_out_channel, W)\n",
    "    rslt = zeros(num_out_channel)\n",
    "    Xs = reshape(Xs, 1, num_in_channel)\n",
    "    b = rand()\n",
    "    for i = 1:num_out_channel\n",
    "        temp = reshape(W[:, i], num_in_channel, 1)\n",
    "        rslt[i] = (Xs * temp)[1]+b\n",
    "    end\n",
    "    return rslt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs in only 1 set of input in a batch.\n",
    "function MLP(Xs, out_channel; num_hidden_layers = 0, hidden_layer_size = 64, Ws = nothing, para_init=1)\n",
    "    Ws_idx = 1\n",
    "    hidden_outs = []\n",
    "    # layer in\n",
    "    if Ws == nothing\n",
    "        Wss = []\n",
    "        W = rand(Uniform(-para_init, para_init), size(Xs)[1], hidden_layer_size)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(Xs, size(Xs)[1], hidden_layer_size, W)\n",
    "    Relu!(out)\n",
    "    push!(hidden_outs, deepcopy(out))\n",
    "    # layer hidden\n",
    "    for i = 1 : num_hidden_layers\n",
    "        if Ws == nothing\n",
    "            W = rand(Uniform(-para_init, para_init), hidden_layer_size, hidden_layer_size)\n",
    "            push!(Wss, W)\n",
    "        else\n",
    "            W = Ws[Ws_idx]\n",
    "            Ws_idx += 1\n",
    "        end\n",
    "        out = fc(out, hidden_layer_size, hidden_layer_size, W)\n",
    "        Relu!(out)\n",
    "        push!(hidden_outs, deepcopy(out))\n",
    "    end\n",
    "    # layer out\n",
    "    if Ws == nothing\n",
    "        W = rand(Uniform(-para_init, para_init), hidden_layer_size, out_channel)\n",
    "        push!(Wss, W)\n",
    "    else\n",
    "        W = Ws[Ws_idx]\n",
    "        Ws_idx += 1\n",
    "    end\n",
    "    out = fc(out, hidden_layer_size, out_channel, W)\n",
    "    if Ws == nothing\n",
    "        return out, hidden_outs, Wss\n",
    "    end\n",
    "    return out, hidden_outs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loss function: soft-max loss\n",
    "Soft-max loss is a combination of a soft-max activation layer and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_pred in only 1 set of prediction in a batch. Ys in only 1 set of truth label in a batch.\n",
    "function softmax_loss(Y_pred, Ys)\n",
    "    # softmax\n",
    "    temp_sum = 0.0\n",
    "    for x in Y_pred\n",
    "        temp_sum += exp(x)\n",
    "    end\n",
    "    l = length(Y_pred)\n",
    "    scores = zeros(l)\n",
    "    for i = 1:l\n",
    "        scores[i] = exp(Y_pred[i])/temp_sum\n",
    "    end\n",
    "    # loss\n",
    "    losses = zeros(l)\n",
    "    for i = 1:l\n",
    "        losses[i] = -Ys[i] * log(scores[i])\n",
    "    end\n",
    "    total_loss = sum(losses)\n",
    "    \n",
    "    return scores, losses, total_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many threads we have for cpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a back-propagation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇ (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ws[idx of layer][idx of node, idx of weights]\n",
    "function ∇(Xs, Ys, Ws, scores, outs, hidden_outs)\n",
    "    num_in_channel = size(Xs)[1]\n",
    "    num_out_channel = size(Ys)[1]\n",
    "    hidden_layer_size = size(hidden_outs[1])[1]\n",
    "    num_hidden_layers = size(hidden_outs)[1]\n",
    "    # step -2\n",
    "    ∂loss_∂netout = zeros(num_out_channel)\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] = -Ys[i]/scores[i]\n",
    "    end\n",
    "    # println(\"step -2: $∂loss_∂netout\")\n",
    "    # step -1\n",
    "    outs_exp_sum = 0.0\n",
    "    for out in outs\n",
    "        outs_exp_sum += exp(out)\n",
    "    end\n",
    "    # println(\"step -1: $outs_exp_sum\")\n",
    "    for i = 1:num_out_channel\n",
    "        ∂loss_∂netout[i] *= (exp(outs[i])*(outs_exp_sum - exp(outs[i])))/(outs_exp_sum^2)\n",
    "    end\n",
    "    # println(\"step -1: $∂loss_∂netout\")\n",
    "    # Assume we use same hidden_size of all hidden layers for the sake for brevity\n",
    "    ∇s = []\n",
    "    # first layer\n",
    "    push!(∇s, zeros(num_in_channel, hidden_layer_size))\n",
    "    # hidden layer\n",
    "    for i = 1 : num_hidden_layers-1\n",
    "        push!(∇s, zeros(hidden_layer_size, hidden_layer_size))\n",
    "    end\n",
    "    # last layer\n",
    "    push!(∇s, zeros(hidden_layer_size, num_out_channel))\n",
    "\n",
    "    # Start back_propagations\n",
    "    ∂loss_∂prev_nodes = zeros(hidden_layer_size)\n",
    "    # layer weights from outs to last of hidden layer\n",
    "    for i = 1 : hidden_layer_size\n",
    "        ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[num_hidden_layers][i],\n",
    "        num_hidden_layers+1, i, ∇s,\n",
    "        Ws[num_hidden_layers+1][i, :], ∂loss_∂netout)\n",
    "    end\n",
    "    # layer weights from i-th hidden layer to i-1th hidden layer\n",
    "    for k = num_hidden_layers : -1 : 2\n",
    "        ∂loss_∂curr_nodes = deepcopy(∂loss_∂prev_nodes)\n",
    "        for i = 1 : hidden_layer_size\n",
    "            ∂loss_∂prev_nodes[i] = back_propagation(hidden_outs[k-1][i],\n",
    "            k, i, ∇s,\n",
    "            Ws[k][i, :], ∂loss_∂curr_nodes)\n",
    "        end\n",
    "    end\n",
    "    # layer weights from 1st hidden layer to input layer\n",
    "    for i = 1 : num_in_channel\n",
    "        back_propagation(Xs[i],\n",
    "        1, i, ∇s,\n",
    "        Ws[1][i, :], ∂loss_∂prev_nodes)\n",
    "    end\n",
    "    return ∇s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagation (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function back_propagation(node_out, layer_idx, i, ∇s, weights, ∂loss_∂nodes)\n",
    "    ∂loss_∂node_i = (node_out==0) ? 0 : dot(weights, ∂loss_∂nodes) # considering d of ReLU\n",
    "    ∇s[layer_idx][i, :] = ∂loss_∂nodes*node_out\n",
    "    return ∂loss_∂node_i\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_step(optimizer, Xs_batch, Ys_batch, Ws_wrapped; hidden_layer_size = 64)\n",
    "    b_s = size(Xs_batch)[2]\n",
    "    t = typeof(optimizer)\n",
    "    # b_s = 33\n",
    "    cache_new_Ws_unwrapped = zeros(b_s, num_Ws)\n",
    "    total_losses = zeros(b_s)\n",
    "    Ws_unwrapped = _unwrap(Ws_wrapped)\n",
    "    optimizer.k = optimizer.k + 1\n",
    "    for i in 1:b_s\n",
    "        if t <: LimitedMemoryBFGS\n",
    "            m = length(optimizer.δs)\n",
    "            δs, γs, qs = deepcopy(optimizer.δs), deepcopy(optimizer.γs), deepcopy(optimizer.qs)\n",
    "        elseif t <: ConjugateGD\n",
    "            d_prev, g_prev = deepcopy(optimizer.d), deepcopy(optimizer.g)\n",
    "        end\n",
    "        Xs = Xs_batch[:, i]\n",
    "        Y_truth = Ys_batch[:, i]\n",
    "        # println(\"*********$Y_truth*********\")\n",
    "        #forward\n",
    "        Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "        scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "        total_losses[i] = total_loss\n",
    "        #backward\n",
    "        if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "            Ws_unwrapped = step!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "        else\n",
    "            gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "            if t <: ConjugateGD\n",
    "                Ws_unwrapped = \n",
    "                step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss,\n",
    "                d_prev, g_prev)\n",
    "            elseif t <: LimitedMemoryBFGS\n",
    "                Ws_unwrapped = \n",
    "                step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss, m,\n",
    "                deepcopy(δs), deepcopy(γs), deepcopy(qs))\n",
    "            elseif t <: DFP || t <: semi_DFP\n",
    "                Ws_unwrapped = step!(optimizer, _unwrap(gradients), Ws_unwrapped,\n",
    "                total_loss, Xs, Y_truth)\n",
    "            else\n",
    "                Ws_unwrapped = step!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "            end\n",
    "        end\n",
    "    end    \n",
    "    # if t <: LimitedMemoryBFGS\n",
    "    #     m = length(optimizer.δs)\n",
    "    #     δs, γs, qs = deepcopy(optimizer.δs), deepcopy(optimizer.γs), deepcopy(optimizer.qs)\n",
    "    # elseif t <: ConjugateGD\n",
    "    #     d_prev, g_prev = deepcopy(optimizer.d), deepcopy(optimizer.g)\n",
    "    # end\n",
    "    # Threads.@threads for i in 1:b_s-1\n",
    "    #     Xs = Xs_batch[:, i]\n",
    "    #     Y_truth = Ys_batch[:, i]\n",
    "    #     # println(\"*********$Y_truth*********\")\n",
    "    #     #forward\n",
    "    #     Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "    #     scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "    #     total_losses[i] = total_loss\n",
    "    #     #backward\n",
    "    #     if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "    #         cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "    #     else\n",
    "    #         gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "    #         if t <: ConjugateGD\n",
    "    #             cache_new_Ws_unwrapped[i,:] = \n",
    "    #             step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss,\n",
    "    #             d_prev, g_prev)\n",
    "    #         elseif t <: LimitedMemoryBFGS\n",
    "    #             cache_new_Ws_unwrapped[i,:] = \n",
    "    #             step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss, m,\n",
    "    #             deepcopy(δs), deepcopy(γs), deepcopy(qs))\n",
    "    #         elseif t <: DFP\n",
    "    #             cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped,\n",
    "    #             total_loss, Xs, Y_truth)\n",
    "    #         else\n",
    "    #             cache_new_Ws_unwrapped[i,:] = step_without_update!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "    #         end\n",
    "    #     end\n",
    "    #     # if i==1\n",
    "    #     #     println()\n",
    "    #     #     println(Y_pred)\n",
    "    #     #     println(scores)\n",
    "    #     #     println(Y_truth)\n",
    "    #     #     println()\n",
    "    #     # end\n",
    "    # end\n",
    "    # i = b_s\n",
    "    # Xs = Xs_batch[:, i]\n",
    "    # Y_truth = Ys_batch[:, i]\n",
    "    # # println(\"*********$Y_truth*********\")\n",
    "    # #forward\n",
    "    # Y_pred, hidden_outs = MLP(Xs, 3; num_hidden_layers = 2, Ws=Ws_wrapped, hidden_layer_size = hidden_layer_size)\n",
    "    # scores, losses, total_loss = softmax_loss(Y_pred, Y_truth)\n",
    "    # total_losses[i] = total_loss\n",
    "    # #backward\n",
    "    # gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs)\n",
    "    # if  t <: NesterovMomentum_SGD || t <: Nelder_Mead\n",
    "    #     cache_new_Ws_unwrapped[i,:] = step!(optimizer, Ws_unwrapped, Xs, Y_truth)\n",
    "    # else\n",
    "    #     gradients = ∇(Xs, Y_truth, Ws_wrapped, scores, Y_pred, hidden_outs);\n",
    "    #     if t <: ConjugateGD\n",
    "    #         cache_new_Ws_unwrapped[i,:] = \n",
    "    #         step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss,\n",
    "    #         d_prev, g_prev)\n",
    "    #     elseif t <: LimitedMemoryBFGS\n",
    "    #         cache_new_Ws_unwrapped[i,:] = \n",
    "    #         step!(optimizer, _unwrap(gradients), Ws_unwrapped, Xs, Y_truth, total_loss, m,\n",
    "    #         deepcopy(δs), deepcopy(γs), deepcopy(qs))\n",
    "    #     elseif t <: DFP\n",
    "    #         cache_new_Ws_unwrapped[i,:] = step!(optimizer, _unwrap(gradients), Ws_unwrapped,\n",
    "    #         total_loss, Xs, Y_truth)\n",
    "    #     else\n",
    "    #         cache_new_Ws_unwrapped[i,:] = step!(optimizer, _unwrap(gradients), Ws_unwrapped)\n",
    "    #     end\n",
    "    # end\n",
    "    # #update parameters\n",
    "    # new_Ws_unwrapped = mean(cache_new_Ws_unwrapped, dims=1)\n",
    "    return _wrap(Ws_unwrapped, params_shape), mean(total_losses)\n",
    "    # return _wrap(new_Ws_unwrapped, params_shape), mean(total_losses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch_step (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function epoch_step(optimizer, new_Ws_wrapped; hidden_layer_size = 64, print_info = true)\n",
    "    ct_batch = 1\n",
    "    curr_loss = 0\n",
    "    for (xs, ys) in dl_train\n",
    "        if print_info\n",
    "            print(\"batch# $ct_batch --> \")\n",
    "            flush(stdout)\n",
    "        end\n",
    "        ct_batch += 1\n",
    "        new_Ws_wrapped, curr_loss = batch_step(optimizer, xs, ys, new_Ws_wrapped; hidden_layer_size = hidden_layer_size)\n",
    "    end\n",
    "    if print_info\n",
    "        println(\"\\n\\ttrain loss =  $curr_loss\")\n",
    "    end\n",
    "    return new_Ws_wrapped, curr_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to make inputs compatible with optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_wrap (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _unwrap(wrapped)\n",
    "    unwrapped = []\n",
    "    for layer in wrapped\n",
    "        unwrapped = vcat(unwrapped, reduce(vcat,layer))\n",
    "    end\n",
    "    return unwrapped\n",
    "end\n",
    "function _wrap(unwrapped, shape)\n",
    "    wrapped = []\n",
    "    curr_idx = 1\n",
    "    for s in shape\n",
    "        ct = s[1]*s[2]\n",
    "        push!(wrapped, Float64.(reshape(unwrapped[curr_idx:curr_idx-1+ct], s)))\n",
    "        curr_idx += ct\n",
    "    end\n",
    "    return wrapped\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple and fast line search algorithm by David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line_search (generic function with 1 method)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function line_search(f, g, x, curr_loss, train_x, train_y; ϵ = 0.01, α=0.05, p=0.5, β=1e-4, loop_limit = 30)\n",
    "    if curr_loss < ϵ\n",
    "        return x, nothing, nothing, nothing, nothing # Vanishing gradient\n",
    "    end\n",
    "    # d = -g\n",
    "    d = -g/norm(g)\n",
    "    new_x = x + α*g\n",
    "    loss_new, hidden_outs, Y_pred, scores = f(train_x, train_y, new_x) \n",
    "    ct = 0\n",
    "    # println(\"\\tloss = $(curr_loss)\")\n",
    "    # println(\"\\tnew loss = $loss_new, $(curr_loss + β*α*(g⋅d))\")\n",
    "    while loss_new >= ϵ && loss_new > curr_loss + β*α*(g⋅d) && ct <= loop_limit\n",
    "        α *= p\n",
    "        new_x = x + α*d\n",
    "        loss_new, hidden_outs, Y_pred, scores = f(train_x, train_y, new_x)\n",
    "        ct += 1\n",
    "        # println(\"\\tnew loss = $loss_new, $(curr_loss + β*α*(g⋅d))\")\n",
    "    end\n",
    "    # println(\"\\t here α = $α\")\n",
    "    # println()\n",
    "    return new_x, scores, Y_pred, hidden_outs, loss_new\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function line_serach(F, d, x, train_x, train_y; r=0.5,c=1e-4,nmax=20)\n",
    "    \n",
    "#     # params\n",
    "#     # F: function to be optimized\n",
    "#     # x: variable\n",
    "#     # d: direction\n",
    "#     # r: factor by which to reduce step size at each iteration\n",
    "#     # c: parameter [0,1]\n",
    "#     # nmax: max iteration\n",
    "\n",
    "#     # return\n",
    "#     # α step size\n",
    "#     # fk1: function value at new x\n",
    "#     # gkk: gradient at new x\n",
    "\n",
    "#     #https://en.wikipedia.org/wiki/Backtracking_line_search\n",
    "#     α=1\n",
    "\n",
    "#     fk, hidden_outs, Y_pred, scores = F(train_x, train_y, x)\n",
    "#     # println(\"(train_x,train_y,x,scores,Y_pred,hidden_outs)=>\n",
    "#     # [$(size(train_x))],[$(size(train_y))],[$(size(x))],[$(size(scores))],[$(size(Y_pred))],[$(size(hidden_outs))]\")\n",
    "#     gk = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#     # fk,gk=F(x)\n",
    "\n",
    "#     xx=x\n",
    "#     x=x+α*d\n",
    "\n",
    "#     fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "#     gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#     # fk1,gk1=F(x)\n",
    "#     n=1\n",
    "    \n",
    "#     while fk1 > fk+c*α*(gk'*d) && n < nmax\n",
    "#         n = n+1\n",
    "#         α = α*0.5\n",
    "#         x = xx+α*d\n",
    "\n",
    "#         fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "#         gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#         # fk1,gk1=F(x)\n",
    "#     end\n",
    "#     if α == 1\n",
    "#         α = -1\n",
    "#         while fk1>fk+c*α*(gk'*d) && n < nmax\n",
    "#             n=n+1\n",
    "#             α=α*0.5\n",
    "#             x=xx+α*d\n",
    "    \n",
    "#             fk1, hidden_outs, Y_pred, scores = F(train_x, train_y, x) \n",
    "#             gk1 = _unwrap(∇(train_x, train_y, _wrap(x, params_shape), scores, Y_pred, hidden_outs))\n",
    "#             # fk1,gk1=F(x)\n",
    "#         end\n",
    "#     end\n",
    "#     return α, fk1, gk1\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steepest gradient descent\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct SteepestGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::SteepestGD)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end\n",
    "function step_without_update!(M::SteepestGD, ∇f, x)\n",
    "    return x - M.α*∇f\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad accelerated gradient descent \n",
    "Credit: Jinghua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 2 methods)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K&W page 78\n",
    "mutable struct Adagrad <: DescentMethod\n",
    "    α # learning rate\n",
    "    ϵ # small value\n",
    "    s # sum of squared gradient\n",
    "    k # dumb var\n",
    "end\n",
    "function init!(M::Adagrad, x_length)\n",
    "    M.s = zeros(x_length)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::Adagrad, g, x)\n",
    "    α, ϵ = M.α, M.ϵ\n",
    "    M.s += g.*g\n",
    "    return x - α*g ./ (sqrt.(M.s) .+ ϵ)\n",
    "end\n",
    "function step_without_update!(M::Adagrad, g, x)\n",
    "    α, ϵ = M.α, M.ϵ\n",
    "    ss = M.s + g.*g\n",
    "    return x - α*g ./ (sqrt.(ss) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam gradient descent\n",
    "Credit: Wenbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 3 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type DescentMethod end\n",
    "# Adam from K&W page 81\n",
    "mutable struct Adam <: DescentMethod\n",
    "    α # learning rate\n",
    "    γv # decay\n",
    "    γs # decay\n",
    "    ϵ # small value\n",
    "    k # step counter\n",
    "    v # 1st moment estimate\n",
    "    s # 2nd moment estimate\n",
    "end\n",
    "function init!(M::Adam, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "    M.s = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    s, v, g = M.s, M.v, ∇f\n",
    "    M.v = γv*v + (1-γv)*g\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    # M.k = k += 1\n",
    "    v_hat = M.v ./ (1 - γv^k)\n",
    "    s_hat = M.s ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end\n",
    "function step_without_update!(M::Adam, ∇f, x)\n",
    "    α, γv, γs, ϵ, k = M.α, M.γv, M.γs, M.ϵ, M.k\n",
    "    vv = γv*M.v + (1-γv)*∇f\n",
    "    ss = γs*M.s + (1-γs)*∇f.*∇f\n",
    "    # M.k = k += 1\n",
    "    v_hat = vv ./ (1 - γv^k)\n",
    "    s_hat = ss ./ (1 - γs^k)\n",
    "    return x - α*v_hat ./ (sqrt.(s_hat) .+ ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate gradient descent\n",
    "Credit: Yashuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 10 methods)"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct ConjugateGD <: DescentMethod\n",
    "    α\n",
    "    g\n",
    "    f\n",
    "    d\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::ConjugateGD)\n",
    "    M.k = 0\n",
    "    M.d = -M.g\n",
    "end\n",
    "# function step!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step!(M::ConjugateGD, g, x, train_x, train_y, curr_loss, d_prev, g_prev)\n",
    "    # d_prev, g_prev = M.d, M.g\n",
    "    β = dot(g, g-g_prev)/(g_prev ⋅ g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    # d = d/sqrt(sum(d.^2))\n",
    "    x_next, dumb1, dumb2, dumb3, loss_new = line_search(M.f, -d, x, curr_loss, train_x, train_y; α=M.α)\n",
    "    # M.α, dumb1, dumb2 = line_serach(M.f, g, x, train_x, train_y)\n",
    "    # x_next = x + M.α*d\n",
    "    if dumb1 != nothing\n",
    "        M.d, M.g = d, g\n",
    "    end\n",
    "    return x_next\n",
    "end\n",
    "# function step_without_update!(M::ConjugateGD, g, x, train_x, train_y)\n",
    "function step_without_update!(M::ConjugateGD, g, x, train_x, train_y, curr_loss, d_prev, g_prev)\n",
    "    # d_prev, g_prev = M.d, M.g\n",
    "    β = dot(g, g-g_prev)/(g_prev ⋅ g_prev)\n",
    "    # β = dot(g, g-g_prev)/dot(g_prev, g_prev)\n",
    "    β = max(0, β)\n",
    "    d = -g + β*d_prev\n",
    "    # d = d/sqrt(sum(d.^2))\n",
    "    x_next, dumb1, dumb2, dumb3, loss_new = line_search(M.f, -d, x, curr_loss, train_x, train_y; α=M.α)\n",
    "    # α, dumb1, dumb2 = line_serach(M.f, g, x, train_x, train_y)\n",
    "    # x_next = x + α*d\n",
    "    if dumb1 != nothing\n",
    "        M.d, M.g = d, g\n",
    "    end\n",
    "    return x_next\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Momentum (SGD)\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 5 methods)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From K&W 76\n",
    "mutable struct NesterovMomentum_SGD <: DescentMethod\n",
    "    α # learning rate\n",
    "    β # momentum decay\n",
    "    f\n",
    "    v # momentum\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::NesterovMomentum_SGD, x_length)\n",
    "    M.k = 0\n",
    "    M.v = zeros(x_length)\n",
    "end\n",
    "function step!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    M.v = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + M.v\n",
    "end\n",
    "function step_without_update!(M::NesterovMomentum_SGD, x, train_x, train_y)\n",
    "    α, β, v = M.α, M.β, M.v\n",
    "    x_new = x + β*v\n",
    "    f_x_βv, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    ∇f_x_βv = ∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs)\n",
    "    vv = β*v - α*_unwrap(∇f_x_βv)\n",
    "    return x + vv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adadelta gradient descent\n",
    "Credit: Dylan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 6 methods)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: K&W page 80\n",
    "mutable struct Adadelta <: DescentMethod\n",
    "    γs # gradient decay\n",
    "    γx # update decay\n",
    "    ϵ # small value\n",
    "    s # sum of squared gradients\n",
    "    u # sum of squared updates\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::Adadelta, x_length)\n",
    "    M.k = 0\n",
    "    M.s = zeros(x_length)\n",
    "    M.u = zeros(x_length)\n",
    "    return M\n",
    "end\n",
    "function step!(M::Adadelta, g, x)\n",
    "    γs, γx, ϵ, s, u = M.γs, M.γx, M.ϵ, M.s, M.u\n",
    "    M.s = γs*s + (1-γs)*g.*g\n",
    "    Δx = - (sqrt.(u) .+ ϵ) ./ (sqrt.(M.s) .+ ϵ) .* g\n",
    "    M.u = γx*u + (1-γx)*Δx.*Δx\n",
    "    return x + Δx\n",
    "end\n",
    "function step_without_update!(M::Adadelta, g, x)\n",
    "    γs, γx, ϵ, s, u = M.γs, M.γx, M.ϵ, M.s, M.u\n",
    "    ss = γs*s + (1-γs)*g.*g\n",
    "    Δx = - (sqrt.(u) .+ ϵ) ./ (sqrt.(ss) .+ ϵ) .* g\n",
    "    # u[:] = γx*u + (1-γx)*Δx.*Δx\n",
    "    return x + Δx\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nelder-Mead gradient descent\n",
    "Credit: Yashuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 7 methods)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct Nelder_Mead <: DescentMethod\n",
    "    f\n",
    "    k # dumb variable\n",
    "end\n",
    "function init!(M::Nelder_Mead)\n",
    "    M.k = 0\n",
    "end\n",
    "function step!(M::Nelder_Mead, x, train_x, train_y)\n",
    "    return nmsmax(M.f, x, train_x, train_y)\n",
    "end\n",
    "function step_without_update!(M::Nelder_Mead, x, train_x, train_y)\n",
    "    return nmsmax(M.f, x, train_x, train_y)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nmsmax (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nmsmax(fun, x, train_x, train_y; trace = true, initial_simplex = 0, target_f = Inf, max_its = Inf, max_evals = Inf, tol = 1e-3 )\n",
    "    x0 = x[:];  # Work with column vector internally.\n",
    "    n = length(x0);\n",
    "\n",
    "   #  V = [zeros(n,1) eye(n)];\n",
    "    V = [zeros(n,1) Matrix(1.0I, n, n)];\n",
    "    f = zeros(n+1,1);\n",
    "    V[:,1] = x0; \n",
    "    \n",
    "    f[1], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "   #  f[1] = fun(x);\n",
    "\n",
    "    \n",
    "    fmax_old = f[1];\n",
    "    fmax     = -Inf; # Some initial value\n",
    "\n",
    "   #  if trace\n",
    "   #      @printf \"f(x0) = %9.4e\\n\" f[1]\n",
    "   #  end\n",
    "\n",
    "    k = 0; m = 0;\n",
    "\n",
    "    # Set up initial simplex.\n",
    "    scale = max(norm(x0,Inf),1);\n",
    "    if initial_simplex == 0\n",
    "       # Regular simplex - all edges have same length.\n",
    "       # Generated from construction given in reference [18, pp. 80-81] of [1].\n",
    "       alpha = scale / (n*sqrt(2)) * [ sqrt(n+1)-1+n  sqrt(n+1)-1 ];\n",
    "       V[:,2:n+1] = (x0 + alpha[2]*ones(n,1)) * ones(1,n);\n",
    "       for j=2:n+1\n",
    "           V[j-1,j] = x0[j-1] + alpha[1];\n",
    "           x[:] = V[:,j]; \n",
    "\n",
    "           f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "         #   f[j] = fun(x);\n",
    "       end\n",
    "    else\n",
    "       # Right-angled simplex based on co-ordinate axes.\n",
    "       alpha = scale*ones(n+1,1);\n",
    "       for j=2:n+1\n",
    "           V[:,j] = x0 + alpha[j]*V[:,j];\n",
    "           x[:] = V[:,j]; \n",
    "           f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "         #   f[j] = fun(x);\n",
    "       end\n",
    "    end\n",
    "    nf = n+1;\n",
    "    how = \"initial  \";\n",
    "\n",
    "    j = sortperm(f[:]);\n",
    "    temp = f[j];\n",
    "    j = j[n+1:-1:1];\n",
    "    f = f[j]; V = V[:,j];\n",
    "\n",
    "    alpha = 1;  beta = 1/2;  gamma = 2;\n",
    "\n",
    "    msg = \"\"\n",
    "\n",
    "    while true    ###### Outer (and only) loop.\n",
    "    k = k+1;\n",
    "\n",
    "        fmax = f[1];\n",
    "      #   if fmax > fmax_old\n",
    "      #       if trace\n",
    "      #          @printf \"Iter. %2.0f,\" k\n",
    "      #          print(string(\"  how = \", how, \" \"));\n",
    "      #          @printf \"nf = %3.0f,  f = %9.4e  (%2.1f%%)\\n\" nf fmax 100*(fmax-fmax_old)/(abs(fmax_old)+eps(fmax_old));\n",
    "      #       end\n",
    "      #   end\n",
    "        fmax_old = fmax;\n",
    "\n",
    "        ### Three stopping tests from MDSMAX.M\n",
    "\n",
    "        # Stopping Test 1 - f reached target value?\n",
    "        if fmax >= target_f\n",
    "           msg = \"Exceeded target...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 2 - too many f-evals?\n",
    "        if nf >= max_evals\n",
    "           msg = \"Max no. of function evaluations exceeded...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 3 - too many iterations?\n",
    "        if k > max_its\n",
    "           msg = \"Max no. of iterations exceeded...quitting\\n\";\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        # Stopping Test 4 - converged?   This is test (4.3) in [1].\n",
    "        v1 = V[:,1];\n",
    "        size_simplex = norm(V[:,2:n+1]-v1[:,ones(Int,n)],1) / max(1, norm(v1,1));\n",
    "        if size_simplex <= tol\n",
    "         #   msg = @sprintf(\"Simplex size %9.4e <= %9.4e...quitting\\n\", size_simplex, tol)\n",
    "           break  # Quit.\n",
    "        end\n",
    "\n",
    "        #  One step of the Nelder-Mead simplex algorithm\n",
    "        #  NJH: Altered function calls and changed CNT to NF.\n",
    "        #       Changed each `fr < f[1]' type test to `>' for maximization\n",
    "        #       and re-ordered function values after sort.\n",
    "\n",
    "        temp = sum(V[:,1:n]'; dims = 1)\n",
    "        vbar = (temp/n)';  # Mean value\n",
    "        vr = (1 + alpha)*vbar - alpha*V[:,n+1]; x[:] = vr; \n",
    "        \n",
    "        fr, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "      #   fr = fun(x);\n",
    "\n",
    "        nf = nf + 1;\n",
    "        vk = vr;  fk = fr; how = \"reflect, \";\n",
    "        if fr > f[n]\n",
    "                if fr > f[1]\n",
    "                   ve = gamma*vr + (1-gamma)*vbar; x[:] = ve; \n",
    "\n",
    "                   fe, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                  #  fe = fun(x);\n",
    "\n",
    "                   nf = nf + 1;\n",
    "                   if fe > f[1]\n",
    "                      vk = ve; fk = fe;\n",
    "                      how = \"expand,  \";\n",
    "                   end\n",
    "                end\n",
    "        else\n",
    "                vt = V[:,n+1]; ft = f[n+1];\n",
    "                if fr > ft\n",
    "                   vt = vr;  ft = fr;\n",
    "                end\n",
    "                vc = beta*vt + (1-beta)*vbar; x[:] = vc; \n",
    "                \n",
    "                fc, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "               #  fc = fun(x);\n",
    "\n",
    "                nf = nf + 1;\n",
    "                if fc > f[n]\n",
    "                   vk = vc; fk = fc;\n",
    "                   how = \"contract,\";\n",
    "                else\n",
    "                   for j = 2:n\n",
    "                       V[:,j] = (V[:,1] + V[:,j])/2;\n",
    "                       x[:] = V[:,j]; \n",
    "                       \n",
    "                       f[j], hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                     #   f[j] = fun(x);\n",
    "                   end\n",
    "                   nf = nf + n-1;\n",
    "                   vk = (V[:,1] + V[:,n+1])/2; x[:] = vk; \n",
    "\n",
    "                   fk, hidden_outs, Y_pred, scores = fun(train_x, train_y, x) \n",
    "                  #  fk = fun(x);\n",
    "\n",
    "                   nf = nf + 1;\n",
    "                   how = \"shrink,  \";\n",
    "                end\n",
    "        end\n",
    "        V[:,n+1] = vk;\n",
    "        f[n+1] = fk;\n",
    "        j = sortperm(f[:]);\n",
    "        temp = f[j];\n",
    "        j = j[n+1:-1:1];\n",
    "        f = f[j]; V = V[:,j];\n",
    "\n",
    "    end   ###### End of outer (and only) loop.\n",
    "\n",
    "    # Finished.\n",
    "   #  if trace\n",
    "   #      print(msg)\n",
    "   #  end\n",
    "    x[:] = V[:,1];\n",
    "\n",
    "   #  return x, fmax, nf, k-1\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davidon-Fletcher-Powel gradient descent(DFP)\n",
    "Credit: Jinghua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 10 methods)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K&W 93\n",
    "mutable struct DFP <: DescentMethod\n",
    "    Q\n",
    "    f\n",
    "    k # dumb var\n",
    "end\n",
    "function init!(M::DFP, x_length)\n",
    "    M.k = 0\n",
    "    M.Q = Matrix(1.0I, x_length, x_length)\n",
    "end\n",
    "function step!(M::DFP, g, x, curr_loss, train_x, train_y)\n",
    "    Q = M.Q\n",
    "    x′, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, M.Q*g, x, curr_loss, train_x, train_y; α=0.2)\n",
    "    # x′ = line_search(f, x, -Q*g)\n",
    "    if scores == nothing\n",
    "        return x\n",
    "    end\n",
    "    g′ = _unwrap(∇(train_x, train_y, _wrap(x′, params_shape), scores, Y_pred, hidden_outs))\n",
    "    # g′ = ∇f(x′)\n",
    "    δ = x′ - x\n",
    "    γ = g′ - g\n",
    "    M.Q = Q - Q*γ*γ'*Q/(γ'*Q*γ) + δ*δ'/(δ'*γ)\n",
    "    return x′\n",
    "end\n",
    "function step_without_update!(M::DFP, g, x, curr_loss, train_x, train_y)\n",
    "    x′, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, M.Q*g, x, curr_loss, train_x, train_y; α=0.2)\n",
    "    if scores == nothing\n",
    "        return x\n",
    "    end\n",
    "    return x′\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi Davidon-Fletcher-Powel gradient descent(semi-DFP)\n",
    "Credit: Jinghua\n",
    "\n",
    "This is a DFP optimizer but with fixed step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_without_update! (generic function with 10 methods)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K&W 93\n",
    "mutable struct semi_DFP <: DescentMethod\n",
    "    α # learning rate\n",
    "    Q\n",
    "    f\n",
    "    k # dumb var\n",
    "end\n",
    "function init!(M::semi_DFP, x_length)\n",
    "    M.k = 0\n",
    "    M.Q = Matrix(1.0I, x_length, x_length)\n",
    "end\n",
    "function step!(M::semi_DFP, g, x, curr_loss, train_x, train_y)\n",
    "    Q = M.Q\n",
    "    # x′, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, -M.Q*g, x, curr_loss, train_x, train_y; α=0.2)\n",
    "    # x′ = line_search(f, x, -Q*g)\n",
    "    x′ = x - M.α * (M.Q*g)\n",
    "    # if scores == nothing\n",
    "    #     return x\n",
    "    # end\n",
    "    g′ = _unwrap(∇(train_x, train_y, _wrap(x′, params_shape), scores, Y_pred, hidden_outs))\n",
    "    # g′ = ∇f(x′)\n",
    "    δ = x′ - x\n",
    "    γ = g′ - g\n",
    "    M.Q = Q - Q*γ*γ'*Q/(γ'*Q*γ) + δ*δ'/(δ'*γ)\n",
    "    return x′\n",
    "end\n",
    "function step_without_update!(M::semi_DFP, g, x, curr_loss, train_x, train_y)\n",
    "    # x′, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, -M.Q*g, x, curr_loss, train_x, train_y; α=0.2)\n",
    "    # if scores == nothing\n",
    "    #     return x\n",
    "    x′ = x - M.α * (M.Q*g)\n",
    "    return x′\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limited memory Broyden-Fletcher-Goldfarb-Shanno gradient descent(LBFGS)\n",
    "Credit: David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step! (generic function with 10 methods)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct LimitedMemoryBFGS <: DescentMethod\n",
    "    m\n",
    "    f\n",
    "    δs\n",
    "    γs\n",
    "    qs\n",
    "    k # dumb var\n",
    "end\n",
    "function init!(M::LimitedMemoryBFGS)\n",
    "    M.k = 0\n",
    "    M.δs = []\n",
    "    M.γs = []\n",
    "    M.qs = []\n",
    "end\n",
    "function step_without_update!(M::LimitedMemoryBFGS, g, x, train_x, train_y, curr_loss, m, δs, γs, qs)    \n",
    "    # sleep(0.02)\n",
    "    # while l > M.m+1\n",
    "    #     popfirst!(M.δs); popfirst!(M.γs); popfirst!(M.qs)\n",
    "    #     l -= 1\n",
    "    # end\n",
    "    # x_new = step_without_update!(M, g, x, train_x, train_y, curr_loss, l, δs, γs, qs)\n",
    "    # return x_new\n",
    "end\n",
    "function step!(M::LimitedMemoryBFGS, g, x, train_x, train_y, curr_loss, m, δs, γs, qs)\n",
    "\n",
    "    m = min(M.m, m)\n",
    "    # println(m)\n",
    "    g′ = g\n",
    "    # m = length(δs)\n",
    "    if m > 0\n",
    "        q = g\n",
    "        for i in m : -1 : 1\n",
    "            qs[i] = q\n",
    "            q -= (δs[i]⋅q)/(γs[i]⋅δs[i])*γs[i]\n",
    "        end\n",
    "        z = (γs[m] .* δs[m] .* q) / (γs[m]⋅γs[m])\n",
    "        for i in 1 : m\n",
    "            z += δs[i]*(δs[i]⋅qs[i] - γs[i]⋅z)/(γs[i]⋅δs[i])\n",
    "        end\n",
    "        x_neww, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, z, x, curr_loss, train_x, train_y; α=0.05)\n",
    "        if scores == nothing\n",
    "            push!(M.qs, zeros(length(x)))\n",
    "            push!(M.δs, x - x)\n",
    "            push!(M.γs, g′ - g)\n",
    "\n",
    "            while m > M.m+1\n",
    "                popfirst!(M.δs); popfirst!(M.γs); popfirst!(M.qs)\n",
    "                m -= 1\n",
    "            end\n",
    "            return x\n",
    "        end\n",
    "        if loss_new < curr_loss\n",
    "            x_new = x_neww\n",
    "        else\n",
    "            x_new, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, g, x, curr_loss, train_x, train_y; α=0.0001)\n",
    "            if scores == nothing\n",
    "                push!(M.qs, zeros(length(x)))\n",
    "                push!(M.δs, x - x)\n",
    "                push!(M.γs, g′ - g)\n",
    "\n",
    "                while m > M.m+1\n",
    "                    popfirst!(M.δs); popfirst!(M.γs); popfirst!(M.qs)\n",
    "                    m -= 1\n",
    "                end\n",
    "                return x\n",
    "            end\n",
    "        end\n",
    "        g′ = _unwrap(∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs))\n",
    "        # x_new = line_search(f, x, -z)\n",
    "    else\n",
    "        x_new, scores, Y_pred, hidden_outs, loss_new = line_search(M.f, g, x, curr_loss, train_x, train_y; α=0.0001)\n",
    "        if scores == nothing\n",
    "            push!(M.qs, zeros(length(x)))\n",
    "            push!(M.δs, x - x)\n",
    "            push!(M.γs, g′ - g)\n",
    "\n",
    "            while m > M.m+1\n",
    "                popfirst!(M.δs); popfirst!(M.γs); popfirst!(M.qs)\n",
    "                m -= 1\n",
    "            end\n",
    "            return x\n",
    "        end\n",
    "        g′ = _unwrap(∇(train_x, train_y, _wrap(x_new, params_shape), scores, Y_pred, hidden_outs))\n",
    "        # x_new = line_search(f, x, -g)\n",
    "    end\n",
    "    # dumb1, hidden_outs, Y_pred, scores = M.f(train_x, train_y, x_new) \n",
    "    # g′ = ∇f(x_new)\n",
    "\n",
    "    push!(M.qs, zeros(length(x)))\n",
    "    push!(M.δs, x_new - x)\n",
    "    push!(M.γs, g′ - g)\n",
    "\n",
    "    while m > M.m+1\n",
    "        popfirst!(M.δs); popfirst!(M.γs); popfirst!(M.qs)\n",
    "        m -= 1\n",
    "    end\n",
    "    return x_new\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutable struct LBFGS <: DescentMethod\n",
    "# \tn # number of variables\n",
    "# \tf\n",
    "\n",
    "#     m # Memory length, was ∈ [2, 54] in paper\n",
    "# \tprev_g # gradient at previous timestep \n",
    "# \tprev_x # x at previous timestep \n",
    "# \tSm # previous m x's\n",
    "# \tYm # previous m gradients\n",
    "# \tk # Internal iteration index\n",
    "# end\n",
    "\n",
    "# function init!(M::LBFGS)\n",
    "#     M.m = 20 \n",
    "#     M.prev_g = zeros(M.n)\n",
    "#     M.prev_x = zeros(M.n)\n",
    "#     M.Sm = zeros(M.n,M.m)\n",
    "#     M.Ym = zeros(M.n,M.m)\n",
    "#     M.k = 0 \n",
    "# end\n",
    "# function step!(o::LBFGS, ∇, x, train_x, train_y)\n",
    "#     # o.k += 1\n",
    "# \tm = o.m\n",
    "\t\n",
    "# \tgnorm = norm(∇)\n",
    "\t\n",
    "# \t# if gnorm < τgrad # tolerance for the norm of the slope \n",
    "# \t# \treturn; \n",
    "# \t# end\n",
    "\t\n",
    "# \ts0 = x-o.prev_x\n",
    "# \ty0 = ∇-o.prev_g\n",
    "\t\n",
    "# \t# println(\"y0=$y0\")\n",
    "# \tH0 = s0'*y0/(y0'*y0) # hessian diagonal satisfying secant condition\n",
    "    \n",
    "#     k = o.k\n",
    "# \t# update Sm and Ym\n",
    "# \tif k <= m\n",
    "# \t\to.Sm[:,k].=s0\n",
    "# \t\to.Ym[:,k].=y0\n",
    "# \t\tp=-approxInvHess(∇,o.Sm[:,1:k],o.Ym[:,1:k],H0) \n",
    "# \t# only keep m entries in Sm and Ym so purge the old ones\n",
    "\t\t\n",
    "# \telse\n",
    "# \t\to.Sm[:,1:(m-1)].=o.Sm[:,2:m]\n",
    "# \t\to.Ym[:,1:(m-1)].=o.Sm[:,2:m]\n",
    "# \t\to.Sm[:,m].=s0\n",
    "# \t\to.Ym[:,m].=y0\n",
    "# \t\tp.=-approxInvHess(∇,o.Sm,o.Ym,H0)\n",
    "# \tend\n",
    "\t\n",
    "# \t# new direction=p, find new step size\n",
    "#     # α = 0.01\n",
    "# \tα, fs, gs=line_serach(o.f, p, x, train_x, train_y)\n",
    "\t\n",
    "# \t# update for next iteration\n",
    "# \to.prev_x = x\n",
    "# \to.prev_g = ∇\n",
    "# \tx .= x + α.*p\n",
    "#     return x\n",
    "# \t# f1=fs\n",
    "# \t# g1=gs\n",
    "# \t# k=k+1\n",
    "\t\n",
    "# \t# if verbose == 1 \n",
    "# \t# \tprintln(\"Iteration: $k -- x = $x1\")\n",
    "# \t# end\n",
    "# end\n",
    "# function step_without_update!(o::LBFGS, ∇, x, train_x, train_y)\n",
    "#     Smm = deepcopy(o.Sm)\n",
    "#     Ymm = deepcopy(o.Ym)\n",
    "\n",
    "#     # o.k += 1\n",
    "# \tm = o.m\n",
    "\t\n",
    "# \tgnorm = norm(∇)\n",
    "\t\n",
    "# \t# if gnorm < τgrad # tolerance for the norm of the slope \n",
    "# \t# \treturn; \n",
    "# \t# end\n",
    "\t\n",
    "# \ts0 = x-o.prev_x\n",
    "# \ty0 = ∇-o.prev_g\n",
    "\t\n",
    "# \t# println(\"y0=$y0\")\n",
    "# \tH0 = s0'*y0/(y0'*y0) # hessian diagonal satisfying secant condition\n",
    "\n",
    "#     k = o.k\n",
    "# \t# update Sm and Ym\n",
    "# \tif k <= m\n",
    "# \t\tSmm[:,k].=s0\n",
    "# \t\tYmm[:,k].=y0\n",
    "# \t\tp=-approxInvHess(∇,Smm[:,1:k],Ymm[:,1:k],H0) \n",
    "# \t# only keep m entries in Sm and Ym so purge the old ones\n",
    "\t\t\n",
    "# \telse\n",
    "# \t\tSmm[:,1:(m-1)].=Smm[:,2:m]\n",
    "# \t\tYmm[:,1:(m-1)].=Smm[:,2:m]\n",
    "# \t\tSmm[:,m].=s0\n",
    "# \t\tYmm[:,m].=y0\n",
    "# \t\tp.=-approxInvHess(∇,Smm,Ymm,H0)\n",
    "# \tend\n",
    "\t\n",
    "# \t# new direction=p, find new step size\n",
    "#     # α = 0.01\n",
    "# \tα, fs, gs=line_serach(o.f, p, x, train_x, train_y)\n",
    "\t\n",
    "# \t# update for next iteration\n",
    "# \t# o.prev_x = x\n",
    "# \t# o.prev_g = ∇\n",
    "# \tx .= x + α.*p\n",
    "#     return x\n",
    "# \t# f1=fs\n",
    "# \t# g1=gs\n",
    "# \t# k=k+1\n",
    "\t\n",
    "# \t# if verbose == 1 \n",
    "# \t# \tprintln(\"Iteration: $k -- x = $x1\")\n",
    "# \t# end\n",
    "# end\n",
    "\n",
    "# function approxInvHess(g,S,Y,H0)\n",
    "#     #INPUT\n",
    "\n",
    "#     #g: gradient nx1 vector\n",
    "#     #S: nxk matrixs storing S[i]=x[i+1]-x[i]\n",
    "#     #Y: nxk matrixs storing Y[i]=g[i+1]-g[i]\n",
    "#     #H0: initial hessian diagnol scalar\n",
    "\n",
    "#     #OUTPUT\n",
    "#     # p:  the approximate inverse hessian multiplied by the gradient g\n",
    "#     #     which is the new direction\n",
    "#     #notation follows:\n",
    "#     #https://en.wikipedia.org/wiki/Limited-memory_BFGS\n",
    "\n",
    "#     n,k=size(S)\n",
    "#     rho=zeros(k)\n",
    "#     for i=1:k\n",
    "#         rho[i] = 1 /(Y[:,i]'*S[:,i])\n",
    "#         if rho[i]<0\n",
    "#             rho[i]=-rho[i]\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "\n",
    "#     q=zeros(n,k+1)\n",
    "#     r=zeros(n,1)\n",
    "#     α=zeros(k,1)\n",
    "#     β=zeros(k,1)\n",
    "\n",
    "#     q[:,k+1]=g\n",
    "\n",
    "#     for i=k:-1:1\n",
    "#         α[i] =rho[i]*S[:,i]'*q[:,i+1]\n",
    "#         q[:,i].=q[:,i+1]-α[i]*Y[:,i]\n",
    "#     end\n",
    "\n",
    "#     z=zeros(size(q[:,1])[1])\n",
    "#     # println(size(H0))\n",
    "#     # println()\n",
    "#     # println(size(H0*q[:,1]))\n",
    "#     z.= H0*q[:,1]\n",
    "\n",
    "\n",
    "#     for i=1:k\n",
    "#         β[i] = rho[i]*Y[:,i]'*z\n",
    "#         z.=z+S[:,i]*(α[i]-β[i])\n",
    "#     end\n",
    "\n",
    "#     p=copy(z)\n",
    "\n",
    "#     return p\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control variable for a scientifically correct comparison: **Give every optimizers a same NN to train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created!\n",
      "Inital loss = 10.782513134966402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ε = 0.001 # stop when loss <= loss_ε\n",
    "# Initializing \n",
    "Y_pred, hidden_outs, Ws_copy = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "# println(Y_pred)\n",
    "scores, losses, total_loss = softmax_loss(Y_pred, train_y_data[1])\n",
    "∇_copy = ∇(train_x_data[1], train_y_data[1], Ws_copy, scores, Y_pred, hidden_outs);\n",
    "println(\"New model created!\\nInital loss = $total_loss\")\n",
    "params_shape = []\n",
    "num_Ws = 0\n",
    "for layer in Ws_copy\n",
    "    # println(size(layer))\n",
    "    s1, s2 = size(layer)\n",
    "    num_Ws += s1 * s2\n",
    "    push!(params_shape, size(layer))\n",
    "end\n",
    "function f(train_x, train_y, w_unwrapped)\n",
    "    Y_pred, hidden_outs = MLP(train_x, 3; num_hidden_layers = 2, Ws=_wrap(w_unwrapped, params_shape), hidden_layer_size = 16)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, train_y)\n",
    "    return total_loss, hidden_outs, Y_pred, scores\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Steepest gradient descent done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Steepest_GD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_SteepestGD = SteepestGD(0.0001, nothing)\n",
    "init!(optimizer_SteepestGD)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  2.1696429587637915\n",
      "Epoch# 2\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.336624933594325\n",
      "Epoch# 3\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1421590519778645\n",
      "Epoch# 4\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.090387115997092\n",
      "Epoch# 5\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.12222686592406969\n",
      "Epoch# 6\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.127023486182307\n",
      "Epoch# 7\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.12921138012269984\n",
      "Epoch# 8\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.13259639345379812\n",
      "Epoch# 9\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11106118530218627\n",
      "Epoch# 10\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.27856610453524827\n",
      "Epoch# 11\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10601587580935846\n",
      "Epoch# 12\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.15471359119589587\n",
      "Epoch# 13\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06624020372433474\n",
      "Epoch# 14\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1157620217950767\n",
      "Epoch# 15\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.12596449352895048\n",
      "Epoch# 16\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.3390979080917053\n",
      "Epoch# 17\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0784532210518057\n",
      "Epoch# 18\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06547646933605238\n",
      "Epoch# 19\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10136915848661947\n",
      "Epoch# 20\n",
      "\tbatch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.14172665799220188\n",
      "Finished\n",
      "Any[2.813510706644876, 2.02460295859139, 1.5263206152571462, 1.3057685326331536, 1.2307670738637182, 0.987813676184092, 0.9466814308758973, 0.8393073645260343, 0.8231484991751225, 0.7581408708124603, 0.7330502427688372, 0.6924758387275292, 0.6637626211184114, 0.6582469447456496, 0.5594356947416214, 0.5905102070516289, 0.5923914177492404, 0.5189721736077326, 0.5708995588541544, 0.5024111047807144]\n",
      "0.5098774896946194\n",
      "(0.4666021269420901, 0.75)\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_Steepest_GD\n",
    "    print(\"Epoch# $i\\n\\t\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_SteepestGD, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")\n",
    "println(epoch_vs_losses)\n",
    "println(train_scores(Ws))\n",
    "println(test_scores(Ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adagrad done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adagrad = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adagrad = Adagrad(0.005, 1e-5, nothing, nothing)\n",
    "init!(optimizer_Adagrad, num_Ws);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.48756383497199424\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.31177849601546637\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11525098481730005\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11918896341985073\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.16629175788976489\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.24484459056963964\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0903127357181439\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09944719187454347\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10297359229590043\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0784335763785937\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08793193772657666\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07726613560270819\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09570328443183294\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.04751618680723816\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06300817044744438\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06852583868309438\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.2766385491524844\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.2285325930189065\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09383214860209564\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08696585871897212\n",
      "Finished\n",
      "Any[1.0406702866007653, 0.7808086668756182, 0.6562691342642183, 0.5732303571372507, 0.5374988728271974, 0.4917201052459664, 0.4348909360097417, 0.4592842639796625, 0.3510243717537436, 0.35105131515555876, 0.34395594863163526, 0.34341837783838375, 0.31536601645553053, 0.29344946573150427, 0.31692087951817355, 0.2968665854441016, 0.2859705810351707, 0.24388630096654285, 0.2581603042497033, 0.27274272504512015]\n",
      "0.2894304161491129\n",
      "(0.3540053870525814, 0.875)\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_Adagrad\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adagrad, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")\n",
    "println(epoch_vs_losses)\n",
    "println(train_scores(Ws))\n",
    "println(test_scores(Ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adam done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adam = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adam = Adam(0.00057, 0.9, 0.999, 1e-4, nothing, nothing, nothing)\n",
    "init!(optimizer_Adam, num_Ws);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  3.341747667855796\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.4688462291258866\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.39858866531169923\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.3576027657651817\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.3250543725844321\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.5415656750595957\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1698219991948634\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.16096175475587293\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10971337153182024\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10149162554996742\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09824057416099446\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.3457755942401312\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08705971808430091\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.062270889143764635\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.05578719399144418\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.2328680335015297\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06134070043390372\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.047748468454806015\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.053076697837226544\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.043259822388825475\n",
      "Finished\n",
      "Any[3.341747667855796, 1.4688462291258866, 0.39858866531169923, 0.3576027657651817, 0.3250543725844321, 0.5415656750595957, 0.1698219991948634, 0.16096175475587293, 0.10971337153182024, 0.10149162554996742, 0.09824057416099446, 0.3457755942401312, 0.08705971808430091, 0.062270889143764635, 0.05578719399144418, 0.2328680335015297, 0.06134070043390372, 0.047748468454806015, 0.053076697837226544, 0.043259822388825475]\n",
      "0.3510178294703253\n",
      "(0.3859263421116556, 0.7916666666666666)\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_Adam\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adam, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")\n",
    "\n",
    "println(epoch_vs_losses)\n",
    "println(train_scores(Ws))\n",
    "println(test_scores(Ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = ConjugateGD done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_ConjugateGD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_ConjugateGD = ConjugateGD(0.003, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "init!(optimizer_ConjugateGD);\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  5.018214868242809\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  3.5503155972683604\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  2.525316751172077\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.6602582838413988\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.5459365044621194\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.8732302528341198\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.5204137074289746\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10327984294292003\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08094941187938877\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06268009983775985\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0832341961947874\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.04589874683162767\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.02259665203339804\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.02651887650804775\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.010026888694630993\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03282870402578537\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1083903608489949\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.017173861687222892\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03797677499976211\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.021707644436038013\n",
      "Finished\n",
      "Any[4.175634591043, 3.3799337196400887, 2.6871588804396387, 2.1628126654387985, 2.006800469083726, 1.8214526858219986, 1.580611834050191, 1.664290587313308, 1.5935523547492365, 1.5330806261783754, 1.5601436523536718, 1.4939428747919774, 1.3524765615278456, 1.1571325271547488, 1.0595179833909893, 0.9455578463036914, 0.7547309091649913, 0.6891133356559334, 0.5138009206735611, 0.4235035940764227]\n",
      "0.48357586379616957\n",
      "(0.5415589039152386, 0.7916666666666666)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "for i = 1:num_epoch_ConjugateGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_ConjugateGD, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")\n",
    "println(epoch_vs_losses)\n",
    "println(train_scores(Ws))\n",
    "println(test_scores(Ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = NesterovMomentum_SGD done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_NesterovMomentum_SGD = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "step_size = 0.000002\n",
    "optimizer_NesterovMomentum_SGD = NesterovMomentum_SGD(step_size, 0.99, f, nothing, nothing)\n",
    "init!(optimizer_NesterovMomentum_SGD, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  5.3273580341476094\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.21834284280729735\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0539453401972226\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10407222096563036\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.2651539916825724\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.2444879667076695\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.16743282826623065\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.24858436288275737\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.22904537169847394\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.14926196147915705\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1412912569092202\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.32081800651145786\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.18598465476065051\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1359690005618052\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1861783775221717\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.20281007509547785\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.15770656384543225\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.32094815452485426\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.143751083190812\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.20890588890068298\n",
      "Finished\n",
      "Any[3.2478182569668763, 1.5073105799524276, 1.1706560160314312, 0.8512706581860503, 0.7439251224030555, 0.6479145176736252, 0.6090152965735263, 0.5616231290102505, 0.5312585800962715, 0.4430113753393696, 0.4424356424796815, 0.42992856965864484, 0.39109535865259565, 0.3730720431825076, 0.3607623115236548, 0.32141748996470104, 0.37369389963113286, 0.36159723729262316, 0.36128107225339506, 0.4006233324577647]\n",
      "0.3370669343828043\n",
      "(0.3413911683249613, 0.875)\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_NesterovMomentum_SGD\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_NesterovMomentum_SGD, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores(Ws)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For **1.1.NesterovMomentum_SGD**, the **epoch_vs_losses** = Any[3.2478182569668763, 1.5073105799524276, 1.1706560160314312, 0.8512706581860503, 0.7439251224030555, 0.6479145176736252, 0.6090152965735263, 0.5616231290102505, 0.5312585800962715, 0.4430113753393696, 0.4424356424796815, 0.42992856965864484, 0.39109535865259565, 0.3730720431825076, 0.3607623115236548, 0.32141748996470104, 0.37369389963113286, 0.36159723729262316, 0.36128107225339506, 0.4006233324577647]\n",
      "This is with a inital α = 2.0e-6\n",
      "And after 20 epochs, **loss on whole train set** = 0.4088383224058831\n",
      "And **loss on whole test set** = 0.3757932035120766\n",
      "**accuracy ** = 0.875\n",
      "which is 87.5%"
     ]
    }
   ],
   "source": [
    "print(\"For **1.1.NesterovMomentum_SGD**, the **epoch_vs_losses** = \")\n",
    "println(epoch_vs_losses)\n",
    "println(\"This is with a inital α = $step_size\")\n",
    "print(\"And after 20 epochs, **loss on whole train set** = \")\n",
    "println(train_scores(Ws))\n",
    "print(\"And **loss on whole test set** = \")\n",
    "score = test_scores(Ws)\n",
    "println(score[1])\n",
    "print(\"**accuracy ** = \")\n",
    "println(score[2])\n",
    "print(\"which is $(round(score[2]*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Adadelta done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Adadelta = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Adadelta = Adadelta(0.95, 0.95, 0.00005, nothing, nothing, nothing)\n",
    "init!(optimizer_Adadelta, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.5828262243268345\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.5274823730036112\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.125848705860908\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07696882303822083\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.13812042667223623\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.12980819503446928\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08581844794564493\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09922549728314634\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0577387954769318\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.033383064412105284\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.14528013795305603\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.027403913069941433\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.17908837361013547\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03475044662938916\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.02637533660983657\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.026424161736713983\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.04474898688628905\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.024298653205284654\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.016380726845647127\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.025674716620944536\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_Adadelta\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_Adadelta, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For **1.1.Adadelta**, the **epoch_vs_losses** = Any[2.553577984207015, 1.6125522425364232, 1.0939591541606564, 0.8819035625498827, 0.7525594753819873, 0.6999908940606767, 0.6268413108439863, 0.5796714961470538, 0.5622541402105504, 0.5690850832789728, 0.5777792444993618, 0.6186225185544469, 0.6250890981400434, 0.6194836064239608, 0.5761945914478452, 0.5679918389765839, 0.5576121705503273, 0.5527786184529979, 0.5435231922891999, 0.496742623298741]\n",
      "And after 20 epochs, **loss on whole train set** = 0.4324020506240093\n",
      "And **loss on whole test set** = 0.49458979226324246\n",
      "**accuracy ** = 0.8333333333333334\n",
      "which is 83.33%"
     ]
    }
   ],
   "source": [
    "print(\"For **1.1.Adadelta**, the **epoch_vs_losses** = \")\n",
    "println(epoch_vs_losses)\n",
    "# println(\"This is with a inital α = $step_size\")\n",
    "print(\"And after 20 epochs, **loss on whole train set** = \")\n",
    "println(train_scores(Ws))\n",
    "print(\"And **loss on whole test set** = \")\n",
    "score = test_scores(Ws)\n",
    "println(score[1])\n",
    "print(\"**accuracy ** = \")\n",
    "println(score[2])\n",
    "print(\"which is $(round(score[2]*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = Nelder-Mead\n",
    "\\*\\*\\*\\*\\*\\*\\*\\*\\*\\***NOT WORKING**\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_Nelder_Mead = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_Nelder_Mead = Nelder_Mead(f, nothing)\n",
    "init!(optimizer_Nelder_Mead)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# for i = 1:num_epoch_Nelder_Mead\n",
    "#     println(\"Epoch# $i\")\n",
    "#     Ws, curr_loss = epoch_step(optimizer_Nelder_Mead, Ws; hidden_layer_size = 16)\n",
    "#     if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "#         break\n",
    "#     end\n",
    "# end\n",
    "# println(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = DFP done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_DFP = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_DFP = DFP(nothing, f, nothing)\n",
    "init!(optimizer_DFP, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0005077023275417916\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  7.56460182822734e-5\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0005920229500069326\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.00023028369988820017\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.4970199827674456\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  2.6606923472969646\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.5921671010108573\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.8615862534646722\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  2.13896124015488\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.43802570856129\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.2251302452429793\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.3079975628617941\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.6912598376080663\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.1851344014503262\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.512191672622657\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.6241296461979402\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.7346803683956387\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.0038906105072223\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.3463967506890147\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.4560940761656525\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_DFP\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_DFP, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For **1.1.DFP**, the **epoch_vs_losses** = Any[1.1255128843134643, 1.8139494227858344, 1.3526529496901767, 3.380623149147914, 2.1870365732369335, 2.1962281909787174, 2.327573888441127, 1.5937501503171827, 1.3883283540062044, 1.4241786800965988, 1.404759306207787, 1.512829740434173, 1.405419025345664, 1.6003276016516415, 1.4654341430002447, 1.5957913522910432, 1.5978547616096979, 1.4357445937524835, 1.582453606571444, 1.6458025409788217]\n",
      "And after 20 epochs, **loss on whole train set** = 1.66058091381958\n",
      "And **loss on whole test set** = 0.8083977219731779\n",
      "**accuracy ** = 0.5416666666666666\n",
      "which is 54.17%"
     ]
    }
   ],
   "source": [
    "print(\"For **1.1.DFP**, the **epoch_vs_losses** = \")\n",
    "println(epoch_vs_losses)\n",
    "# println(\"This is with a inital α = $step_size\")\n",
    "print(\"And after 20 epochs, **loss on whole train set** = \")\n",
    "println(train_scores(Ws))\n",
    "print(\"And **loss on whole test set** = \")\n",
    "score = test_scores(Ws)\n",
    "println(score[1])\n",
    "print(\"**accuracy ** = \")\n",
    "println(score[2])\n",
    "print(\"which is $(round(score[2]*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = semi-DFP done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_semi_DFP = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "optimizer_semi_DFP = semi_DFP(0.003, nothing, f, nothing)\n",
    "init!(optimizer_semi_DFP, num_Ws)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07302259271332447\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09745239667577277\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08715939925414744\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08651049905763371\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06214930733800068\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07059796724594071\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06902894618111764\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07834985178877234\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11693189663350605\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09256258961455582\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08846369600603982\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07740507560179798\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0722529341799195\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11330392634653219\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06647876837914805\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06705830169663135\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06441014216217399\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.057638956730820014\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09745204802443347\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08514251746670365\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Train\n",
    "for i = 1:num_epoch_semi_DFP\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_semi_DFP, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For **1.1.smei-DFP**, the **epoch_vs_losses** = Any[0.8970499335079404, 0.9319714761618514, 0.8973227504729017, 0.9392187105184473, 0.9042583386748708, 0.9193498702244056, 0.8921557179160868, 0.8768993753400174, 0.9392332085728637, 0.9131188643217615, 0.8886630170312965, 0.8970369292550615, 0.9462036079626132, 0.8885676765179372, 0.95813977168375, 0.8786778355827368, 0.9176907252470387, 0.9279673608297146, 0.9722710553827264, 0.9243282103059455]\n",
      "And after 20 epochs, **loss on whole train set** = 0.8703657101593534\n",
      "And **loss on whole test set** = 0.8128235528431832\n",
      "**accuracy ** = 0.6666666666666666\n",
      "which is 66.67%"
     ]
    }
   ],
   "source": [
    "print(\"For **1.1.smei-DFP**, the **epoch_vs_losses** = \")\n",
    "println(epoch_vs_losses)\n",
    "# println(\"This is with a inital α = $step_size\")\n",
    "print(\"And after 20 epochs, **loss on whole train set** = \")\n",
    "println(train_scores(Ws))\n",
    "print(\"And **loss on whole test set** = \")\n",
    "score = test_scores(Ws)\n",
    "println(score[1])\n",
    "print(\"**accuracy ** = \")\n",
    "println(score[2])\n",
    "print(\"which is $(round(score[2]*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer = LBFGS done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_LimitedMemoryBFGS = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vs_losses = []\n",
    "curr_train_loss = 0.0\n",
    "# Initializing \n",
    "optimizer_LimitedMemoryBFGS = LimitedMemoryBFGS(20, f, nothing, nothing, nothing, nothing)\n",
    "init!(optimizer_LimitedMemoryBFGS)\n",
    "Ws = deepcopy(Ws_copy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 1\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  5.232887644419244\n",
      "Epoch# 2\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  5.889627086106204\n",
      "Epoch# 3\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  4.042893749520418\n",
      "Epoch# 4\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  2.416337190510231\n",
      "Epoch# 5\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  2.112620454521046\n",
      "Epoch# 6\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.7874131305179433\n",
      "Epoch# 7\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.038379665175128\n",
      "Epoch# 8\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.7350546989473191\n",
      "Epoch# 9\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  1.044094664282125\n",
      "Epoch# 10\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.6849317864763043\n",
      "Epoch# 11\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.31421167680237533\n",
      "Epoch# 12\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1526137636398385\n",
      "Epoch# 13\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.2075140831998297\n",
      "Epoch# 14\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11996379941931246\n",
      "Epoch# 15\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.050337592300801044\n",
      "Epoch# 16\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09400911100252814\n",
      "Epoch# 17\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11196499355932693\n",
      "Epoch# 18\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07073400386989318\n",
      "Epoch# 19\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.05115741590044182\n",
      "Epoch# 20\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06503007638617746\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for i = 1:num_epoch_LimitedMemoryBFGS\n",
    "    println(\"Epoch# $i\")\n",
    "    Ws, curr_loss = epoch_step(optimizer_LimitedMemoryBFGS, Ws; hidden_layer_size = 16)\n",
    "    if !isnan(curr_loss)\n",
    "        curr_train_loss = train_scores(Ws)\n",
    "        push!(epoch_vs_losses, curr_train_loss)\n",
    "    end\n",
    "    if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For **1.1.LimitedMemoryBFGS**, the **epoch_vs_losses** = Any[2.883047610020867, 2.0065541170126546, 1.620689946502641, 1.3931216509495734, 1.2552017494677035, 1.2270935159300604, 1.126629882134411, 1.1207909985479536, 1.1092457214265974, 1.1224675442328351, 1.1693821413625354, 1.2321269154014602, 1.252417392764308, 1.245077746298742, 1.289158411883179, 1.3057483584215588, 1.3224399015383812, 1.2449846035521757, 1.4237011937155306, 1.3463238796991803]\n",
      "And after 20 epochs, **loss on whole train set** = 1.3145815760814066\n",
      "And **loss on whole test set** = 1.4831769358410105\n",
      "**accuracy ** = 0.625\n",
      "which is 62.5%"
     ]
    }
   ],
   "source": [
    "print(\"For **1.1.LimitedMemoryBFGS**, the **epoch_vs_losses** = \")\n",
    "println(epoch_vs_losses)\n",
    "# println(\"This is with a inital α = $step_size\")\n",
    "print(\"And after 20 epochs, **loss on whole train set** = \")\n",
    "println(train_scores(Ws))\n",
    "print(\"And **loss on whole test set** = \")\n",
    "score = test_scores(Ws)\n",
    "println(score[1])\n",
    "print(\"**accuracy ** = \")\n",
    "println(score[2])\n",
    "print(\"which is $(round(score[2]*100, digits=2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to produces scores, etc for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter_scores (generic function with 1 method)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_scores(Ws)\n",
    "    ct_correct = 0\n",
    "    ct_total = 0\n",
    "    loss = []\n",
    "    for i = 1 : size(test_x_data)[1]\n",
    "        x = test_x_data[i]\n",
    "        y = test_y_data[i]\n",
    "        total_loss, hidden_outs, Y_pred, scores = f(x, y, _unwrap(Ws))\n",
    "        # println(\"y_truth = $y, scores = $scores\")\n",
    "        push!(loss, total_loss)\n",
    "        ct_total += 1\n",
    "        # println(\"$scores -> $(argmax(scores)), $y -> $(argmax(y)[1])\")\n",
    "        if argmax(scores) == argmax(y)[1]\n",
    "            ct_correct += 1\n",
    "        end\n",
    "    end\n",
    "    # println(loss)\n",
    "    return mean(loss), ct_correct/ct_total\n",
    "end\n",
    "function train_scores(Ws)\n",
    "    loss = []\n",
    "    for i = 1 : size(train_x_data)[1]\n",
    "        x = train_x_data[i]\n",
    "        y = train_y_data[i]\n",
    "        total_loss, hidden_outs, Y_pred, scores = f(x, y, _unwrap(Ws))\n",
    "        # println(\"y_truth = $y, scores = $scores\")\n",
    "        push!(loss, total_loss)\n",
    "    end\n",
    "    return mean(loss)\n",
    "end\n",
    "function filter_scores(test_losses, accs)\n",
    "    ct_all = length(test_losses)\n",
    "    ct_nan = 0\n",
    "    # new_train_losses = []\n",
    "    new_test_losses = []\n",
    "    new_accs = []\n",
    "    for i = 1 : ct_all\n",
    "        if isnan(test_losses[i])\n",
    "            ct_nan += 1\n",
    "        else\n",
    "            # push!(new_train_losses, train_losses[i])\n",
    "            push!(new_test_losses, test_losses[i])\n",
    "            push!(new_accs, accs[i])\n",
    "        end\n",
    "    end\n",
    "    return new_test_losses, new_accs, ct_all-ct_nan\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Performance after a same number of epochs = 20 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_1 = 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 : Train 100 times on a same inital NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created!\n",
      "Inital loss = 4.8123363545039295\n"
     ]
    }
   ],
   "source": [
    "loss_ε = 0.1\n",
    "# Initializing \n",
    "Y_pred, hidden_outs, Ws_copy = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "# println(Y_pred)\n",
    "scores, losses, total_loss = softmax_loss(Y_pred, train_y_data[1])\n",
    "∇_copy = ∇(train_x_data[1], train_y_data[1], Ws_copy, scores, Y_pred, hidden_outs)\n",
    "println(\"New model created!\\nInital loss = $total_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Steepest gradient descent done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# optimizer_SteepestGD = SteepestGD(0.0025, nothing)\n",
    "optimizer_SteepestGD = SteepestGD(0.0003, nothing)\n",
    "# Start training\n",
    "train_losses_20Epochs_SGD = []\n",
    "test_losses_20Epochs_SGD = []\n",
    "accuracies_20Epochs_SGD = []\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    init!(optimizer_SteepestGD)\n",
    "    # Train\n",
    "    curr_loss = 0.0\n",
    "    curr_train_loss = 0.0\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_SteepestGD, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        # println(mean(_unwrap(Ws)))\n",
    "        if !isnan(curr_loss)\n",
    "            curr_train_loss = train_scores(Ws)\n",
    "            # println(curr_train_loss)\n",
    "        end\n",
    "        if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_SGD, curr_train_loss)\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_SGD, loss_curr)\n",
    "    push!(accuracies_20Epochs_SGD, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_SGD, accuracies_20Epochs_SGD, ct_valid_loss = filter_scores(test_losses_20Epochs_SGD, accuracies_20Epochs_SGD)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986\n"
     ]
    }
   ],
   "source": [
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SteepestGD after 100 individual trains on the same inital NN:\n",
      "\t3 runs converged.\n",
      "\tTrain loss = NaN ± NaN\n",
      "\tTest loss = 4.2783 ± 0.2652\n",
      "\tAccuracy = 59.72% ± 0.05%\n"
     ]
    }
   ],
   "source": [
    "println(\"SteepestGD after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_SGD); digits = 4)) ± $(round(std(train_losses_20Epochs_SGD); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_SGD); digits = 4)) ± $(round(std(test_losses_20Epochs_SGD); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_SGD)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_SGD); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adagrad done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With inital step size = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "optimizer_Adagrad = Adagrad(0.01, 1e-5, nothing, nothing)\n",
    "# Start training\n",
    "train_losses_20Epochs_Adagrad = []\n",
    "losses_20Epochs_Adagrad = []\n",
    "accuracies_20Epochs_Adagrad = []\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    init!(optimizer_Adagrad, num_Ws)\n",
    "    # Train\n",
    "    curr_loss = 0.0\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adagrad, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adagrad, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(losses_20Epochs_Adagrad, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adagrad, acc_curr)\n",
    "end\n",
    "losses_20Epochs_Adagrad, accuracies_20Epochs_Adagrad, ct_valid_loss = \n",
    "filter_scores(losses_20Epochs_Adagrad, accuracies_20Epochs_Adagrad)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad after 100 individual trains on the same inital NN:\n",
      "\t100 runs converged.\n",
      "\tTrain loss = 1.2115 ± 0.1189\n",
      "\tTest loss = 1.2235 ± 0.15\n",
      "\tAccuracy = 56.12% ± 0.06%\n"
     ]
    }
   ],
   "source": [
    "println(\"Adagrad after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adagrad); digits = 4)) ± $(round(std(train_losses_20Epochs_Adagrad); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(losses_20Epochs_Adagrad); digits = 4)) ± $(round(std(losses_20Epochs_Adagrad); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adagrad)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adagrad); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With inital step size = 0.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "optimizer_Adagrad = Adagrad(0.016, 1e-5, nothing, nothing)\n",
    "# Start training\n",
    "train_losses_20Epochs_Adagrad = []\n",
    "losses_20Epochs_Adagrad = []\n",
    "accuracies_20Epochs_Adagrad = []\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    init!(optimizer_Adagrad, num_Ws)\n",
    "    # Train\n",
    "    curr_loss = 0.0\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adagrad, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adagrad, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(losses_20Epochs_Adagrad, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adagrad, acc_curr)\n",
    "end\n",
    "losses_20Epochs_Adagrad, accuracies_20Epochs_Adagrad, ct_valid_loss = \n",
    "filter_scores(losses_20Epochs_Adagrad, accuracies_20Epochs_Adagrad)\n",
    "println(\"Finished\")\n",
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Adagrad after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adagrad); digits = 4)) ± $(round(std(train_losses_20Epochs_Adagrad); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(losses_20Epochs_Adagrad); digits = 4)) ± $(round(std(losses_20Epochs_Adagrad); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adagrad)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adagrad); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adam done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "optimizer_Adam = Adam(0.003, 0.9, 0.999, 1e-4, nothing, nothing, nothing)\n",
    "# Start training\n",
    "train_losses_20Epochs_Adam = []\n",
    "test_losses_20Epochs_Adam = []\n",
    "accuracies_20Epochs_Adam = []\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    init!(optimizer_Adam, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adam, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adam, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_Adam, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adam, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_Adam, accuracies_20Epochs_Adam, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_Adam, accuracies_20Epochs_Adam)\n",
    "println(\"Finished\")\n",
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam after 100 individual trains on the same inital NN:\n",
      "\t100 runs converged.\n",
      "\tTrain loss = 0.7258 ± 0.2388\n",
      "\tTest loss = 0.8585 ± 0.297\n",
      "\tAccuracy = 74.54% ± 0.05%\n"
     ]
    }
   ],
   "source": [
    "println(\"Adam after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adam); digits = 4)) ± $(round(std(train_losses_20Epochs_Adam); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_Adam); digits = 4)) ± $(round(std(test_losses_20Epochs_Adam); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adam)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adam); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = ConjugateGD done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  Inf\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 2 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.033495011704952456\n",
      "run# 3 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.007608786974418685\n",
      "run# 4 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.023770864086609483\n",
      "run# 5 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.052053119409005956\n",
      "run# 6 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0018181439089769034\n",
      "run# 7 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 8 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.056387719643460156\n",
      "run# 9 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.021010656637458398\n",
      "run# 10 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0394769573324159\n",
      "run# 11 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06971532767524974\n",
      "run# 12 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.038615330434547526\n",
      "run# 13 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.001046413406138759\n",
      "run# 14 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.12791100266163538\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0011033156400298172\n",
      "run# 15 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.18172087543012413\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.04011092637946335\n",
      "run# 16 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.16233235327800544\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.021688751801564033\n",
      "run# 17 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.008986899101841188\n",
      "run# 18 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10177999880794762\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 19 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07249487280602392\n",
      "run# 20 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.009838393435414058\n",
      "run# 21 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.030869960068648474\n",
      "run# 22 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.018469364573845754\n",
      "run# 23 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.01248754769182269\n",
      "run# 24 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.21924611749048453\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.007891676605093734\n",
      "run# 25 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.04534824026180054\n",
      "run# 26 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03411377568608255\n",
      "run# 27 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06269375224022149\n",
      "run# 28 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 29 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10827904210490981\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 30 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 31 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 32 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08494438506638713\n",
      "run# 33 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 34 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.009095160442414435\n",
      "run# 35 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 36 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 37 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  199.0792841492529\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 38 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 39 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0165433456665663\n",
      "run# 40 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.008478402046530102\n",
      "run# 41 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.05529927772525937\n",
      "run# 42 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.023162919932094125\n",
      "run# 43 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07881331750786895\n",
      "run# 44 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1037417526923831\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.002003430791367264\n",
      "run# 45 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.008665177419215497\n",
      "run# 46 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.02260690870863079\n",
      "run# 47 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.056180652455650994\n",
      "run# 48 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.26449978376375566\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 49 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.008632362660362592\n",
      "run# 50 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03330626444728379\n",
      "run# 51 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.023014008832535498\n",
      "run# 52 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.044222229128815536\n",
      "run# 53 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0029662898034775414\n",
      "run# 54 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07004014628761436\n",
      "run# 55 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.02454439118667723\n",
      "run# 56 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.08277384826220703\n",
      "run# 57 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03936004564389315\n",
      "run# 58 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 59 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.054311545568151716\n",
      "run# 60 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.018603044043849173\n",
      "run# 61 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0001959190942322919\n",
      "run# 62 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07932398306948787\n",
      "run# 63 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.05928377206865645\n",
      "run# 64 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.18076511074479754\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 65 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10045894320869714\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 66 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.005618948552945054\n",
      "run# 67 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.026994768909203003\n",
      "run# 68 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.014354379778152066\n",
      "run# 69 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.39710196928341374\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 70 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11896680440876188\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.07934153269596225\n",
      "run# 71 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.11503108671154104\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 72 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.14311268363705698\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 73 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 74 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1487752973005328\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  71.27756491093497\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  64.08276655642888\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  58.38767225201581\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  53.57595106788157\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  47.741133330697075\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  74.15734634687858\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  65.73055312221946\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 75 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.030065816248115013\n",
      "run# 76 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.06751499286272004\n",
      "run# 77 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03425210652388109\n",
      "run# 78 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.03812372811197\n",
      "run# 79 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.13483166993192805\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.48570983539051094\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  13.353922531794636\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  7.673949177251647\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 80 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.01309011848666062\n",
      "run# 81 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.0518735328722838\n",
      "run# 82 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.014066006584036582\n",
      "run# 83 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 84 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.026198898292824364\n",
      "run# 85 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.020805756722747537\n",
      "run# 86 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.018167496939615495\n",
      "run# 87 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.002082170238225614\n",
      "run# 88 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.4542739115661924\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.4427868568743018\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 89 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.012633415004841442\n",
      "run# 90 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.002108073606125893\n",
      "run# 91 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.025514657424617313\n",
      "run# 92 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.10686690063211257\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.048578201377289625\n",
      "run# 93 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1080279626833741\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 94 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.12485822950570945\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 95 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 96 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.1593159550305733\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "run# 97 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.029774499467239583\n",
      "run# 98 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.025882002583743832\n",
      "run# 99 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  0.09283869840184167\n",
      "run# 100 --> batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  60.6280959718352\n",
      "batch# 1 --> batch# 2 --> batch# 3 --> batch# 4 --> batch# 5 --> batch# 6 --> batch# 7 --> batch# 8 --> \n",
      "\ttrain loss =  NaN\n",
      "Finished\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_ConjugateGD = []\n",
    "test_losses_20Epochs_ConjugateGD = []\n",
    "accuracies_20Epochs_ConjugateGD = []\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    optimizer_ConjugateGD = ConjugateGD(0.00875, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "    init!(optimizer_ConjugateGD)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_ConjugateGD, Ws; hidden_layer_size = 16, print_info = true)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_ConjugateGD, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_ConjugateGD, loss_curr)\n",
    "    push!(accuracies_20Epochs_ConjugateGD, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_ConjugateGD, accuracies_20Epochs_ConjugateGD, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_ConjugateGD, accuracies_20Epochs_ConjugateGD)\n",
    "println(\"Finished\")\n",
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConjugateGD after 100 individual trains on the same inital NN:\n",
      "\t100 runs converged.\n",
      "\tTrain loss = 2.988 ± 0.1345\n",
      "\tTest loss = 2.795 ± 0.1813\n",
      "\tAccuracy = 66.46% ± 0.01%\n"
     ]
    }
   ],
   "source": [
    "println(\"ConjugateGD after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_ConjugateGD); digits = 4)) ± $(round(std(train_losses_20Epochs_ConjugateGD); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_ConjugateGD); digits = 4)) ± $(round(std(test_losses_20Epochs_ConjugateGD); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_ConjugateGD)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_ConjugateGD); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = NesterovMomentum_SGD done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n",
      "1469\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_NesterovMomentum_SGD= []\n",
    "test_losses_20Epochs_NesterovMomentum_SGD = []\n",
    "accuracies_20Epochs_NesterovMomentum_SGD = []\n",
    "optimizer_NesterovMomentum_SGD = NesterovMomentum_SGD(0.000002, 0.99, f, nothing, nothing)\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "init!(optimizer_NesterovMomentum_SGD, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_NesterovMomentum_SGD, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_NesterovMomentum_SGD, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_NesterovMomentum_SGD, loss_curr)\n",
    "    push!(accuracies_20Epochs_NesterovMomentum_SGD, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_NesterovMomentum_SGD, accuracies_20Epochs_NesterovMomentum_SGD, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_NesterovMomentum_SGD, accuracies_20Epochs_NesterovMomentum_SGD)\n",
    "println(\"Finished\")\n",
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NesterovMomentum_SGD after 100 individual trains on the same inital NN:\n",
      "\t100 runs converged.\n",
      "\tTrain loss = 0.3786 ± 0.0393\n",
      "\tTest loss = 0.3888 ± 0.0554\n",
      "\tAccuracy = 84.5% ± 0.05%\n"
     ]
    }
   ],
   "source": [
    "println(\"NesterovMomentum_SGD after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_NesterovMomentum_SGD); digits = 4)) ± $(round(std(train_losses_20Epochs_NesterovMomentum_SGD); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_NesterovMomentum_SGD); digits = 4)) ± $(round(std(test_losses_20Epochs_NesterovMomentum_SGD); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_NesterovMomentum_SGD)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_NesterovMomentum_SGD); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adadelta done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_Adadelta= []\n",
    "test_losses_20Epochs_Adadelta = []\n",
    "accuracies_20Epochs_Adadelta = []\n",
    "optimizer_Adadelta = Adadelta(0.95, 0.95, 0.00005, nothing, nothing, nothing)\n",
    "ct_epochs = 0\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    init!(optimizer_Adadelta, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adadelta, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adadelta, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_Adadelta, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adadelta, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_Adadelta, accuracies_20Epochs_Adadelta, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_Adadelta, accuracies_20Epochs_Adadelta)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adadelta after 100 individual trains on the same inital NN:\n",
      "\t100 runs converged.\n",
      "\tTrain loss = 0.439 ± 0.0569\n",
      "\tTest loss = 0.5172 ± 0.0629\n",
      "\tAccuracy = 81.42% ± 0.03%\n"
     ]
    }
   ],
   "source": [
    "println(\"Adadelta after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adadelta); digits = 4)) ± $(round(std(train_losses_20Epochs_Adadelta); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_Adadelta); digits = 4)) ± $(round(std(test_losses_20Epochs_Adadelta); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adadelta)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adadelta); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = DFP done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> Finished\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_DFP= []\n",
    "test_losses_20Epochs_DFP = []\n",
    "accuracies_20Epochs_DFP = []\n",
    "optimizer_DFP = DFP(nothing, f, nothing)\n",
    "ct_epochs = 0\n",
    "for i = 1 : 5\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    init!(optimizer_DFP, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_DFP, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_DFP, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_DFP, loss_curr)\n",
    "    push!(accuracies_20Epochs_DFP, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_DFP, accuracies_20Epochs_DFP, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_DFP, accuracies_20Epochs_DFP)\n",
    "println(\"Finished\")\n",
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFP after 100 individual trains on the same inital NN:\n",
      "\t5 runs converged.\n",
      "\tTrain loss = 1.3765 ± 0.6059\n",
      "\tTest loss = 1.2537 ± 0.4109\n",
      "\tAccuracy = 68.33% ± 0.2%\n"
     ]
    }
   ],
   "source": [
    "println(\"DFP after 100 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_DFP); digits = 4)) ± $(round(std(train_losses_20Epochs_DFP); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_DFP); digits = 4)) ± $(round(std(test_losses_20Epochs_DFP); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_DFP)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_DFP); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = LBFGS done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> Finished\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_LBFGS= []\n",
    "test_losses_20Epochs_LBFGS = []\n",
    "accuracies_20Epochs_LBFGS = []\n",
    "optimizer_LimitedMemoryBFGS = LimitedMemoryBFGS(20, f, nothing, nothing, nothing, nothing)\n",
    "ct_epochs = 0\n",
    "for i = 1 : 5\n",
    "    print(\"run# $i --> \")\n",
    "    Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    init!(optimizer_LimitedMemoryBFGS)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        ct_epochs += 1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_LimitedMemoryBFGS, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_LBFGS, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_LBFGS, loss_curr)\n",
    "    push!(accuracies_20Epochs_LBFGS, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_LBFGS, accuracies_20Epochs_LBFGS, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_LBFGS, accuracies_20Epochs_LBFGS)\n",
    "println(\"Finished\")\n",
    "println(ct_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LimitedMemoryBFGS after 5 individual trains on the same inital NN:\n",
      "\t5 runs converged.\n",
      "\tTrain loss = 1.22 ± 0.53\n",
      "\tTest loss = 1.45 ± 0.21\n",
      "\tAccuracy = 66.67% ± 0.73%\n"
     ]
    }
   ],
   "source": [
    "println(\"LimitedMemoryBFGS after 5 individual trains on the same inital NN:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_LBFGS); digits = 4)) ± $(round(std(train_losses_20Epochs_LBFGS); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_LBFGS); digits = 4)) ± $(round(std(test_losses_20Epochs_LBFGS); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_LBFGS)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_LBFGS); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 : Train 100 times on randomly initialized NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Steepest gradient descent done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> "
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_SGD = []\n",
    "test_losses_20Epochs_SGD = []\n",
    "accuracies_20Epochs_SGD = []\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    dumb1, dumb2, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    optimizer_SteepestGD = SteepestGD(0.0002, nothing)\n",
    "    init!(optimizer_SteepestGD)\n",
    "    # Train\n",
    "    curr_loss = 0.0\n",
    "    curr_train_loss = 0.0\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_SteepestGD, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        # println(mean(_unwrap(Ws)))\n",
    "        if !isnan(curr_loss)\n",
    "            curr_train_loss = train_scores(Ws)\n",
    "            # println(curr_train_loss)\n",
    "        end\n",
    "        if curr_train_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_SGD, curr_train_loss)\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_SGD, loss_curr)\n",
    "    push!(accuracies_20Epochs_SGD, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_SGD, accuracies_20Epochs_SGD, ct_valid_loss = filter_scores(test_losses_20Epochs_SGD, accuracies_20Epochs_SGD)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SteepestGD after 100 individual trains on 100 randomly initialized NNs:\n",
      "\t73 runs converged.\n",
      "\tTrain loss = 0.8863 ± 0.6263\n",
      "\tTest loss = 1.0504 ± 0.689\n",
      "\tAccuracy = 70.95% ± 0.13%\n"
     ]
    }
   ],
   "source": [
    "println(\"SteepestGD after 100 individual trains on 100 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_SGD); digits = 4)) ± $(round(std(train_losses_20Epochs_SGD); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_SGD); digits = 4)) ± $(round(std(test_losses_20Epochs_SGD); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_SGD)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_SGD); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adagrad done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_Adagrad = []\n",
    "losses_20Epochs_Adagrad = []\n",
    "accuracies_20Epochs_Adagrad = []\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    dumb1, dumb2, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    optimizer_Adagrad = Adagrad(0.00675, 1e-5, nothing, nothing)\n",
    "    init!(optimizer_Adagrad, num_Ws)\n",
    "    # Train\n",
    "    curr_loss = 0.0\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adagrad, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adagrad, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(losses_20Epochs_Adagrad, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adagrad, acc_curr)\n",
    "end\n",
    "losses_20Epochs_Adagrad, accuracies_20Epochs_Adagrad, ct_valid_loss = \n",
    "filter_scores(losses_20Epochs_Adagrad, accuracies_20Epochs_Adagrad)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad after 100 individual trains on 100 randomly initialized NNs:\n",
      "\t75 runs converged.\n",
      "\tTrain loss = 1.1799 ± 2.1373\n",
      "\tTest loss = 1.3938 ± 2.2428\n",
      "\tAccuracy = 74.22% ± 0.15%\n"
     ]
    }
   ],
   "source": [
    "println(\"Adagrad after 100 individual trains on 100 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adagrad); digits = 4)) ± $(round(std(train_losses_20Epochs_Adagrad); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(losses_20Epochs_Adagrad); digits = 4)) ± $(round(std(losses_20Epochs_Adagrad); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adagrad)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adagrad); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adam done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_Adam = []\n",
    "test_losses_20Epochs_Adam = []\n",
    "accuracies_20Epochs_Adam = []\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    dumb1, dumb2, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    optimizer_Adam = Adam(0.0005, 0.9, 0.999, 1e-4, nothing, nothing, nothing)\n",
    "    init!(optimizer_Adam, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adam, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adam, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_Adam, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adam, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_Adam, accuracies_20Epochs_Adam, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_Adam, accuracies_20Epochs_Adam)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam after 100 individual trains on 100 randomly initialized NNs:\n",
      "\t99 runs converged.\n",
      "\tTrain loss = 1.586 ± 2.7111\n",
      "\tTest loss = 1.8236 ± 2.879\n",
      "\tAccuracy = 70.71% ± 0.15%\n"
     ]
    }
   ],
   "source": [
    "println(\"Adam after 100 individual trains on 100 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adam); digits = 4)) ± $(round(std(train_losses_20Epochs_Adam); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_Adam); digits = 4)) ± $(round(std(test_losses_20Epochs_Adam); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adam)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adam); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = ConjugateGD done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_ConjugateGD = []\n",
    "test_losses_20Epochs_ConjugateGD = []\n",
    "accuracies_20Epochs_ConjugateGD = []\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Y_pred, hidden_outs, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    scores, losses, total_loss = softmax_loss(Y_pred, train_y_data[1])\n",
    "    ∇_copy = ∇(train_x_data[1], train_y_data[1], Ws, scores, Y_pred, hidden_outs)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    optimizer_ConjugateGD = ConjugateGD(mean(_unwrap(Ws))/500, _unwrap(deepcopy(∇_copy)), f, nothing, nothing)\n",
    "    init!(optimizer_ConjugateGD)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_ConjugateGD, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_ConjugateGD, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_ConjugateGD, loss_curr)\n",
    "    push!(accuracies_20Epochs_ConjugateGD, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_ConjugateGD, accuracies_20Epochs_ConjugateGD, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_ConjugateGD, accuracies_20Epochs_ConjugateGD)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConjugateGD after 100 individual trains on 100 randomly initialized NNs:\n",
      "\t94 runs converged.\n",
      "\tTrain loss = 2.79 ± 0.62\n",
      "\tTest loss = 3.58 ± 4.29\n",
      "\tAccuracy = 66.67% ± 0.19%\n"
     ]
    }
   ],
   "source": [
    "println(\"ConjugateGD after 100 individual trains on 100 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_ConjugateGD); digits = 4)) ± $(round(std(train_losses_20Epochs_ConjugateGD); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_ConjugateGD); digits = 4)) ± $(round(std(test_losses_20Epochs_ConjugateGD); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_ConjugateGD)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_ConjugateGD); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = NesterovMomentum_SGD done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_NesterovMomentum_SGD= []\n",
    "test_losses_20Epochs_NesterovMomentum_SGD = []\n",
    "accuracies_20Epochs_NesterovMomentum_SGD = []\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Y_pred, hidden_outs, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    optimizer_NesterovMomentum_SGD = NesterovMomentum_SGD(0.000002, 0.99, f, nothing, nothing)\n",
    "    init!(optimizer_NesterovMomentum_SGD, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_NesterovMomentum_SGD, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_NesterovMomentum_SGD, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_NesterovMomentum_SGD, loss_curr)\n",
    "    push!(accuracies_20Epochs_NesterovMomentum_SGD, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_NesterovMomentum_SGD, accuracies_20Epochs_NesterovMomentum_SGD, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_NesterovMomentum_SGD, accuracies_20Epochs_NesterovMomentum_SGD)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NesterovMomentum_SGD after 100 individual trains on 100 randomly initialized NNs:\n",
      "\t79 runs converged.\n",
      "\tTrain loss = 1.2243 ± 2.1855\n",
      "\tTest loss = 1.3638 ± 2.147\n",
      "\tAccuracy = 73.36% ± 0.15%\n"
     ]
    }
   ],
   "source": [
    "println(\"NesterovMomentum_SGD after 100 individual trains on 100 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_NesterovMomentum_SGD); digits = 4)) ± $(round(std(train_losses_20Epochs_NesterovMomentum_SGD); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_NesterovMomentum_SGD); digits = 4)) ± $(round(std(test_losses_20Epochs_NesterovMomentum_SGD); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_NesterovMomentum_SGD)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_NesterovMomentum_SGD); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adadelta done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> run# 11 --> run# 12 --> run# 13 --> run# 14 --> run# 15 --> run# 16 --> run# 17 --> run# 18 --> run# 19 --> run# 20 --> run# 21 --> run# 22 --> run# 23 --> run# 24 --> run# 25 --> run# 26 --> run# 27 --> run# 28 --> run# 29 --> run# 30 --> run# 31 --> run# 32 --> run# 33 --> run# 34 --> run# 35 --> run# 36 --> run# 37 --> run# 38 --> run# 39 --> run# 40 --> run# 41 --> run# 42 --> run# 43 --> run# 44 --> run# 45 --> run# 46 --> run# 47 --> run# 48 --> run# 49 --> run# 50 --> run# 51 --> run# 52 --> run# 53 --> run# 54 --> run# 55 --> run# 56 --> run# 57 --> run# 58 --> run# 59 --> run# 60 --> run# 61 --> run# 62 --> run# 63 --> run# 64 --> run# 65 --> run# 66 --> run# 67 --> run# 68 --> run# 69 --> run# 70 --> run# 71 --> run# 72 --> run# 73 --> run# 74 --> run# 75 --> run# 76 --> run# 77 --> run# 78 --> run# 79 --> run# 80 --> run# 81 --> run# 82 --> run# 83 --> run# 84 --> run# 85 --> run# 86 --> run# 87 --> run# 88 --> run# 89 --> run# 90 --> run# 91 --> run# 92 --> run# 93 --> run# 94 --> run# 95 --> run# 96 --> run# 97 --> run# 98 --> run# 99 --> run# 100 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_Adadelta= []\n",
    "test_losses_20Epochs_Adadelta = []\n",
    "accuracies_20Epochs_Adadelta = []\n",
    "for i = 1 : 100\n",
    "    print(\"run# $i --> \")\n",
    "    Y_pred, hidden_outs, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    optimizer_Adadelta = Adadelta(0.95, 0.95, 0.00005, nothing, nothing, nothing)\n",
    "    init!(optimizer_Adadelta, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_Adadelta, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_Adadelta, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_Adadelta, loss_curr)\n",
    "    push!(accuracies_20Epochs_Adadelta, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_Adadelta, accuracies_20Epochs_Adadelta, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_Adadelta, accuracies_20Epochs_Adadelta)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adadelta after 100 individual trains on 100 randomly initialized NNs:\n",
      "\t62 runs converged.\n",
      "\tTrain loss = 1.0104 ± 1.7167\n",
      "\tTest loss = 1.304 ± 1.8591\n",
      "\tAccuracy = 75.4% ± 0.13%\n"
     ]
    }
   ],
   "source": [
    "println(\"Adadelta after 100 individual trains on 100 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_Adadelta); digits = 4)) ± $(round(std(train_losses_20Epochs_Adadelta); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_Adadelta); digits = 4)) ± $(round(std(test_losses_20Epochs_Adadelta); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_Adadelta)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_Adadelta); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = DFP done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> run# 6 --> run# 7 --> run# 8 --> run# 9 --> run# 10 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_DFP= []\n",
    "test_losses_20Epochs_DFP = []\n",
    "accuracies_20Epochs_DFP = []\n",
    "for i = 1 : 10\n",
    "    print(\"run# $i --> \")\n",
    "    Y_pred, hidden_outs, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    optimizer_DFP = DFP(nothing, f, nothing)\n",
    "    init!(optimizer_DFP, num_Ws)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_DFP, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_DFP, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_DFP, loss_curr)\n",
    "    push!(accuracies_20Epochs_DFP, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_DFP, accuracies_20Epochs_DFP, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_DFP, accuracies_20Epochs_DFP)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFP after 10 individual trains on 10 randomly initialized NNs:\n",
      "\t1 runs converged.\n",
      "\tTrain loss = 21.5844 ± NaN\n",
      "\tTest loss = 19.7231 ± NaN\n",
      "\tAccuracy = 66.67% ± NaN%\n"
     ]
    }
   ],
   "source": [
    "println(\"DFP after 10 individual trains on 10 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_DFP); digits = 4)) ± $(round(std(train_losses_20Epochs_DFP); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_DFP); digits = 4)) ± $(round(std(test_losses_20Epochs_DFP); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_DFP)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_DFP); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = LBFGS done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run# 1 --> run# 2 --> run# 3 --> run# 4 --> run# 5 --> Finished\n"
     ]
    }
   ],
   "source": [
    "# Initializing \n",
    "# Start training\n",
    "train_losses_20Epochs_LBFGS= []\n",
    "test_losses_20Epochs_LBFGS = []\n",
    "accuracies_20Epochs_LBFGS = []\n",
    "for i = 1 : 5\n",
    "    print(\"run# $i --> \")\n",
    "    Y_pred, hidden_outs, Ws = MLP(train_x_data[1], 3; hidden_layer_size = 16, num_hidden_layers = 2)\n",
    "    # Ws = deepcopy(Ws_copy)\n",
    "    # Reset params for optimizer\n",
    "    optimizer_LimitedMemoryBFGS = LimitedMemoryBFGS(20, f, nothing, nothing, nothing, nothing)\n",
    "    init!(optimizer_LimitedMemoryBFGS)\n",
    "    curr_loss = 0.0\n",
    "    # Train\n",
    "    for i = 1:num_epoch_1\n",
    "        # print(\"Epoch# $i\\n\\t\")\n",
    "        Ws, curr_loss = epoch_step(optimizer_LimitedMemoryBFGS, Ws; hidden_layer_size = 16, print_info = false)\n",
    "        if curr_loss <= loss_ε || isnan(curr_loss)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if !isnan(curr_loss)\n",
    "        push!(train_losses_20Epochs_LBFGS, train_scores(Ws))\n",
    "    end\n",
    "    flush(stdout)\n",
    "    # println(\"Finished\")\n",
    "    loss_curr, acc_curr = test_scores(Ws)\n",
    "    push!(test_losses_20Epochs_LBFGS, loss_curr)\n",
    "    push!(accuracies_20Epochs_LBFGS, acc_curr)\n",
    "end\n",
    "test_losses_20Epochs_LBFGS, accuracies_20Epochs_LBFGS, ct_valid_loss = \n",
    "filter_scores(test_losses_20Epochs_LBFGS, accuracies_20Epochs_LBFGS)\n",
    "println(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LimitedMemoryBFGS after 5 individual trains on 5 randomly initialized NNs:\n",
      "\t5 runs converged.\n",
      "\tTrain loss = 1.28 ± 3.71\n",
      "\tTest loss = 2.93 ± 4.72\n",
      "\tAccuracy = 66.67% ± 3.79%\n"
     ]
    }
   ],
   "source": [
    "println(\"LimitedMemoryBFGS after 5 individual trains on 5 randomly initialized NNs:\")\n",
    "println(\"\\t$ct_valid_loss runs converged.\")\n",
    "println(\"\\tTrain loss = $(round(mean(train_losses_20Epochs_LBFGS); digits = 4)) ± $(round(std(train_losses_20Epochs_LBFGS); digits = 4))\")\n",
    "println(\"\\tTest loss = $(round(mean(test_losses_20Epochs_LBFGS); digits = 4)) ± $(round(std(test_losses_20Epochs_LBFGS); digits = 4))\")\n",
    "println(\"\\tAccuracy = $(round(mean(accuracies_20Epochs_LBFGS)*100; digits = 2))% ± $(round(std(accuracies_20Epochs_LBFGS); digits = 2))%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Number of epochs and runtime needed to reach a same loss threshold = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ε = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Steepest gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = ConjugateGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = NesterovMomentum_SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = DFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer = LBFGS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (auto threads) 1.7.2",
   "language": "julia",
   "name": "julia-(auto-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
